---
title: "Confidence intervals"
---
Resampling from a sample

To investigate how much the estimates of a population proportion change from sample to sample, you will set up two sampling experiments.

In the first experiment, you will simulate repeated samples from a population. In the second, you will choose a single sample from the first experiment and repeatedly resample from that sample: a method called bootstrapping. More specifically:

Experiment 1: Assume the true proportion of people who will vote for Candidate X is 0.6. Repeatedly sample 30 people from the population and measure the variability of 
 (the sample proportion).

Experiment 2: Take one sample of size 30 from the same population. Repeatedly sample 30 people (with replacement!) from the original sample and measure the variability of 
 (the resample proportion).

It's important to realize that the first experiment relies on knowing the population and is typically impossible in practice. The second relies only on the sample of data and is therefore easy to implement for any statistic. Fortunately, as you will see, the variability in 
, or the proportion of "successes" in a sample, is approximately the same whether we sample from the population or resample from a sample.

We have created 1000 random samples, each of size 30, from the population. The resulting data frame, all_polls, is available in your workspace. Take a look before getting started.
## 

## Instructions `100 XP` {.unnumbered}

- Compute the sample proportion for each of the 1000 original samples, assigning to ex1_props.
  - Group by poll.
  - Summarize to calculate stat as the mean() of cases of vote equalling "yes".

- Select one poll from which to resample, as one_poll.
  - Filter for the first poll. That is, when poll equals 1.
  - Select the vote column.
- Compute p-hat for each resampled poll, as ex2_props.
  - Specify the response as vote. The success value is "yes".
  - Generate 1000 replicates of type "bootstrap".
  - Calculate the "prop"ortion summary statistic.
- Note that because you are looking for an interval estimate, you have not made a hypothesis claim about the proportion (thus, there is no hypothesize step needed in the infer pipeline).

- Using ex1_props, calculate the variability of p-hat In the call to summarize(), set variability to the standard deviation, sd(), of stat.
- Do the same with ex2_props to calculate the variability of p-hat.
A- gain, because you are looking for an interval estimate, you have not made a hypothesis claim about the proportion (thus, there is no hypothesize step needed in the infer pipeline).
``` {.r filename="E1.R"}
# Compute p-hat for each poll
ex1_props <- all_polls %>% 
  # Group by poll
  group_by(poll) %>% 
  # Calculate proportion of yes votes
  summarize(stat = mean(vote == "yes"))
  
# Review the result
ex1_props




# Select one poll from which to resample
one_poll <- all_polls %>%
  # Filter for the first poll
  filter(poll == 1) %>%
  # Select vote
  select(vote)
  
# Compute p-hat* for each resampled poll
ex2_props <- one_poll %>%
  # Specify vote as the response, where yes means success
  specify(response = vote, success = "yes") %>%
  # Generate 1000 reps of type bootstrap
  generate(reps = 1000, type = "bootstrap") %>% 
  # Calculate the summary stat "prop"
  calculate(stat = "prop")




# From previous steps
ex1_props <- all_polls %>% 
  group_by(poll) %>% 
  summarize(stat = mean(vote == "yes"))
ex2_props <- all_polls %>%
  filter(poll == 1) %>%
  select(vote) %>%
  specify(response = vote, success = "yes") %>%
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "prop")
  
# Calculate variability of p-hat
ex1_props %>% 
  summarize(variability = sd(stat))
  
# Calculate variability of p-hat*
ex2_props %>% 
  summarize(variability = sd(stat))
```

## Visualizing the variability of p-hat

In order to compare the variability of the sampled 
 and 
 values in the previous exercises, it is valuable to visualize their distributions. To recall, the exercises walked through two different experiments for investigating the variability of 
 and 
:

* Experiment 1: Sample ( n=30 ) repeatedly from an extremely large population (gold standard, but unrealistic)
* Experiment 2: Resample ( n=30 ) repeatedly with replacement from a single sample of size 30

## Instructions `100 XP` {.unnumbered}

- Combine data from both experiments by calling bind_rows(), passing ex1_props and ex2_props. Call this both_ex_props. A dataset ID column named experiment will be created.
- Using both_ex_props, plot stat colored by experiment.
- Add a density curve using geom_density(), setting the bandwidth argument, bw, to 0.1.

``` {.r filename="E2.R"}
# Combine data from both experiments
both_ex_props <- bind_rows(ex1_props, ex2_props, .id = "experiment")

# Using both_ex_props, plot stat colored by experiment
ggplot(both_ex_props, aes(stat, color = experiment)) + 
  # Add a density layer with bandwidth 0.1
  geom_density(bw = 0.1)
```

## Empirical Rule

Many statistics we use in data analysis (including both the sample average and sample proportion) have nice properties that are used to better understand the population parameter(s) of interest.

One such property is that if the variability of the sample proportion (called the standard error, or 
) is known, then approximately 95% of 
 values (from different samples) will be within 
 of the true population proportion.

To check whether that holds in the situation at hand, let's go back to the polls generated by taking many samples from the same population.

The all_polls dataset contains 1000 samples of size 30 from a population with a probability of voting for Candidate X equal to 0.6.

Note that you will use the R function sd() which calculates the variability of any set of numbers. In statistics, when sd() is applied to a variable (e.g., price of house) we call it the standard deviation. When sd() is applied to a statistic (e.g., set of sample proportions) we call it the standard error.

## Instructions `100 XP` {.unnumbered}

- Run the code to generate props, the proportion of individuals who are planning to vote yes in each poll. This is based upon ex1_props from previous exercises.
- Add a column, is_in_conf_int that is TRUE when the sampled proportion of yes votes is less than 2 standard errors away from the true population proportion of yes votes. That is, the abs()solute difference between prop_yes and true_prop_yes is less than twice sd() of prop_yes.
- Calculate the proportion of sample statistics in the confidence interval, prop_in_conf_int, by taking the mean() of is_in_conf_int.  

``` {.r filename="E3.R"}
# Proportion of yes votes by poll
props <- all_polls %>% 
  group_by(poll) %>% 
  summarize(prop_yes = mean(vote == "yes"))

# The true population proportion of yes votes
true_prop_yes <- 0.6

# Proportion of polls within 2SE
props %>%
  # Add column: is prop_yes in 2SE of 0.6
  mutate(is_in_conf_int = abs(prop_yes - true_prop_yes) < 2 * sd(prop_yes)) %>%
  # Calculate  proportion in conf int
  summarize(prop_in_conf_int = mean(is_in_conf_int))
```

## Bootstrap t-confidence interval

The previous exercises told you two things:

1. You can measure the variability associated with p-hat by resampling from the original sample.
2. Once you know the variability of p-hat, you can use it as a way to measure how far away the true proportion is.

Note that the rate of closeness (here 95%) refers to how often a sample is chosen so that it is close to the population parameter. You won't ever know if a particular dataset is close to the parameter or far from it, but you do know that over your lifetime, 95% of the samples you collect should give you estimates that are within 
 of the true population parameter.

The votes from a single poll, one_poll, and the data from 1000 bootstrap resamples, one_poll_boot are available in your workspace. These are based on Experiment 2 from earlier in the chapter.

As in the previous exercise, when discussing the variability of a statistic, the number is referred to as the standard error.

## Instructions `100 XP` {.unnumbered}

- Calculate 
 and assign the result to p_hat. In the call to summarize(), calculate stat as the mean of vote equalling "yes".
- Find an interval of values that are plausible for the true parameter by calculating 
.
  - The lower bound of the confidence interval is p_hat minus twice the standard error of stat. Use sd() to calculate the standard error.
  - The upper bound is p_hat plus twice the standard error of stat.

``` {.r filename="E4.R"}
# From previous exercises
one_poll <- all_polls %>%
  filter(poll == 1) %>%
  select(vote)
one_poll_boot <- one_poll %>%
  specify(response = vote, success = "yes") %>%
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "prop")
  
p_hat <- one_poll %>%
  # Calculate proportion of yes votes
  summarize(stat = mean(vote == "yes")) %>%
  pull()

# Create an interval of plausible values
one_poll_boot %>%
  summarize(
    # Lower bound is p_hat minus 2 std errs
    lower = p_hat - 2 * sd(stat),
    # Upper bound is p_hat plus 2 std errs
    upper = p_hat + 2 * sd(stat)
  )
```

## Bootstrap percentile interval

The main idea in the previous exercise was that the distance between the original sample 
 and the resampled (or bootstrapped) 
 values gives a measure for how far the original 
 is from the true population proportion.

The same variability can be measured through a different mechanism. As before, if 
 is sufficiently close to the true parameter, then the resampled (bootstrapped) 
 values will vary in such a way that they overlap with the true parameter.

Instead of using 
 as a way to measure the middle 95% of the sampled 
 values, you can find the middle of the resampled 
 values by removing the upper and lower 2.5%. Note that this second method of constructing bootstrap intervals also gives an intuitive way for making 90% or 99% confidence intervals as well as 95% intervals.

The bootstrapped resamples, one_poll_boot, and the proportion of yes votes, p_hat are available in your workspace.

## Instructions `100 XP` {.unnumbered}

- Run the code to remind yourself of the t-interval from the previous exercise.
Calculate the 95 percent interval of the bootstrapped p-hat values contained in one_poll_boot.
Summarize to calculate the lower end at the 2.5% quantile of stat by setting p to 0.025.
Calculate the upper end in a similar way.
- Perform the same calculation using infer's convenience function, get_confidence_interval(). For the interval, use level = 0.95, and call the output percentile_ci.
- visualize() the distribution of bootstrapped proportions with the middle 95 percent highlighted.
Set the endpoints of the highlighted region to percentile_ci.
Set direction to "between" to highlight in-between those endpoints.

``` {.r filename="E5.R"}
# From previous exercise: bootstrap t-confidence interval
one_poll_boot %>%
  summarize(
    lower = p_hat - 2 * sd(stat),
    upper = p_hat + 2 * sd(stat)
  )
  
# Manually calculate a 95% percentile interval
one_poll_boot %>%
  summarize(
    lower = quantile(stat, 0.025),
    upper = quantile(stat, 0.975)
  )




# From previous step
one_poll_boot %>%
  summarize(
    lower = quantile(stat, 0.025),
    upper = quantile(stat, 0.975)
  )
  
# Calculate the same interval, more conveniently
percentile_ci <- one_poll_boot %>% 
  get_confidence_interval(p = 0.025)
  
# Review the value
percentile_ci




# From previous step
percentile_ci <- one_poll_boot %>% 
  get_confidence_interval(level = 0.95)
  
one_poll_boot %>% 
  # Visualize in-between the endpoints given by percentile_ci
  visualize(endpoints = percentile_ci, direction = "between")
```

## Sample size effects on bootstrap CIs

In a previous multiple choice exercise, you realized that if you resampled the data with the wrong size (e.g. 300 or 3 instead of 30), the standard error (SE) of the sample proportions was off. With 300 resampled observations, the SE was too small. With 3 resampled observations, the SE was too large.

Here, you will use the incorrect standard error (based on the incorrect sample size) to create a confidence interval. The idea is that when the standard error is off, the interval is not particularly useful, nor is it correct.

## Instructions `100 XP` {.unnumbered}

- A function for calculating the bootstrapped t-confidence interval, calc_t_conf_int(), is shown is the script. Read the code and try to understand it.
- Call calc_t_conf_int() on one_poll_boot to calculate the correct t-confidence interval.
- Do the same on one_poll_boot_300, to find an incorrect interval for the resamples of size 300.
- Do the same on one_poll_boot_3, to find an incorrect interval for the resamples of size 3.

``` {.r filename="E6.R"}
calc_t_conf_int <- function(resampled_dataset) {
  resampled_dataset %>%
    summarize(
      lower = p_hat - 2 * sd(stat),
      upper = p_hat + 2 * sd(stat)
    )
}

# Find the bootstrap t-confidence interval for 30 resamples
calc_t_conf_int(one_poll_boot)

# ... and for 300 resamples
calc_t_conf_int(one_poll_boot_300)

# ... and for 3 resamples
calc_t_conf_int(one_poll_boot_3)
```

## Sample proportion value effects on bootstrap CIs

One additional element that changes the width of the confidence interval is the sample parameter value, 
p-hat.

Generally, when the true parameter is close to 0.5, the standard error of 
 is larger than when the true parameter is closer to 0 or 1. When calculating a bootstrap t-confidence interval, the standard error controls the width of the CI, and here (given a true parameter of 0.8) the sample proportion is higher than in previous exercises, so the width of the confidence interval will be narrower.

## Instructions `100 XP` {.unnumbered}

- calc_p_hat() is shown in the script to calculate the sample proportions. calc_t_conf_int() from the previous exercise has been updated to now use any value of p_hat as an argument. Read their definitions and try to understand them.
- Run the code to calculate the bootstrap t-confidence interval for the original population.
- Consider a new population where the true parameter is 0.8, one_poll_0.8. Calculate 
 of this new sample, using the same technique as with the original dataset. Call it p_hat_0.8.
- Find the bootstrap t-confidence interval using the new bootstrapped data, one_poll_boot_0.8, and the new 
p-hat. Notice that it is narrower than previously calculated. 

``` {.r filename="E7.R"}
calc_p_hat <- function(dataset) {
  dataset %>%
    summarize(stat = mean(vote == "yes")) %>%
    pull()
}
calc_t_conf_int <- function(resampled_dataset, p_hat) {
  resampled_dataset %>%
    summarize(
      lower = p_hat - 2 * sd(stat),
      upper = p_hat + 2 * sd(stat)
    )
}

# Find proportion of yes votes from original population
p_hat <- calc_p_hat(one_poll)

# Review the value
p_hat  

# Calculate bootstrap t-confidence interval (original 0.6 param)
calc_t_conf_int(one_poll_boot, p_hat)

# Find proportion of yes votes from new population
p_hat_0.8 <- calc_p_hat(one_poll_0.8)
  
# Review the value
p_hat_0.8  
  
# Calculate the bootstrap t-confidence interval (new 0.8 param)
calc_t_conf_int(one_poll_boot_0.8, p_hat_0.8)
```

## Percentile effects on bootstrap CIs

Most scientists use 95% intervals to quantify their uncertainty about an estimate. That is, they understand that over a lifetime of creating confidence intervals, only 95% of them will actually contain the parameter that they set out to estimate.

There are studies, however, which warrant either stricter or more lenient confidence intervals (and subsequent error rates).

The previous bootstrapped p-hat values have been loaded for you and are available in one_poll_boot.

## Instructions `100 XP` {.unnumbered}

- Calculate a 95% percentile interval by calling get_confidence_interval(), setting level to 0.95.
- Do the same for a 99% interval,
- … and a 90% interval.
- The results you just got are stored in a dataframe called conf_int_data. With this dataset, plot ci_endpoints (vertical axis) vs. ci_percent (horizontal axis), and add a line layer using geom_line(). 

``` {.r filename="E8.R"}
# Calculate a 95% bootstrap percentile interval
one_poll_boot %>% 
  get_confidence_interval(level = 0.95) 

# Calculate a 99% bootstrap percentile interval
one_poll_boot %>% 
  get_confidence_interval(level = 0.99) 

# Calculate a 90% bootstrap percentile interval
one_poll_boot %>% 
  get_confidence_interval(level = 0.90) 

# Plot ci_endpoints vs. ci_percent to compare the intervals
ggplot(conf_int_data, aes(ci_percent, ci_endpoints)) +
  # Add a line layer
  geom_line()
```