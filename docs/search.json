[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "libro",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Introduction_to_R_C1.html",
    "href": "Introduction_to_R_C1.html",
    "title": "2  Intro to basics",
    "section": "",
    "text": "Intro to basics"
  },
  {
    "objectID": "Introduction_to_R_C1.html#how-it-works",
    "href": "Introduction_to_R_C1.html#how-it-works",
    "title": "2  Intro to basics",
    "section": "2.1 How it works",
    "text": "2.1 How it works\nIn the editor on the right you should type R code to solve the exercises. When you hit the ‘Submit Answer’ button, every line of code is interpreted and executed by R and you get a message whether or not your code was correct. The output of your R code is shown in the console in the lower right corner.\nR makes use of the # sign to add comments, so that you and others can understand what the R code is about. Just like Twitter! Comments are not run as R code, so they will not influence your result. For example, Calculate 3 + 4 in the editor on the right is a comment.\nYou can also execute R commands straight in the console. This is a good way to experiment with R code, as your submission is not checked for correctness."
  },
  {
    "objectID": "Introduction_to_R_C1.html#instructions-100-xp",
    "href": "Introduction_to_R_C1.html#instructions-100-xp",
    "title": "2  Intro to basics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nIn the editor on the right there is already some sample code. Can you see which ines are actual R code and which are comments? Add a line of code that calculates the sum of 6 and 12, and hit the ‘Submit Answer’ button.\n\n\nE1.R\n\n# Calculate 3 + 4\n3 + 4\n\n# Calculate 6 + 12\n6 + 12"
  },
  {
    "objectID": "Introduction_to_R_C1.html#arithmetic-with-r",
    "href": "Introduction_to_R_C1.html#arithmetic-with-r",
    "title": "2  Intro to basics",
    "section": "2.2 Arithmetic with R",
    "text": "2.2 Arithmetic with R\nIn its most basic form, R can be used as a simple calculator. Consider the following arithmetic operators:\n\nAddition: +\nSubtraction: -\nMultiplication: *\nDivision: /\nExponentiation: ^\nModulo: %%\n\nThe last two might need some explaining:\nThe ^ operator raises the number to its left to the power of the number to its right: for example 3^2 is 9. The modulo returns the remainder of the division of the number to the left by the number on its right, for example 5 modulo 3 or 5 %% 3 is 2. With this knowledge, follow the instructions to complete the exercise.\n\nType 2^5 in the editor to calculate 2 to the power 5.\nType 28 %% 6 to calculate 28 modulo 6.\nSubmit the answer and have a look at the R output in the console.\nNote how the # symbol is used to add comments on the R code.\n\n\n\nE2.R\n\n# An addition\n5 + 5 \n\n# A subtraction\n5 - 5 \n\n# A multiplication\n3 * 5\n\n # A division\n(5 + 5) /2 \n\n# Exponentiation\n2 ^5\n\n# Modulo\n28 %% 6"
  },
  {
    "objectID": "Introduction_to_R_C1.html#variable-assignment",
    "href": "Introduction_to_R_C1.html#variable-assignment",
    "title": "2  Intro to basics",
    "section": "2.3 Variable assignment",
    "text": "2.3 Variable assignment\nA basic concept in (statistical) programming is called a variable.\nA variable allows you to store a value (e.g. 4) or an object (e.g. a function description) in R. You can then later use this variable’s name to easily access the value or the object that is stored within this variable.\nYou can assign a value 4 to a variable my_var with the command\n\nmy_var <- 4"
  },
  {
    "objectID": "Introduction_to_R_C1.html#instructions-100-xp-1",
    "href": "Introduction_to_R_C1.html#instructions-100-xp-1",
    "title": "2  Intro to basics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nOver to you: complete the code in the editor such that it assigns the value 42 to the variable x in the editor. Submit the answer. Notice that when you ask R to print x, the value 42 appears.\n\n\nE3.R\n\n# Assign the value 42 to x\nx <- 42\n\n# Print out the value of the variable x\nx"
  },
  {
    "objectID": "Introduction_to_R_C1.html#variable-assignment-2",
    "href": "Introduction_to_R_C1.html#variable-assignment-2",
    "title": "2  Intro to basics",
    "section": "2.4 Variable assignment (2)",
    "text": "2.4 Variable assignment (2)\nSuppose you have a fruit basket with five apples. As a data analyst in training, you want to store the number of apples in a variable with the name my_apples."
  },
  {
    "objectID": "Introduction_to_R_C1.html#instructions-100-xp-2",
    "href": "Introduction_to_R_C1.html#instructions-100-xp-2",
    "title": "2  Intro to basics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nType the following code in the editor: my_apples <- 5. This will assign the value 5 to my_apples. Type: my_apples below the second comment. This will print out the value of my_apples. Submit your answer, and look at the output: you see that the number 5 is printed. So R now links the variable my_apples to the value 5.\n\n\nE4.R\n\n# Assign the value 5 to the variable my_apples\nmy_apples <- 5\n\n# Print out the value of the variable my_apples\nmy_apples"
  },
  {
    "objectID": "Introduction_to_R_C1.html#variable-assignment-3",
    "href": "Introduction_to_R_C1.html#variable-assignment-3",
    "title": "2  Intro to basics",
    "section": "2.5 Variable assignment (3)",
    "text": "2.5 Variable assignment (3)\nEvery tasty fruit basket needs oranges, so you decide to add six oranges. As a data analyst, your reflex is to immediately create the variable my_oranges and assign the value 6 to it. Next, you want to calculate how many pieces of fruit you have in total. Since you have given meaningful names to these values, you can now code this in a clear way:\n\nmy_apples + my_oranges\n\nInstructions 100 XP\n\nAssign to my_oranges the value 6.\nAdd the variables my_apples and my_oranges and have R simply print the result.\nAssign the result of adding my_apples and my_oranges to a new variable my_fruit.\n\n\n\nE5.R\n\n# Assign a value to the variables my_apples and my_oranges\nmy_apples <- 5\n\n\n# Add these two variables together\nmy_oranges<-6\n\nmy_apples\nmy_oranges\n\n# Create the variable my_fruit\nmy_fruit = my_apples + my_oranges\nmy_fruit"
  },
  {
    "objectID": "Introduction_to_R_C1.html#apples-and-oranges",
    "href": "Introduction_to_R_C1.html#apples-and-oranges",
    "title": "2  Intro to basics",
    "section": "2.6 Apples and oranges",
    "text": "2.6 Apples and oranges\nCommon knowledge tells you not to add apples and oranges. But hey, that is what you just did, no :-)? The my_apples and my_oranges variables both contained a number in the previous exercise. The + operator works with numeric variables in R. If you really tried to add “apples” and “oranges”, and assigned a text value to the variable my_oranges (see the editor), you would be trying to assign the addition of a numeric and a character variable to the variable my_fruit. This is not possible."
  },
  {
    "objectID": "Introduction_to_R_C1.html#instructions-100-xp-3",
    "href": "Introduction_to_R_C1.html#instructions-100-xp-3",
    "title": "2  Intro to basics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nSubmit the answer and read the error message. Make sure to understand why this did not work. Adjust the code so that R knows you have 6 oranges and thus a fruit basket with 11 pieces of fruit.\n\n\nE6.R\n\n# Assign a value to the variable my_apples\nmy_apples <- 5 \n\n# Fix the assignment of my_oranges\nmy_oranges <- 6\n\n# Create the variable my_fruit and print it out\nmy_fruit <- my_apples + my_oranges \nmy_fruit"
  },
  {
    "objectID": "Introduction_to_R_C1.html#basic-data-types-in-r",
    "href": "Introduction_to_R_C1.html#basic-data-types-in-r",
    "title": "2  Intro to basics",
    "section": "2.7 Basic data types in R",
    "text": "2.7 Basic data types in R\nR works with numerous data types. Some of the most basic types to get started are:\n\nDecimal values like 4.5 are called numerics.\nWhole numbers like 4 are called integers. Integers are also numerics.\nBoolean values (TRUE or FALSE) are called logical.\nText (or string) values are called characters.\nNote how the quotation marks in the editor indicate that “some text” is a string."
  },
  {
    "objectID": "Introduction_to_R_C1.html#instructions-100-xp-4",
    "href": "Introduction_to_R_C1.html#instructions-100-xp-4",
    "title": "2  Intro to basics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nChange the value of the:\n\nmy_numeric variable to 42.\nmy_character variable to “universe”. Note that the quotation marks indicate that “universe” is a character.\nmy_logical variable to FALSE.\nNote that R is case sensitive!\n\n\n\nE7.R\n\n# Change my_numeric to be 42\nmy_numeric <- 42\n\n# Change my_character to be \"universe\"\nmy_character <- \"universe\"\n\n# Change my_logical to be FALSE\nmy_logical <- FALSE"
  },
  {
    "objectID": "Introduction_to_R_C1.html#whats-that-data-type",
    "href": "Introduction_to_R_C1.html#whats-that-data-type",
    "title": "2  Intro to basics",
    "section": "2.8 What’s that data type?",
    "text": "2.8 What’s that data type?\nDo you remember that when you added 5 + “six”, you got an error due to a mismatch in data types? You can avoid such embarrassing situations by checking the data type of a variable beforehand. You can do this with the class() function, as the code in the editor shows."
  },
  {
    "objectID": "Introduction_to_R_C1.html#instructions-100-xp-5",
    "href": "Introduction_to_R_C1.html#instructions-100-xp-5",
    "title": "2  Intro to basics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nComplete the code in the editor and also print out the classes of my_character and my_logical.\n\n\nE8.R\n\n# Declare variables of different types\nmy_numeric <- 42\nmy_character <- \"universe\"\nmy_logical <- FALSE \n\n# Check class of my_numeric\nclass(my_numeric)\n\n# Check class of my_character\nclass(my_character)\n\n# Check class of my_logical\nclass(my_logical)"
  },
  {
    "objectID": "Introduction_to_R_C2.html",
    "href": "Introduction_to_R_C2.html",
    "title": "3  Vectors",
    "section": "",
    "text": "Vectors"
  },
  {
    "objectID": "Introduction_to_R_C2.html#create-a-vector",
    "href": "Introduction_to_R_C2.html#create-a-vector",
    "title": "3  Vectors",
    "section": "3.1 Create a vector",
    "text": "3.1 Create a vector\nFeeling lucky? You better, because this chapter takes you on a trip to the City of Sins, also known as Statisticians Paradise!\nThanks to R and your new data-analytical skills, you will learn how to uplift your performance at the tables and fire off your career as a professional gambler. This chapter will show how you can easily keep track of your betting progress and how you can do some simple analyses on past actions. Next stop, Vegas Baby… VEGAS!!"
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp",
    "href": "Introduction_to_R_C2.html#instructions-100-xp",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nDo you still remember what you have learned in the first chapter? Assign the value “Go!” to the variable vegas. Remember: R is case sensitive!\n\n\nE1.R\n\n# Define the variable vegas\nvegas <- \"Go!\""
  },
  {
    "objectID": "Introduction_to_R_C2.html#create-a-vector-2",
    "href": "Introduction_to_R_C2.html#create-a-vector-2",
    "title": "3  Vectors",
    "section": "3.2 Create a vector (2)",
    "text": "3.2 Create a vector (2)\nLet us focus first!\nOn your way from rags to riches, you will make extensive use of vectors. ectors are one-dimension arrays that can hold numeric data, character data, or logical data. In other words, a vector is a simple tool to store data. For example, you can store your daily gains and losses in the casinos.\nIn R, you create a vector with the combine function c(). You place the vector elements separated by a comma between the parentheses. For example:\n\nnumeric_vector <- c(1, 2, 3) character_vector <- c(“a”, “b”, “c”)\n\nOnce you have created these vectors in R, you can use them to do calculations."
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-1",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-1",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nComplete the code such that boolean_vector contains the three elements: TRUE, FALSE and TRUE (in that order).\n\n\nE2.R\n\nnumeric_vector <- c(1, 10, 49)\ncharacter_vector <- c(\"a\", \"b\", \"c\")\n\n# Complete the code for boolean_vector\nboolean_vector <-c(TRUE,FALSE,TRUE)\n\nCreate a vector (3) After one week in Las Vegas and still zero Ferraris in your garage, you decide that it is time to start using your data analytical superpowers.\nBefore doing a first analysis, you decide to first collect all the winnings and losses for the last week:\nFor poker_vector:\n\nOn Monday you won $140\nTuesday you lost $50\nWednesday you won $20\nThursday you lost $120\nFriday you won $240\n\nFor roulette_vector:\n\nOn Monday you lost $24\nTuesday you lost $50\nWednesday you won $100\nThursday you lost $350\nFriday you won $10\n\nYou only played poker and roulette, since there was a delegation of mediums that occupied the craps tables. To be able to use this data in R, you decide to create the variables poker_vector and roulette_vector.\nInstructions 100 XP {.unnumbered}\nAssign the winnings/losses for roulette to the variable roulette_vector. You lost $24, then lost $50, won $100, lost $350, and won $10.\n\n\nE3.R\n\n# Poker winnings from Monday to Friday\npoker_vector <- c(140, -50, 20, -120, 240)\n\n# Roulette winnings from Monday to Friday\nroulette_vector <- c(-24,-50,100,-350,10)"
  },
  {
    "objectID": "Introduction_to_R_C2.html#naming-a-vector",
    "href": "Introduction_to_R_C2.html#naming-a-vector",
    "title": "3  Vectors",
    "section": "3.3 Naming a vector",
    "text": "3.3 Naming a vector\nAs a data analyst, it is important to have a clear view on the data that you are using. Understanding what each element refers to is therefore essential.\nIn the previous exercise, we created a vector with your winnings over the week. Each vector element refers to a day of the week but it is hard to tell which element belongs to which day. It would be nice if you could show that in the vector itself.\nYou can give a name to the elements of a vector with the names() function. Have a look at this example:\n\nsome_vector <- c(“John Doe”, “poker player”) names(some_vector) <- c(“Name”, “Profession”)\n\nThis code first creates a vector some_vector and then gives the two elements a name. The first element is assigned the name Name, while the second element is labeled Profession. Printing the contents to the console yields following output:\n\n     Name     Profession \n“John Doe” “poker player”"
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-2",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-2",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nThe code in the editor names the elements in poker_vector with the days of the week. Add code to do the same thing for roulette_vector\n\n\nE4.R\n\n# Poker winnings from Monday to Friday\npoker_vector <- c(140, -50, 20, -120, 240)\n\n# Roulette winnings from Monday to Friday\nroulette_vector <- c(-24, -50, 100, -350, 10)\n\n# Assign days as names of poker_vector\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\n\n# Assign days as names of roulette_vector\nnames(poker_vector) <-days_vector \n  names(roulette_vector) <-days_vector"
  },
  {
    "objectID": "Introduction_to_R_C2.html#naming-a-vector-2",
    "href": "Introduction_to_R_C2.html#naming-a-vector-2",
    "title": "3  Vectors",
    "section": "3.4 Naming a vector (2)",
    "text": "3.4 Naming a vector (2)\nIf you want to become a good statistician, you have to become lazy. (If you are already lazy, chances are high you are one of those exceptional, natural-born statistical talents.)\nIn the previous exercises you probably experienced that it is boring and frustrating to type and retype information such as the days of the week. However, when you look at it from a higher perspective, there is a more efficient way to do this, namely, to assign the days of the week vector to a variable!\nJust like you did with your poker and roulette returns, you can also create a variable that contains the days of the week. This way you can use and re-use it."
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-3",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-3",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nA variable days_vector that contains the days of the week has already been created for you.\nUse days_vector to set the names of poker_vector and roulette_vector.\n\n\n\nE5.R\n\n# Poker winnings from Monday to Friday\npoker_vector <- c(140, -50, 20, -120, 240)\n\n# Roulette winnings from Monday to Friday\nroulette_vector <- c(-24, -50, 100, -350, 10)\n\n# The variable days_vector\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\n \n# Assign the names of the day to roulette_vector and poker_vector\nnames(poker_vector) <-days_vector \n  names(roulette_vector) <-days_vector"
  },
  {
    "objectID": "Introduction_to_R_C2.html#calculating-total-winnings",
    "href": "Introduction_to_R_C2.html#calculating-total-winnings",
    "title": "3  Vectors",
    "section": "3.5 Calculating total winnings",
    "text": "3.5 Calculating total winnings\nNow that you have the poker and roulette winnings nicely as named vectors, you can start doing some data analytical magic.\nYou want to find out the following type of information:\nHow much has been your overall profit or loss per day of the week? Have you lost money over the week in total? Are you winning/losing money on poker or on roulette? To get the answers, you have to do arithmetic calculations on vectors.\nIt is important to know that if you sum two vectors in R, it takes the element-wise sum. For example, the following three statements are completely equivalent:\n\nc(1, 2, 3) + c(4, 5, 6) c(1 + 4, 2 + 5, 3 + 6) c(5, 7, 9)\n\nYou can also do the calculations with variables that represent vectors:\n\na <- c(1, 2, 3) b <- c(4, 5, 6) c <- a + b"
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-4",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-4",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nTake the sum of the variables A_vector and B_vector and assign it to total_vector. Inspect the result by printing out total_vector.\n\n\nE6.R\n\nA_vector <- c(1, 2, 3)\nB_vector <- c(4, 5, 6)\n\n# Take the sum of A_vector and B_vector\ntotal_vector <- A_vector+B_vector\n  \n# Print out total_vector\ntotal_vector"
  },
  {
    "objectID": "Introduction_to_R_C2.html#calculating-total-winnings-2",
    "href": "Introduction_to_R_C2.html#calculating-total-winnings-2",
    "title": "3  Vectors",
    "section": "3.6 Calculating total winnings (2)",
    "text": "3.6 Calculating total winnings (2)\nNow you understand how R does arithmetic with vectors, it is time to get those Ferraris in your garage! First, you need to understand what the overall profit or loss per day of the week was. The total daily profit is the sum of the profit/loss you realized on poker per day, and the profit/loss you realized on roulette per day.\nIn R, this is just the sum of roulette_vector and poker_vector.\nInstructions 100 XP\nAssign to the variable total_daily how much you won or lost on each day in total (poker and roulette combined).\n\n\nE7.R\n\n# Poker and roulette winnings from Monday to Friday:\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\n# Assign to total_daily how much you won/lost on each day\ntotal_daily <- roulette_vector+ poker_vector\ntotal_daily"
  },
  {
    "objectID": "Introduction_to_R_C2.html#calculating-total-winnings-3",
    "href": "Introduction_to_R_C2.html#calculating-total-winnings-3",
    "title": "3  Vectors",
    "section": "3.7 Calculating total winnings (3)",
    "text": "3.7 Calculating total winnings (3)\nBased on the previous analysis, it looks like you had a mix of good and bad days. This is not what your ego expected, and you wonder if there may be a very tiny chance you have lost money over the week in total?\nA function that helps you to answer this question is sum(). It calculates the sum of all elements of a vector. For example, to calculate the total amount of money you have lost/won with poker you do:\ntotal_poker <- sum(poker_vector)"
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-5",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-5",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nCalculate the total amount of money that you have won/lost with roulette and assign to the variable total_roulette. Now that you have the totals for roulette and poker, you can easily calculate total_week (which is the sum of all gains and losses of the week). Print out total_week.\n\n\nE8.R\n\n# Total winnings with roulette\ntotal_roulette <-  sum (roulette_vector)\n\n# Total winnings overall\ntotal_week <-(total_roulette + total_poker)\n\n# Print out total_week\n  total_poker\n  total_roulette\n  total_week"
  },
  {
    "objectID": "Introduction_to_R_C2.html#comparing-total-winnings",
    "href": "Introduction_to_R_C2.html#comparing-total-winnings",
    "title": "3  Vectors",
    "section": "3.8 Comparing total winnings",
    "text": "3.8 Comparing total winnings\nOops, it seems like you are losing money. Time to rethink and adapt your strategy! This will require some deeper analysis…\nAfter a short brainstorm in your hotel’s jacuzzi, you realize that a possible explanation might be that your skills in roulette are not as well developed as your skills in poker. So maybe your total gains in poker are higher (or > ) than in roulette."
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-6",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-6",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate total_poker and total_roulette as in the previous exercise. Use the sum() function twice.\nCheck if your total gains in poker are higher than for roulette by using a comparison. Simply print out the result of this comparison. What do you conclude, should you focus on roulette or on poker?\n\n\n\nE9.R\n\n# Poker and roulette winnings from Monday to Friday:\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\n# Calculate total gains for poker and roulette\ntotal_poker <- sum(poker_vector)\ntotal_roulette <- sum(roulette_vector)\n\n# Check if you realized higher total gains in poker than in roulette\ntotal_poker > total_roulette"
  },
  {
    "objectID": "Introduction_to_R_C2.html#vector-selection-the-good-times",
    "href": "Introduction_to_R_C2.html#vector-selection-the-good-times",
    "title": "3  Vectors",
    "section": "3.9 Vector selection: the good times",
    "text": "3.9 Vector selection: the good times\nYour hunch seemed to be right. It appears that the poker game is more your cup of tea than roulette.\nAnother possible route for investigation is your performance at the beginning of the working week compared to the end of it. You did have a couple of Margarita cocktails at the end of the week…\nTo answer that question, you only want to focus on a selection of the total_vector. In other words, our goal is to select specific elements of the vector. To select elements of a vector (and later matrices, data frames, …), you can use square brackets. Between the square brackets, you indicate what elements to select. For example, to select the first element of the vector, you type poker_vector[1]. To select the second element of the vector, you type poker_vector[2], etc. Notice that the first element in a vector has index 1, not 0 as in many other programming languages."
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-7",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-7",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nAssign the poker results of Wednesday to the variable poker_wednesday.\n\n\nE10.R\n\n# Poker and roulette winnings from Monday to Friday:\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\n# Define a new variable based on a selection\npoker_wednesday <- poker_vector[3]"
  },
  {
    "objectID": "Introduction_to_R_C2.html#vector-selection-the-good-times-2",
    "href": "Introduction_to_R_C2.html#vector-selection-the-good-times-2",
    "title": "3  Vectors",
    "section": "3.10 Vector selection: the good times (2)",
    "text": "3.10 Vector selection: the good times (2)\nHow about analyzing your midweek results?\nTo select multiple elements from a vector, you can add square brackets at the end of it. You can indicate between the brackets what elements should be selected. For example: suppose you want to select the first and the fifth day of the week: use the vector c(1, 5) between the square brackets. For example, the code below selects the first and fifth element of poker_vector:\npoker_vector[c(1, 5)]"
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-8",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-8",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nAssign the poker results of Tuesday, Wednesday and Thursday to the variable poker_midweek.\n\n\nE11.R\n\n\n# Poker and roulette winnings from Monday to Friday:\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\n# Define a new variable based on a selection\npoker_midweek <- poker_vector[c(2, 3, 4)]"
  },
  {
    "objectID": "Introduction_to_R_C2.html#vector-selection-the-good-times-3",
    "href": "Introduction_to_R_C2.html#vector-selection-the-good-times-3",
    "title": "3  Vectors",
    "section": "3.11 Vector selection: the good times (3)",
    "text": "3.11 Vector selection: the good times (3)\nSelecting multiple elements of poker_vector with c(2, 3, 4) is not very convenient. Many statisticians are lazy people by nature, so they created an easier way to do this: c(2, 3, 4) can be abbreviated to2:4, which generates a vector with all natural numbers from 2 up to 4.\nSo, another way to find the mid-week results is poker_vector[2:4]. Notice how the vector 2:4 is placed between the square brackets to select element 2 up to 4."
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-9",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-9",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nAssign to roulette_selection_vector the roulette results from Tuesday up to Friday; make use of : if it makes things easier for you.\n\n\nE12.R\n\n# Poker and roulette winnings from Monday to Friday:\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\n# Define a new variable based on a selection\nroulette_selection_vector <- roulette_vector[2:5]"
  },
  {
    "objectID": "Introduction_to_R_C2.html#vector-selection-the-good-times-4",
    "href": "Introduction_to_R_C2.html#vector-selection-the-good-times-4",
    "title": "3  Vectors",
    "section": "3.12 Vector selection: the good times (4)",
    "text": "3.12 Vector selection: the good times (4)\nAnother way to tackle the previous exercise is by using the names of the vector elements (Monday, Tuesday, …) instead of their numeric positions. For example,\npoker_vector[“Monday”] will select the first element of poker_vector since “Monday” is the name of that first element.\nJust like you did in the previous exercise with numerics, you can also use the element names to select multiple elements, for example:\npoker_vector[c(“Monday”,“Tuesday”)]"
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-10",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-10",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nSelect the first three elements in poker_vector by using their names: “Monday”, “Tuesday” and “Wednesday”. Assign the result of the selection to poker_start. Calculate the average of the values in poker_start with the mean() function. Simply print out the result so you can inspect it.\n\n\nE13.R\n\n\n# Poker and roulette winnings from Monday to Friday:\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\n# Select poker results for Monday, Tuesday and Wednesday\npoker_start <- poker_vector[c(\"Monday\",\"Tuesday\",\"Wednesday\" )]\n  \n# Calculate the average of the elements in poker_start\nmean(poker_start)"
  },
  {
    "objectID": "Introduction_to_R_C2.html#selection-by-comparison---step-1",
    "href": "Introduction_to_R_C2.html#selection-by-comparison---step-1",
    "title": "3  Vectors",
    "section": "3.13 Selection by comparison - Step 1",
    "text": "3.13 Selection by comparison - Step 1\nBy making use of comparison operators, we can approach the previous question in a more proactive way.\nThe (logical) comparison operators known to R are:\n\n< for less than\n\nfor greater than\n\n<= for less than or equal to\n\n= for greater than or equal to\n\n== for equal to each other\n!= not equal to each other\n\nAs seen in the previous chapter, stating 6 > 5 returns TRUE. The nice thing about R is that you can use these comparison operators also on vectors. For example:\n\nc(4, 5, 6) > 5 [1] FALSE FALSE TRUE\n\nThis command tests for every element of the vector if the condition stated by the comparison operator is TRUE or FALSE.\nInstructions 100 XP Check which elements in poker_vector are positive (i.e. > 0) and assign this to selection_vector. Print out selection_vector so you can inspect it. The printout tells you whether you won (TRUE) or lost (FALSE) any money for each day.\n\n\nE14.R\n\n# Poker and roulette winnings from Monday to Friday:\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\n# Which days did you make money on poker?\nselection_vector <- poker_vector > 0\n  \n# Print out selection_vector\nselection_vector"
  },
  {
    "objectID": "Introduction_to_R_C2.html#selection-by-comparison---step-2",
    "href": "Introduction_to_R_C2.html#selection-by-comparison---step-2",
    "title": "3  Vectors",
    "section": "3.14 Selection by comparison - Step 2",
    "text": "3.14 Selection by comparison - Step 2\nWorking with comparisons will make your data analytical life easier. Instead of selecting a subset of days to investigate yourself (like before), you can simply ask R to return only those days where you realized a positive return for poker.\nIn the previous exercises you used selection_vector <- poker_vector > 0 to find the days on which you had a positive poker return. Now, you would like to know not only the days on which you won, but also how much you won on those days.\nYou can select the desired elements, by putting selection_vector between the square brackets that follow poker_vector:\n\npoker_vector[selection_vector]\n\nR knows what to do when you pass a logical vector in square brackets: it will only select the elements that correspond to TRUE in selection_vector."
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-11",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-11",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUse selection_vector in square brackets to assign the amounts that you won on the profitable days to the variable poker_winning_days.\n\n\nE15.R\n\n# Poker and roulette winnings from Monday to Friday:\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\n# Which days did you make money on poker?\nselection_vector <- poker_vector > 0\n\n# Select from poker_vector these days\npoker_winning_days <- poker_vector[selection_vector]"
  },
  {
    "objectID": "Introduction_to_R_C2.html#advanced-selection",
    "href": "Introduction_to_R_C2.html#advanced-selection",
    "title": "3  Vectors",
    "section": "3.15 Advanced selection",
    "text": "3.15 Advanced selection\nJust like you did for poker, you also want to know those days where you realized a positive return for roulette."
  },
  {
    "objectID": "Introduction_to_R_C2.html#instructions-100-xp-12",
    "href": "Introduction_to_R_C2.html#instructions-100-xp-12",
    "title": "3  Vectors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nCreate the variable selection_vector, this time to see if you made profit with roulette for different days. Assign the amounts that you made on the days that you ended positively for roulette to the variable roulette_winning_days. This vector thus contains the positive winnings of roulette_vector.\n\n\nE16.R\n\n# Poker and roulette winnings from Monday to Friday:\npoker_vector <- c(140, -50, 20, -120, 240)\nroulette_vector <- c(-24, -50, 100, -350, 10)\ndays_vector <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nnames(poker_vector) <- days_vector\nnames(roulette_vector) <- days_vector\n\n# Which days did you make money on roulette?\nselection_vector <- roulette_vector > 0\n\n# Select from roulette_vector these days\nroulette_winning_days <- roulette_vector[selection_vector]"
  },
  {
    "objectID": "Introduction_to_R_C3.html#whats-a-matrix",
    "href": "Introduction_to_R_C3.html#whats-a-matrix",
    "title": "4  Matrices",
    "section": "4.1 What’s a matrix?",
    "text": "4.1 What’s a matrix?\nIn R, a matrix is a collection of elements of the same data type (numeric, character, or logical) arranged into a fixed number of rows and columns. Since you are only working with rows and columns, a matrix is called two-dimensional.\nYou can construct a matrix in R with the matrix() function. Consider the following example:\n\nmatrix(1:9, byrow = TRUE, nrow = 3)\n\nIn the matrix() function:\nThe first argument is the collection of elements that R will arrange into the rows and columns of the matrix. Here, we use 1:9 which is a shortcut for c(1, 2, 3, 4, 5, 6, 7, 8, 9). The argument byrow indicates that the matrix is filled by the rows. If we want the matrix to be filled by the columns, we just place byrow = FALSE. The third argument nrow indicates that the matrix should have three rows."
  },
  {
    "objectID": "Introduction_to_R_C3.html#instructions-100-xp",
    "href": "Introduction_to_R_C3.html#instructions-100-xp",
    "title": "4  Matrices",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nConstruct a matrix with 3 rows containing the numbers 1 up to 9, filled row-wise.\n\n\nE1.R\n\n# Construct a matrix with 3 rows that contain the numbers 1 up to 9\nmatrix(1:9, byrow = TRUE, nrow = 3)"
  },
  {
    "objectID": "Introduction_to_R_C3.html#analyze-matrices-you-shall",
    "href": "Introduction_to_R_C3.html#analyze-matrices-you-shall",
    "title": "4  Matrices",
    "section": "4.2 Analyze matrices, you shall",
    "text": "4.2 Analyze matrices, you shall\nIt is now time to get your hands dirty. In the following exercises you will analyze the box office numbers of the Star Wars franchise. May the force be with you!\nIn the editor, three vectors are defined. Each one represents the box office numbers from the first three Star Wars movies. The first element of each vector indicates the US box office revenue, the second element refers to the Non-US box office (source: Wikipedia).\nIn this exercise, you’ll combine all these figures into a single vector. Next, you’ll build a matrix from this vector."
  },
  {
    "objectID": "Introduction_to_R_C3.html#instructions-100-xp-1",
    "href": "Introduction_to_R_C3.html#instructions-100-xp-1",
    "title": "4  Matrices",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUse c(new_hope, empire_strikes, return_jedi) to combine the three vectors into one vector. Call this vector box_office. Construct a matrix with 3 rows, where each row represents a movie. Use the matrix() function to do this. The first argument is the vector box_office, containing all box office figures. Next, you’ll have to specify nrow = 3 and byrow = TRUE. Name the resulting matrix star_wars_matrix.\n\n\nE2.R\n\n# Box office Star Wars (in millions!)\nnew_hope <- c(460.998, 314.4)\nempire_strikes <- c(290.475, 247.900)\nreturn_jedi <- c(309.306, 165.8)\n\n# Create box_office\nbox_office <- c(new_hope, empire_strikes, return_jedi)\n\n# Construct star_wars_matrix\nstar_wars_matrix <- matrix(box_office, nrow = 3, byrow = TRUE)"
  },
  {
    "objectID": "Introduction_to_R_C3.html#naming-a-matrix",
    "href": "Introduction_to_R_C3.html#naming-a-matrix",
    "title": "4  Matrices",
    "section": "4.3 Naming a matrix",
    "text": "4.3 Naming a matrix\nTo help you remember what is stored in star_wars_matrix, you would like to add the names of the movies for the rows. Not only does this help you to read the data, but it is also useful to select certain elements from the matrix.\nSimilar to vectors, you can add names for the rows and the columns of a matrix\n\nrownames(my_matrix) <- row_names_vector colnames(my_matrix) <- col_names_vector\n\nWe went ahead and prepared two vectors for you: region, and titles. You will need these vectors to name the columns and rows of star_wars_matrix, respectively."
  },
  {
    "objectID": "Introduction_to_R_C3.html#instructions-100-xp-2",
    "href": "Introduction_to_R_C3.html#instructions-100-xp-2",
    "title": "4  Matrices",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUse colnames() to name the columns of star_wars_matrix with the region vector. Use rownames() to name the rows of star_wars_matrix with the titles vector. Print out star_wars_matrix to see the result of your work.\n\n\nE3.R\n\n# Box office Star Wars (in millions!)\nnew_hope <- c(460.998, 314.4)\nempire_strikes <- c(290.475, 247.900)\nreturn_jedi <- c(309.306, 165.8)\n\n# Construct matrix\nstar_wars_matrix <- matrix(c(new_hope, empire_strikes, return_jedi), nrow = 3,\nbyrow = TRUE)\n\n# Vectors region and titles, used for naming\nregion <- c(\"US\", \"non-US\")\ntitles <- c(\"A New Hope\", \"The Empire Strikes Back\", \"Return of the Jedi\")\n\n# Name the columns with region\n\ncolnames(star_wars_matrix) <- region\n\n# Name the rows with titles\nrownames(star_wars_matrix) <- titles\n\n# Print out star_wars_matrix\nprint(star_wars_matrix)"
  },
  {
    "objectID": "Introduction_to_R_C3.html#calculating-the-worldwide-box-office",
    "href": "Introduction_to_R_C3.html#calculating-the-worldwide-box-office",
    "title": "4  Matrices",
    "section": "4.4 Calculating the worldwide box office",
    "text": "4.4 Calculating the worldwide box office\nThe single most important thing for a movie in order to become an instant legend in Tinseltown is its worldwide box office figures.\nTo calculate the total box office revenue for the three Star Wars movies, you have to take the sum of the US revenue column and the non-US revenue column.\nIn R, the function rowSums() conveniently calculates the totals for each row of a matrix. This function creates a new vector:\n\nrowSums(my_matrix)"
  },
  {
    "objectID": "Introduction_to_R_C3.html#instructions-100-xp-3",
    "href": "Introduction_to_R_C3.html#instructions-100-xp-3",
    "title": "4  Matrices",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nCalculate the worldwide box office figures for the three movies and put these in the vector named worldwide_vector.\n\n\nE4.R\n\n# Construct star_wars_matrix\nbox_office <- c(460.998, 314.4, 290.475, 247.900, 309.306, 165.8)\nregion <- c(\"US\", \"non-US\")\ntitles <- c(\"A New Hope\", \n                 \"The Empire Strikes Back\", \n                 \"Return of the Jedi\")\n               \nstar_wars_matrix <- matrix(box_office, \n                      nrow = 3, byrow = TRUE,\n                      dimnames = list(titles, region))\n\n# Calculate worldwide box office figures\nworldwide_vector <- rowSums(star_wars_matrix)"
  },
  {
    "objectID": "Introduction_to_R_C3.html#adding-a-column-for-the-worldwide-box-office",
    "href": "Introduction_to_R_C3.html#adding-a-column-for-the-worldwide-box-office",
    "title": "4  Matrices",
    "section": "4.5 Adding a column for the Worldwide box office",
    "text": "4.5 Adding a column for the Worldwide box office\nIn the previous exercise you calculated the vector that contained the worldwide box office receipt for each of the three Star Wars movies. However, this vector is not yet part of star_wars_matrix.\nYou can add a column or multiple columns to a matrix with the cbind() function, which merges matrices and/or vectors together by column. For example:\n\nbig_matrix <- cbind(matrix1, matrix2, vector1 …)"
  },
  {
    "objectID": "Introduction_to_R_C3.html#instructions-100-xp-4",
    "href": "Introduction_to_R_C3.html#instructions-100-xp-4",
    "title": "4  Matrices",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nAdd worldwide_vector as a new column to the star_wars_matrix and assign the result to all_wars_matrix. Use the cbind() function.\n\n\nE5.R\n\n# Construct star_wars_matrix\nbox_office <- c(460.998, 314.4, 290.475, 247.900, 309.306, 165.8)\nregion <- c(\"US\", \"non-US\")\ntitles <- c(\"A New Hope\", \n            \"The Empire Strikes Back\", \n            \"Return of the Jedi\")\n               \nstar_wars_matrix <- matrix(box_office, \n                      nrow = 3, byrow = TRUE,\n                      dimnames = list(titles, region))\n\n# The worldwide box office figures\nworldwide_vector <- rowSums(star_wars_matrix)\n\n# Bind the new variable worldwide_vector as a column to star_wars_matrix\nall_wars_matrix <- cbind(star_wars_matrix,worldwide_vector)"
  },
  {
    "objectID": "Introduction_to_R_C3.html#adding-a-row",
    "href": "Introduction_to_R_C3.html#adding-a-row",
    "title": "4  Matrices",
    "section": "4.6 Adding a row",
    "text": "4.6 Adding a row\nJust like every action has a reaction, every cbind() has an rbind(). (We admit, we are pretty bad with metaphors.)\nYour R workspace, where all variables you defined ‘live’ (check out what a workspace is), has already been initialized and contains two matrices:\n\nstar_wars_matrix that we have used all along, with data on the original trilogy,\nstar_wars_matrix2, with similar data for the prequels trilogy.\n\nExplore these matrices in the console if you want to have a closer look. If you want to check out the contents of the workspace, you can type ls() in the console."
  },
  {
    "objectID": "Introduction_to_R_C3.html#instructions-100-xp-5",
    "href": "Introduction_to_R_C3.html#instructions-100-xp-5",
    "title": "4  Matrices",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUse rbind() to paste together star_wars_matrix and star_wars_matrix2, in this order. Assign the resulting matrix to all_wars_matrix.\n\n\nE6.R\n\n# star_wars_matrix and star_wars_matrix2 are available in your workspace\nstar_wars_matrix  \nstar_wars_matrix2 \n\n# Combine both Star Wars trilogies in one matrix\nall_wars_matrix <- rbind(star_wars_matrix,star_wars_matrix2)"
  },
  {
    "objectID": "Introduction_to_R_C3.html#the-total-box-office-revenue-for-the-entire-saga",
    "href": "Introduction_to_R_C3.html#the-total-box-office-revenue-for-the-entire-saga",
    "title": "4  Matrices",
    "section": "4.7 The total box office revenue for the entire saga",
    "text": "4.7 The total box office revenue for the entire saga\nJust like cbind() has rbind(), colSums() has rowSums(). Your R workspace already contains the all_wars_matrix that you constructed in the previous exercise; type all_wars_matrix to have another look. Let’s now calculate the total box office revenue for the entire saga."
  },
  {
    "objectID": "Introduction_to_R_C3.html#instructions-100-xp-6",
    "href": "Introduction_to_R_C3.html#instructions-100-xp-6",
    "title": "4  Matrices",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nCalculate the total revenue for the US and the non-US region and assign total_revenue_vector. You can use the colSums() function. Print out total_revenue_vector to have a look at the results.\n\n\nE7.R\n\n# all_wars_matrix is available in your workspace\nall_wars_matrix\n\n# Total revenue for US and non-US\ntotal_revenue_vector <- colSums(all_wars_matrix)\n  \n# Print out total_revenue_vector\nprint(total_revenue_vector)"
  },
  {
    "objectID": "Introduction_to_R_C3.html#selection-of-matrix-elements",
    "href": "Introduction_to_R_C3.html#selection-of-matrix-elements",
    "title": "4  Matrices",
    "section": "4.8 Selection of matrix elements",
    "text": "4.8 Selection of matrix elements\nSimilar to vectors, you can use the square brackets [ ] to select one or multiple elements from a matrix. Whereas vectors have one dimension, matrices have two dimensions. You should therefore use a comma to separate the rows you want to select from the columns. For example:\n\nmy_matrix[1,2] selects the element at the first row and second column. *my_matrix[1:3,2:4] results in a matrix with the data on the rows 1, 2, 3 and columns 2, 3, 4.\n\nIf you want to select all elements of a row or a column, no number is needed before or after the comma, respectively:\n\nmy_matrix[,1] selects all elements of the first column. *my_matrix[1,] selects all elements of the first row.\n\nBack to Star Wars with this newly acquired knowledge! As in the previous exercise, all_wars_matrix is already available in your workspace."
  },
  {
    "objectID": "Introduction_to_R_C3.html#instructions-100-xp-7",
    "href": "Introduction_to_R_C3.html#instructions-100-xp-7",
    "title": "4  Matrices",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nSelect the non-US revenue for all movies (the entire second column of all_wars_matrix), store the result as non_us_all. Use mean() on non_us_all to calculate the average non-US revenue for all movies. Simply print out the result. This time, select the non-US revenue for the first two movies in all_wars_matrix. Store the result as non_us_some. Use mean() again to print out the average of the values in non_us_some.\n\n\nE8.R\n\n# all_wars_matrix is available in your workspace\nall_wars_matrix\n\n# Select the non-US revenue for all movies\nnon_us_all <- all_wars_matrix[,2] \n  \n# Average non-US revenue\nmean(all_wars_matrix[,2])\n  \n# Select the non-US revenue for first two movies\nnon_us_some <- all_wars_matrix[1:2,2] \n  \n# Average non-US revenue for first two movies\nmean(all_wars_matrix[1:2,2])"
  },
  {
    "objectID": "Introduction_to_R_C3.html#a-little-arithmetic-with-matrices",
    "href": "Introduction_to_R_C3.html#a-little-arithmetic-with-matrices",
    "title": "4  Matrices",
    "section": "4.9 A little arithmetic with matrices",
    "text": "4.9 A little arithmetic with matrices\nSimilar to what you have learned with vectors, the standard operators like +, -, /, *, etc. work in an element-wise way on matrices in R.\nFor example, 2 * my_matrix multiplies each element of my_matrix by two.\nAs a newly-hired data analyst for Lucasfilm, it is your job to find out how many visitors went to each movie for each geographical area. You already have the total revenue figures in all_wars_matrix. Assume that the price of a ticket was 5 dollars. Simply dividing the box office numbers by this ticket price gives you the number of visitors."
  },
  {
    "objectID": "Introduction_to_R_C3.html#instructions-100-xp-8",
    "href": "Introduction_to_R_C3.html#instructions-100-xp-8",
    "title": "4  Matrices",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nDivide all_wars_matrix by 5, giving you the number of visitors in millions. Assign the resulting matrix to visitors. Print out visitors so you can have a look.\n\n\nE9.R\n\n# all_wars_matrix is available in your workspace\nall_wars_matrix\n\n# Estimate the visitors\nvisitors <- all_wars_matrix/5\n\n  \n# Print the estimate to the console\nprint(visitors)"
  },
  {
    "objectID": "Introduction_to_R_C3.html#a-little-arithmetic-with-matrices-2",
    "href": "Introduction_to_R_C3.html#a-little-arithmetic-with-matrices-2",
    "title": "4  Matrices",
    "section": "4.10 A little arithmetic with matrices (2)",
    "text": "4.10 A little arithmetic with matrices (2)\nJust like 2 * my_matrix multiplied every element of my_matrix by two, my_matrix1 * my_matrix2 creates a matrix where each element is the product of the corresponding elements in my_matrix1 and my_matrix2.\nAfter looking at the result of the previous exercise, big boss Lucas points out that the ticket prices went up over time. He asks to redo the analysis based on the prices you can find in ticket_prices_matrix (source: imagination).\nThose who are familiar with matrices should note that this is not the standard matrix multiplication for which you should use %*% in R."
  },
  {
    "objectID": "Introduction_to_R_C3.html#instructions-100-xp-9",
    "href": "Introduction_to_R_C3.html#instructions-100-xp-9",
    "title": "4  Matrices",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nDivide all_wars_matrix by ticket_prices_matrix to get the estimated number of US and non-US visitors for the six movies. Assign the result to visitors. From the visitors matrix, select the entire first column, representing the number of visitors in the US. Store this selection as us_visitors. Calculate the average number of US visitors; print out the result.\n\n\nE10.R\n\n# all_wars_matrix and ticket_prices_matrix are available in your workspace\nall_wars_matrix\nticket_prices_matrix\n\n# Estimated number of visitors\nvisitors <- all_wars_matrix/ticket_prices_matrix\n\n# US visitors\nus_visitors <- visitors[,1]\n\n# Average number of US visitors\nmean(us_visitors)"
  },
  {
    "objectID": "Introduction_to_R_C4.html#whats-a-factor-and-why-would-you-use-it",
    "href": "Introduction_to_R_C4.html#whats-a-factor-and-why-would-you-use-it",
    "title": "5  Factors",
    "section": "5.1 What’s a factor and why would you use it?",
    "text": "5.1 What’s a factor and why would you use it?\nIn this chapter you dive into the wonderful world of factors.\nThe term factor refers to a statistical data type used to store categorical variables. The difference between a categorical variable and a continuous variable is that a categorical variable can belong to a limited number of categories. A continuous variable, on the other hand, can correspond to an infinite number of values.\nIt is important that R knows whether it is dealing with a continuous or a categorical variable, as the statistical models you will develop in the future treat both types differently. (You will see later why this is the case.)\nA good example of a categorical variable is sex. In many circumstances you can limit the sex categories to “Male” or “Female”. (Sometimes you may need different categories. For example, you may need to consider chromosomal variation, hermaphroditic animals, or different cultural norms, but you will always have a finite number of categories.)"
  },
  {
    "objectID": "Introduction_to_R_C4.html#instructions-100-xp",
    "href": "Introduction_to_R_C4.html#instructions-100-xp",
    "title": "5  Factors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nAssign to variable theory the value “factors”.\n\n\nE1.R\n\n# Assign to the variable theory what this chapter is about!\ntheory <- \"factors\""
  },
  {
    "objectID": "Introduction_to_R_C4.html#whats-a-factor-and-why-would-you-use-it-2",
    "href": "Introduction_to_R_C4.html#whats-a-factor-and-why-would-you-use-it-2",
    "title": "5  Factors",
    "section": "5.2 What’s a factor and why would you use it? (2)",
    "text": "5.2 What’s a factor and why would you use it? (2)\nTo create factors in R, you make use of the function factor(). First thing that you have to do is create a vector that contains all the observations that belong to a limited number of categories. For example, sex_vector contains the sex of 5 different individuals:\n\nsex_vector <- c(“Male”,“Female”,“Female”,“Male”,“Male”)\n\nIt is clear that there are two categories, or in R-terms ‘factor levels’, at work here: “Male” and “Female”.\nThe function factor() will encode the vector as a factor:\n\nfactor_sex_vector <- factor(sex_vector)"
  },
  {
    "objectID": "Introduction_to_R_C4.html#instructions-100-xp-1",
    "href": "Introduction_to_R_C4.html#instructions-100-xp-1",
    "title": "5  Factors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nConvert the character vector sex_vector to a factor with factor() and assign the result to factor_sex_vector Print out factor_sex_vector and assert that R prints out the factor levels below the actual values.\n\n\nE2.R\n\n# Sex vector\nsex_vector <- c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\")\n\n# Convert sex_vector to a factor\nfactor_sex_vector <- factor(sex_vector)\n\n# Print out factor_sex_vector\nprint(factor_sex_vector)"
  },
  {
    "objectID": "Introduction_to_R_C4.html#whats-a-factor-and-why-would-you-use-it-3",
    "href": "Introduction_to_R_C4.html#whats-a-factor-and-why-would-you-use-it-3",
    "title": "5  Factors",
    "section": "5.3 What’s a factor and why would you use it? (3)",
    "text": "5.3 What’s a factor and why would you use it? (3)\nThere are two types of categorical variables: a nominal categorical variable and an ordinal categorical variable.\nA nominal variable is a categorical variable without an implied order. This means that it is impossible to say that ‘one is worth more than the other’. For example, think of the categorical variable animals_vector with the categories “Elephant”, “Giraffe”, “Donkey” and “Horse”. Here, it is impossible to say that one stands above or below the other. (Note that some of you might disagree ;-) ).\nIn contrast, ordinal variables do have a natural ordering. Consider for example the categorical variable temperature_vector with the categories: “Low”, “Medium” and “High”. Here it is obvious that “Medium” stands above “Low”, and “High” stands above “Medium”."
  },
  {
    "objectID": "Introduction_to_R_C4.html#instructions-100-xp-2",
    "href": "Introduction_to_R_C4.html#instructions-100-xp-2",
    "title": "5  Factors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nSubmit the answer to check how R constructs and prints nominal and ordinal variables. Do not worry if you do not understand all the code just yet, we will get to that.\n\n\nE3.R\n\n# Animals\nanimals_vector <- c(\"Elephant\", \"Giraffe\", \"Donkey\", \"Horse\")\nfactor_animals_vector <- factor(animals_vector)\nfactor_animals_vector\n\n# Temperature\ntemperature_vector <- c(\"High\", \"Low\", \"High\",\"Low\", \"Medium\")\nfactor_temperature_vector <- factor(temperature_vector, order = TRUE,\n  levels = c(\"Low\", \"Medium\", \"High\"))\nfactor_temperature_vector"
  },
  {
    "objectID": "Introduction_to_R_C4.html#factor-levels",
    "href": "Introduction_to_R_C4.html#factor-levels",
    "title": "5  Factors",
    "section": "5.4 Factor levels",
    "text": "5.4 Factor levels\nWhen you first get a dataset, you will often notice that it contains factors with specific factor levels. However, sometimes you will want to change the names of these levels for clarity or other reasons. R allows you to do this with the function levels():\n\nlevels(factor_vector) <- c(“name1”, “name2”,…)\n\nA good illustration is the raw data that is provided to you by a survey. A common question for every questionnaire is the sex of the respondent. Here, for simplicity, just two categories were recorded, “M” and “F”. (You usually need more categories for survey data; either way, you use a factor to store the categorical data.)\n\nsurvey_vector <- c(“M”, “F”, “F”, “M”, “M”)\n\nRecording the sex with the abbreviations “M” and “F” can be convenient if you are collecting data with pen and paper, but it can introduce confusion when analyzing the data. At that point, you will often want to change the factor levels to “Male” and “Female” instead of “M” and “F” for clarity.\nWatch out: the order with which you assign the levels is important. If you type levels(factor_survey_vector), you’ll see that it outputs [1] “F” “M”. If you don’t specify the levels of the factor when creating the vector, R will automatically assign them alphabetically. To correctly map “F” to “Female” and “M” to “Male”, the levels should be set to c(“Female”, “Male”), in this order."
  },
  {
    "objectID": "Introduction_to_R_C4.html#instructions-100-xp-3",
    "href": "Introduction_to_R_C4.html#instructions-100-xp-3",
    "title": "5  Factors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nCheck out the code that builds a factor vector from survey_vector. You should use factor_survey_vector in the next instruction. Change the factor levels of factor_survey_vector to c(“Female”, “Male”). Mind the order of the vector elements here.\n\n\nE4.R\n\n# Code to build factor_survey_vector\nsurvey_vector <- c(\"M\", \"F\", \"F\", \"M\", \"M\")\nfactor_survey_vector <- factor(survey_vector)\n\n# Specify the levels of factor_survey_vector\nlevels(factor_survey_vector) <- c(\"Female\", \"Male\")\n\nfactor_survey_vector"
  },
  {
    "objectID": "Introduction_to_R_C4.html#summarizing-a-factor",
    "href": "Introduction_to_R_C4.html#summarizing-a-factor",
    "title": "5  Factors",
    "section": "5.5 Summarizing a factor",
    "text": "5.5 Summarizing a factor\nAfter finishing this course, one of your favorite functions in R will be summary(). This will give you a quick overview of the contents of a variable:\nsummary(my_var) Going back to our survey, you would like to know how many “Male” responses you have in your study, and how many “Female” responses. The summary() function gives you the answer to this question."
  },
  {
    "objectID": "Introduction_to_R_C4.html#instructions-100-xp-4",
    "href": "Introduction_to_R_C4.html#instructions-100-xp-4",
    "title": "5  Factors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nAsk a summary() of the survey_vector and factor_survey_vector. Interpret the results of both vectors. Are they both equally useful in this case?\n\n\nE5.R\n\n# Build factor_survey_vector with clean levels\nsurvey_vector <- c(\"M\", \"F\", \"F\", \"M\", \"M\")\nfactor_survey_vector <- factor(survey_vector)\nlevels(factor_survey_vector) <- c(\"Female\", \"Male\")\nfactor_survey_vector\n\n# Generate summary for survey_vector\nsummary(survey_vector)\n\n# Generate summary for factor_survey_vector\nsummary(factor_survey_vector)"
  },
  {
    "objectID": "Introduction_to_R_C4.html#battle-of-the-sexes",
    "href": "Introduction_to_R_C4.html#battle-of-the-sexes",
    "title": "5  Factors",
    "section": "5.6 Battle of the sexes",
    "text": "5.6 Battle of the sexes\nYou might wonder what happens when you try to compare elements of a factor. In factor_survey_vector you have a factor with two levels: “Male” and “Female”. But how does R value these relative to each other?"
  },
  {
    "objectID": "Introduction_to_R_C4.html#instructions-100-xp-5",
    "href": "Introduction_to_R_C4.html#instructions-100-xp-5",
    "title": "5  Factors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nRead the code in the editor and submit the answer to test if male is greater than (>) female.\n\n\nE6.R\n\n# Build factor_survey_vector with clean levels\nsurvey_vector <- c(\"M\", \"F\", \"F\", \"M\", \"M\")\nfactor_survey_vector <- factor(survey_vector)\nlevels(factor_survey_vector) <- c(\"Female\", \"Male\")\n\n# Male\nmale <- factor_survey_vector[1]\n\n# Female\nfemale <- factor_survey_vector[2]\n\n# Battle of the sexes: Male 'larger' than female?\nmale > female"
  },
  {
    "objectID": "Introduction_to_R_C4.html#ordered-factors",
    "href": "Introduction_to_R_C4.html#ordered-factors",
    "title": "5  Factors",
    "section": "5.7 Ordered factors",
    "text": "5.7 Ordered factors\nSince “Male” and “Female” are unordered (or nominal) factor levels, R returns a warning message, telling you that the greater than operator is not meaningful. As seen before, R attaches an equal value to the levels for such factors.\nBut this is not always the case! Sometimes you will also deal with factors that do have a natural ordering between its categories. If this is the case, we have to make sure that we pass this information to R…\nLet us say that you are leading a research team of five data analysts and that you want to evaluate their performance. To do this, you track their speed, evaluate each analyst as “slow”, “medium” or “fast”, and save the results in speed_vector."
  },
  {
    "objectID": "Introduction_to_R_C4.html#instructions-100-xp-6",
    "href": "Introduction_to_R_C4.html#instructions-100-xp-6",
    "title": "5  Factors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nAs a first step, assign speed_vector a vector with 5 entries, one for each analyst. Each entry should be either “slow”, “medium”, or “fast”. Use the list below:\n\nAnalyst 1 is medium,\nAnalyst 2 is slow,\nAnalyst 3 is slow,\nAnalyst 4 is medium and\nAnalyst 5 is fast.\n\nNo need to specify these are factors yet.\n\n\nE7.R\n\n# Create speed_vector\n\nspeed_vector <- c(\"medium\", \"slow\", \"slow\", \"medium\", \"fast\")"
  },
  {
    "objectID": "Introduction_to_R_C4.html#ordered-factors-2",
    "href": "Introduction_to_R_C4.html#ordered-factors-2",
    "title": "5  Factors",
    "section": "5.8 Ordered factors (2)",
    "text": "5.8 Ordered factors (2)\nspeed_vector should be converted to an ordinal factor since its categories have a natural ordering. By default, the function factor() transforms speed_vector into an unordered factor. To create an ordered factor, you have to add two additional arguments: ordered and levels.\n\nfactor(some_vector, ordered = TRUE, levels = c(“lev1”, “lev2” …))\n\nBy setting the argument ordered to TRUE in the function factor(), you indicate that the factor is ordered. With the argument levels you give the values of the factor in the correct order."
  },
  {
    "objectID": "Introduction_to_R_C4.html#instructions-100-xp-7",
    "href": "Introduction_to_R_C4.html#instructions-100-xp-7",
    "title": "5  Factors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFrom speed_vector, create an ordered factor vector: factor_speed_vector. Set ordered to TRUE, and set levels to c(“slow”, “medium”, “fast”).\n\n\nE8.R\n\n# Create speed_vector\nspeed_vector <- c(\"medium\", \"slow\", \"slow\", \"medium\", \"fast\")\n\n# Convert speed_vector to ordered factor vector\nfactor_speed_vector <- factor(speed_vector,\n    ordered = TRUE,\n    levels = c (\"slow\", \"medium\", \"fast\" ))\n\n# Print factor_speed_vector\nfactor_speed_vector\nsummary(factor_speed_vector)"
  },
  {
    "objectID": "Introduction_to_R_C4.html#comparing-ordered-factors",
    "href": "Introduction_to_R_C4.html#comparing-ordered-factors",
    "title": "5  Factors",
    "section": "5.9 Comparing ordered factors",
    "text": "5.9 Comparing ordered factors\nHaving a bad day at work, ‘data analyst number two’ enters your office and starts complaining that ‘data analyst number five’ is slowing down the entire project. Since you know that ‘data analyst number two’ has the reputation of being a smarty-pants, you first decide to check if his statement is true.\nThe fact that factor_speed_vector is now ordered enables us to compare different elements (the data analysts in this case). You can simply do this by using the well-known operators."
  },
  {
    "objectID": "Introduction_to_R_C4.html#instructions-100-xp-8",
    "href": "Introduction_to_R_C4.html#instructions-100-xp-8",
    "title": "5  Factors",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse [2] to select from factor_speed_vector the factor value for the second data analyst. Store it as da2.\nUse [5] to select the factor_speed_vector factor value for the fifth data analyst. Store it as da5.\nCheck if da2 is greater than da5; simply print out the result. Remember that you can use the > operator to check whether one element is larger than the other.\n\n\n\nE9.R\n\n# Create factor_speed_vector\nspeed_vector <- c(\"medium\", \"slow\", \"slow\", \"medium\", \"fast\")\nfactor_speed_vector <- factor(speed_vector, ordered = TRUE, levels = c(\"slow\",\n\"medium\", \"fast\"))\n\n# Factor value for second data analyst\nda2 <- factor_speed_vector[2]\n\n# Factor value for fifth data analyst\nda5 <- factor_speed_vector[5]\n\n# Is data analyst 2 faster than data analyst 5?\nda2 >da5"
  },
  {
    "objectID": "Introduction_to_R_C5.html#sorting-your-data-frame",
    "href": "Introduction_to_R_C5.html#sorting-your-data-frame",
    "title": "6  Lists",
    "section": "6.1 Sorting your data frame",
    "text": "6.1 Sorting your data frame\nAlright, now that you understand the order() function, let us do something useful with it. You would like to rearrange your data frame such that it starts with the smallest planet and ends with the largest one. A sort on the diameter column."
  },
  {
    "objectID": "Introduction_to_R_C5.html#instructions-100-xp",
    "href": "Introduction_to_R_C5.html#instructions-100-xp",
    "title": "6  Lists",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nCall order() on planets_df$diameter (the diameter column of planets_df). Store the result as positions. Now reshuffle planets_df with the positions vector as row indexes inside square brackets. Keep all columns. Simply print out the result.\n\n\nE1.R\n\n# planets_df is pre-loaded in your workspace\norder(planets_df$diameter)\n# Use order() to create positions\npositions <-  order(planets_df$diameter)\n\n# Use positions to sort planets_df\nplanets_df[positions, ]"
  },
  {
    "objectID": "Introduction_to_R_C5.html#lists-why-would-you-need-them",
    "href": "Introduction_to_R_C5.html#lists-why-would-you-need-them",
    "title": "6  Lists",
    "section": "6.2 Lists, why would you need them?",
    "text": "6.2 Lists, why would you need them?\nCongratulations! At this point in the course you are already familiar with:\nVectors (one dimensional array): can hold numeric, character or logical values. The elements in a vector all have the same data type. Matrices (two dimensional array): can hold numeric, character or logical values. The elements in a matrix all have the same data type. Data frames (two-dimensional objects): can hold numeric, character or logical values. Within a column all elements have the same data type, but different columns can be of different data type. Pretty sweet for an R newbie, right? ;-)"
  },
  {
    "objectID": "Introduction_to_R_C5.html#instructions-100-xp-1",
    "href": "Introduction_to_R_C5.html#instructions-100-xp-1",
    "title": "6  Lists",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nSubmit the answer to start learning everything about lists!\n\n\nE2.R\n\n# Just submit the answer\n#vector\nx <- c(1, 2, 3)\ny <-c(7,8,5)\nz <- c(\"dragon\", \"quinera\", \"leviatan\")\n#matrix\nmatrix <- cbind(x,y)\n\n#data frame\ndf <- data.frame(x,y,z)\n\nprint(df)"
  },
  {
    "objectID": "Introduction_to_R_C5.html#lists-why-would-you-need-them-2",
    "href": "Introduction_to_R_C5.html#lists-why-would-you-need-them-2",
    "title": "6  Lists",
    "section": "6.3 Lists, why would you need them? (2)",
    "text": "6.3 Lists, why would you need them? (2)\nA list in R is similar to your to-do list at work or school: the different items on that list most likely differ in length, characteristic, and type of activity that has to be done.\nA list in R allows you to gather a variety of objects under one name (that is, the name of the list) in an ordered way. These objects can be matrices, vectors, data frames, even other lists, etc. It is not even required that these objects are related to each other in any way.\nYou could say that a list is some kind super data type: you can store practically any piece of information in it!"
  },
  {
    "objectID": "Introduction_to_R_C5.html#instructions-100-xp-2",
    "href": "Introduction_to_R_C5.html#instructions-100-xp-2",
    "title": "6  Lists",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nJust submit the answer to start the first exercise on lists.\n\n\nE3.R\n\n# Just submit the answer to start the first exercise on lists.\ndf <- data.frame(x = c(1, 2, 3), y = c(\"dragon\", \"grifo\", \"wyver\"))\nlista <- list(df)"
  },
  {
    "objectID": "Introduction_to_R_C5.html#creating-a-list",
    "href": "Introduction_to_R_C5.html#creating-a-list",
    "title": "6  Lists",
    "section": "6.4 Creating a list",
    "text": "6.4 Creating a list\nLet us create our first list! To construct a list you use the function list():\nmy_list <- list(comp1, comp2 …) The arguments to the list function are the list components. Remember, these components can be matrices, vectors, other lists, …"
  },
  {
    "objectID": "Introduction_to_R_C5.html#instructions-100-xp-3",
    "href": "Introduction_to_R_C5.html#instructions-100-xp-3",
    "title": "6  Lists",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nConstruct a list, named my_list, that contains the variables my_vector, my_matrix and my_df as list components.\n\n\nE4.R\n\n# Vector with numerics from 1 up to 10\nmy_vector <- 1:10 \n\n# Matrix with numerics from 1 up to 9\nmy_matrix <- matrix(1:9, ncol = 3)\n\n# First 10 elements of the built-in data frame mtcars\nmy_df <- mtcars[1:10,]\n\n# Construct list with these different elements:\nmy_list <-list(my_vector,my_matrix,my_df)"
  },
  {
    "objectID": "Introduction_to_R_C5.html#creating-a-named-list",
    "href": "Introduction_to_R_C5.html#creating-a-named-list",
    "title": "6  Lists",
    "section": "6.5 Creating a named list",
    "text": "6.5 Creating a named list\nWell done, you’re on a roll!\nJust like on your to-do list, you want to avoid not knowing or remembering what the components of your list stand for. That is why you should give names to them:\n\nmy_list <- list(name1 = your_comp1, name2 = your_comp2)\n\nThis creates a list with components that are named name1, name2, and so on. If you want to name your lists after you’ve created them, you can use the names() function as you did with vectors. The following commands are fully equivalent to the assignment above:\n\nmy_list <- list(your_comp1, your_comp2) names(my_list) <- c(“name1”, “name2”)"
  },
  {
    "objectID": "Introduction_to_R_C5.html#instructions-100-xp-4",
    "href": "Introduction_to_R_C5.html#instructions-100-xp-4",
    "title": "6  Lists",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nChange the code of the previous exercise (see editor) by adding names to the components. Use for my_vector the name vec, for my_matrix the name mat and for my_df the name df. Print out my_list so you can inspect the output.\n\n\nE5.R\n\n# Vector with numerics from 1 up to 10\nmy_vector <- 1:10 \n\n# Matrix with numerics from 1 up to 9\nmy_matrix <- matrix(1:9, ncol = 3)\n\n# First 10 elements of the built-in data frame mtcars\nmy_df <- mtcars[1:10,]\n\n# Adapt list() call to give the components names\nmy_list <- list(vec = my_vector,\n                mat = my_matrix,\n                df= my_df\n                )\n\n# Print out my_list\n\nprint(my_list)"
  },
  {
    "objectID": "Introduction_to_R_C5.html#creating-a-named-list-2",
    "href": "Introduction_to_R_C5.html#creating-a-named-list-2",
    "title": "6  Lists",
    "section": "6.6 Creating a named list (2)",
    "text": "6.6 Creating a named list (2)\nBeing a huge movie fan (remember your job at LucasFilms), you decide to start storing information on good movies with the help of lists.\nStart by creating a list for the movie “The Shining”. We have already created the variables mov, act and rev in your R workspace. Feel free to check them out in the console.\nInstructions 100 XP\nComplete the code in the editor to create shining_list; it contains three elements:\n\nmoviename: a character string with the movie title (stored in mov)\nactors: a vector with the main actors’ names (stored in act)\nreviews: a data frame that contains some reviews (stored in rev)\n\nDo not forget to name the list components accordingly (names are moviename, actors and reviews).\n\n\nE6.R\n\n# The variables mov, act and rev are available\n\n# Finish the code to build shining_list\n\nshining_list <- list(moviename = mov, actors = act, reviews = rev)"
  },
  {
    "objectID": "Introduction_to_R_C5.html#selecting-elements-from-a-list",
    "href": "Introduction_to_R_C5.html#selecting-elements-from-a-list",
    "title": "6  Lists",
    "section": "6.7 Selecting elements from a list",
    "text": "6.7 Selecting elements from a list\nYour list will often be built out of numerous elements and components. Therefore, getting a single element, multiple elements, or a component out of it is not always straightforward.\nOne way to select a component is using the numbered position of that component. For example, to “grab” the first component of shining_list you type\n\nshining_list[[1]]\n\nA quick way to check this out is typing it in the console. Important to remember: to select elements from vectors, you use single square brackets: [ ]. Don’t mix them up!\nYou can also refer to the names of the components, with [[ ]] or with the $ sign. Both will select the data frame representing the reviews:\n\nshining_list[[“reviews”]] shining_list$reviews\n\nBesides selecting components, you often need to select specific elements out of these components. For example, with shining_list[[2]][1] you select from the second component, actors (shining_list[[2]]), the first element ([1]). When you type this in the console, you will see the answer is Jack Nicholson."
  },
  {
    "objectID": "Introduction_to_R_C5.html#instructions-100-xp-5",
    "href": "Introduction_to_R_C5.html#instructions-100-xp-5",
    "title": "6  Lists",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nSelect from shining_list the vector representing the actors. Simply print out this vector. Select from shining_list the second element in the vector representing the actors. Do a printout like before.\n\n\nE7.R\n\n# shining_list is already pre-loaded in the workspace\n\n# Print out the vector representing the actors\nprint (shining_list$actors)\n\n# Print the second element of the vector representing the actors\nprint(shining_list$actors[2])"
  },
  {
    "objectID": "Introduction_to_R_C5.html#creating-a-new-list-for-another-movie",
    "href": "Introduction_to_R_C5.html#creating-a-new-list-for-another-movie",
    "title": "6  Lists",
    "section": "6.8 Creating a new list for another movie",
    "text": "6.8 Creating a new list for another movie\nYou found reviews of another, more recent, Jack Nicholson movie: The Departed!\n\nScores Comments 4.6 I would watch it again 5 Amazing! 4.8 I liked it 5 One of the best movies 4.2 Fascinating plot\n\nIt would be useful to collect together all the pieces of information about the movie, like the title, actors, and reviews into a single variable. Since these pieces of data are different shapes, it is natural to combine them in a list variable.\nmovie_title, containing the title of the movie, and movie_actors, containing the names of some of the actors in the movie, are available in your workspace."
  },
  {
    "objectID": "Introduction_to_R_C5.html#instructions-100-xp-6",
    "href": "Introduction_to_R_C5.html#instructions-100-xp-6",
    "title": "6  Lists",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nCreate two vectors, called scores and comments, that contain the information from the reviews shown in the table. Find the average of the scores vector and save it as avg_review. Combine the scores and comments vectors into a data frame called reviews_df. Create a list, called departed_list, that contains the movie_title, movie_actors, reviews data frame as reviews_df, and the average review score as avg_review, and print it out.\n\n\nE68.R\n\n# Use the table from the exercise to define the comments and scores vectors\nscores <- c(4.6, 5, 4.8, 5, 4.2)\ncomments <- c(\"I would watch it again\", \"Amazing!\", \"I liked it\", \"One of \nthe best movies\", \"Fascinating plot\")\n\n# Save the average of the scores vector as avg_review\navg_review <- mean(scores)\n\n# Combine scores and comments into the reviews_df data frame\nreviews_df <-data.frame(scores,comments)\n\n# Create and print out a list, called departed_list\ndeparted_list <- list(movie_title, movie_actors, reviews_df, avg_review)\nprint(departed_list)"
  },
  {
    "objectID": "Intermedio_R_C1.html#equality",
    "href": "Intermedio_R_C1.html#equality",
    "title": "7  Conditionals and Control Flow",
    "section": "7.1 Equality",
    "text": "7.1 Equality\nThe most basic form of comparison is equality. Let’s briefly recap its syntax. The following statements all evaluate to TRUE (feel free to try them out in the console).\n\n3 == (2 + 1) “intermediate” != “r” TRUE != FALSE “Rchitect” != “rchitect”\n\nNotice from the last expression that R is case sensitive: “R” is not equal to “r”. Keep this in mind when solving the exercises in this chapter!"
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp",
    "href": "Intermedio_R_C1.html#instructions-100-xp",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nIn the editor on the right, write R code to see if TRUE equals FALSE. Likewise, check if -6 * 14 is not equal to 17 - 101. Next up: comparison of character strings. Ask R whether the strings “useR” and “user” are equal. Finally, find out what happens if you compare logicals to numerics: are TRUE and 1 equal?\n\n\nE1.R\n\n# Comparison of logicals\nTRUE == FALSE\n# Comparison of numerics\n-6 * 14 != 17 - 101\n\n# Comparison of character strings\n\"useR\" == \"user\"\n\n# Compare a logical with a numeric\nTRUE == 1"
  },
  {
    "objectID": "Intermedio_R_C1.html#greater-and-less-than",
    "href": "Intermedio_R_C1.html#greater-and-less-than",
    "title": "7  Conditionals and Control Flow",
    "section": "7.2 Greater and less than",
    "text": "7.2 Greater and less than\nApart from equality operators, Filip also introduced the less than and greater than operators: < and >. You can also add an equal sign to express less than or equal to or greater than or equal to, respectively. Have a look at the following R expressions, that all evaluate to FALSE:\n\n(1 + 2) > 4 “dog” < “Cats” TRUE <= FALSE\n\nRemember that for string comparison, R determines the greater than relationship based on alphabetical order. Also, keep in mind that TRUE is treated as 1 for arithmetic, and FALSE is treated as 0. Therefore, FALSE < TRUE is TRUE."
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp-1",
    "href": "Intermedio_R_C1.html#instructions-100-xp-1",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nWrite R expressions to check whether:\n\n-6 * 5 + 2 is greater than or equal to -10 + 1.\n“raining” is less than or equal to “raining dogs”.\nTRUE is greater than FALSE.\n\n\n\nE2.R\n\n# Comparison of numerics\n-6 * 5 + 2 >= -10 + 1\n\n# Comparison of character strings\n\n\"raining\" <= \"raining dogs\"\n# Comparison of logicals\nTRUE > FALSE"
  },
  {
    "objectID": "Intermedio_R_C1.html#compare-vectors",
    "href": "Intermedio_R_C1.html#compare-vectors",
    "title": "7  Conditionals and Control Flow",
    "section": "7.3 Compare vectors",
    "text": "7.3 Compare vectors\nYou are already aware that R is very good with vectors. Without having to change anything about the syntax, R’s relational operators also work on vectors.\nLet’s go back to the example that was started in the video. You want to figure out whether your activity on social media platforms have paid off and decide to look at your results for LinkedIn and Facebook. The sample code in the editor initializes the vectors linkedin and facebook. Each of the vectors contains the number of profile views your LinkedIn and Facebook profiles had over the last seven days."
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp-2",
    "href": "Intermedio_R_C1.html#instructions-100-xp-2",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing relational operators, find a logical answer, i.e. TRUE or FALSE, for the following questions:\n\nOn which days did the number of LinkedIn profile views exceed 15?\nWhen was your LinkedIn profile viewed only 5 times or fewer?\nWhen was your LinkedIn profile visited more often than your Facebook profile?\n\n\n\nE3.R\n\n# The linkedin and facebook vectors have already been created for you\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# Popular days\n\nlinkedin > 15\n# Quiet days\n\nlinkedin <= 5\n# LinkedIn more popular than Facebook\nlinkedin > facebook"
  },
  {
    "objectID": "Intermedio_R_C1.html#compare-matrices",
    "href": "Intermedio_R_C1.html#compare-matrices",
    "title": "7  Conditionals and Control Flow",
    "section": "7.4 Compare matrices",
    "text": "7.4 Compare matrices\nR’s ability to deal with different data structures for comparisons does not stop at vectors. Matrices and relational operators also work together seamlessly!\nInstead of in vectors (as in the previous exercise), the LinkedIn and Facebook data is now stored in a matrix called views. The first row contains the LinkedIn information; the second row the Facebook information. The original vectors facebook and linkedin are still available as well."
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp-3",
    "href": "Intermedio_R_C1.html#instructions-100-xp-3",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the relational operators you’ve learned so far, try to discover the following:\nWhen were the views exactly equal to 13? Use the views matrix to return a logical matrix. For which days were the number of views less than or equal to 14? Again, have R return a logical matrix.\n\n\nE4.R\n\n# The social data has been created for you\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\nviews <- matrix(c(linkedin, facebook), nrow = 2, byrow = TRUE)\n\n# When does views equal 13?\nviews == 13\n\n# When is views less than or equal to 14?\n\nviews <= 14"
  },
  {
    "objectID": "Intermedio_R_C1.html#and",
    "href": "Intermedio_R_C1.html#and",
    "title": "7  Conditionals and Control Flow",
    "section": "7.5 & and |",
    "text": "7.5 & and |\nBefore you work your way through the next exercises, have a look at the following R expressions. All of them will evaluate to TRUE:\n\nTRUE & TRUE FALSE | TRUE 5 <= 5 & 2 < 3 3 < 4 | 7 < 6\n\nWatch out: 3 < x < 7 to check if x is between 3 and 7 will not work; you’ll need 3 < x & x < 7 for that.\nIn this exercise, you’ll be working with the last variable. This variable equals the last value of the linkedin vector that you’ve worked with previously. The linkedin vector represents the number of LinkedIn views your profile had in the last seven days, remember? Both the variables linkedin and last have been pre-defined for you."
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp-4",
    "href": "Intermedio_R_C1.html#instructions-100-xp-4",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nWrite R expressions to solve the following questions concerning the variable last:\n\nIs last under 5 or above 10?\nIs last between 15 and 20, excluding 15 but including 20?\n\n\n\nE5.R\n\n# The linkedin and last variable are already defined for you\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nlast <- tail(linkedin, 1)\n\n# Is last under 5 or above 10?\n5 < last | last < 10\n\n# Is last between 15 (exclusive) and 20 (inclusive)?\n15 <= last & last < 20"
  },
  {
    "objectID": "Intermedio_R_C1.html#and-2",
    "href": "Intermedio_R_C1.html#and-2",
    "title": "7  Conditionals and Control Flow",
    "section": "7.6 & and | (2)",
    "text": "7.6 & and | (2)\nLike relational operators, logical operators work perfectly fine with vectors and matrices.\nBoth the vectors linkedin and facebook are available again. Also a matrix - views - has been defined; its first and second row correspond to the linkedin and facebook vectors, respectively. Ready for some advanced queries to gain more insights into your social outreach?"
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp-5",
    "href": "Intermedio_R_C1.html#instructions-100-xp-5",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nWhen did LinkedIn views exceed 10 and did Facebook views fail to reach 10 for a particular day? Use the linkedin and facebook vectors.\nWhen were one or both of your LinkedIn and Facebook profiles visited at least 12 times?\nWhen is the views matrix equal to a number between 11 and 14, excluding 11 and including 14?\n\n\n\nE6.R\n\n# The social data (linkedin, facebook, views) has been created for you\nlinkedin \nfacebook \n# linkedin exceeds 10 but facebook below 10\nlinkedin >10 & facebook < 10 \n\n# When were one or both visited at least 12 times?\nlinkedin >= 12 | facebook >= 12\n\n# When is views between 11 (exclusive) and 14 (inclusive)?\nviews > 11 & views <= 14"
  },
  {
    "objectID": "Intermedio_R_C1.html#reverse-the-result",
    "href": "Intermedio_R_C1.html#reverse-the-result",
    "title": "7  Conditionals and Control Flow",
    "section": "7.7 Reverse the result: !",
    "text": "7.7 Reverse the result: !\nOn top of the & and | operators, you also learned about the ! operator, which negates a logical value. To refresh your memory, here are some R expressions that use !. They all evaluate to FALSE:\n\n!TRUE !(5 > 3) !!FALSE\n\nWhat would the following set of R expressions return?\n\nx <- 5 y <- 7 !(!(x < 4) & !!!(y > 12))"
  },
  {
    "objectID": "Intermedio_R_C1.html#answer-the-question-50xp",
    "href": "Intermedio_R_C1.html#answer-the-question-50xp",
    "title": "7  Conditionals and Control Flow",
    "section": "Answer the question 50XP",
    "text": "Answer the question 50XP\nPossible Answers\nTRUE press 1\nFALSE press 2\nRunning this piece of code would throw an error. press 3"
  },
  {
    "objectID": "Intermedio_R_C1.html#blend-it-all-together",
    "href": "Intermedio_R_C1.html#blend-it-all-together",
    "title": "7  Conditionals and Control Flow",
    "section": "7.8 Blend it all together",
    "text": "7.8 Blend it all together\nWith the things you’ve learned by now, you’re able to solve pretty cool problems.\nInstead of recording the number of views for your own LinkedIn profile, suppose you conducted a survey inside the company you’re working for. You’ve asked every employee with a LinkedIn profile how many visits their profile has had over the past seven days. You stored the results in a data frame called li_df. This data frame is available in the workspace; type li_df in the console to check it out."
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp-6",
    "href": "Intermedio_R_C1.html#instructions-100-xp-6",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSelect the entire second column, named day2, from the li_df data frame as a vector and assign it to second.\nUse second to create a logical vector, that contains TRUE if the corresponding number of views is strictly greater than 25 or strictly lower than 5 and FALSE otherwise. Store this logical vector as extremes.\nUse sum() on the extremes vector to calculate the number of TRUEs in extremes (i.e. to calculate the number of employees that are either very popular or very low-profile). Simply print this number to the console.\n\n\n\nE7.R\n\n# li_df is pre-loaded in your workspace\nli_df\n# Select the second column, named day2, from li_df: second\nsecond <- li_df[,\"day2\"]\n\n# Build a logical vector, TRUE if value in second is extreme: extremes\nextremes<- c(second < 5 |second > 25)\n\n# Count the number of TRUEs in extremes\nsum(extremes)"
  },
  {
    "objectID": "Intermedio_R_C1.html#the-if-statement",
    "href": "Intermedio_R_C1.html#the-if-statement",
    "title": "7  Conditionals and Control Flow",
    "section": "7.9 The if statement",
    "text": "7.9 The if statement\nBefore diving into some exercises on the if statement, have another look at its syntax:\n\nif (condition) { expr }\n\nRemember your vectors with social profile views? Let’s look at it from another angle. The medium variable gives information about the social website; the num_views variable denotes the actual number of views that particular medium had on the last day of your recordings. Both variables have been pre-defined for you."
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp-7",
    "href": "Intermedio_R_C1.html#instructions-100-xp-7",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nExamine the if statement that prints out “Showing LinkedIn information” if the medium variable equals “LinkedIn”. Code an if statement that prints “You are popular!” to the console if the num_views variable exceeds 15.\n\n\nE8.R\n\n# Variables related to your last day of recordings\nmedium <- \"LinkedIn\"\nnum_views <- 14\n\n# Examine the if statement for medium\nif (medium == \"LinkedIn\") {\n  print(\"Showing LinkedIn information\")\n}\n\n# Write the if statement for num_views\nif(num_views > 15) {\nprint(\"You are popular!\")\n}"
  },
  {
    "objectID": "Intermedio_R_C1.html#add-an-else",
    "href": "Intermedio_R_C1.html#add-an-else",
    "title": "7  Conditionals and Control Flow",
    "section": "7.10 Add an else",
    "text": "7.10 Add an else\nYou can only use an else statement in combination with an if statement. The else statement does not require a condition; its corresponding code is simply run if all of the preceding conditions in the control structure are FALSE. Here’s a recipe for its usage:\n\nif (condition) { expr1 } else { expr2 }\n\nIt’s important that the else keyword comes on the same line as the closing bracket of the if part!\nBoth if statements that you coded in the previous exercises are already available to use. It’s now up to you to extend them with the appropriate else statements!"
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp-8",
    "href": "Intermedio_R_C1.html#instructions-100-xp-8",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nAdd an else statement to both control structures, such that\n“Unknown medium” gets printed out to the console when the if-condition on medium does not hold. R prints out “Try to be more visible!” when the if-condition on num_views is not met.\n\n\nE9.R\n\n  print(\"Showing LinkedIn information\")\n}else {\n  print(\"Unknown medium\")\n}\n\n\n# Control structure for num_views\nif (num_views > 15) {\n  print(\"You're popular!\")\n}else {\n  print(\"Try to be more visible!\")\n}"
  },
  {
    "objectID": "Intermedio_R_C1.html#customize-further-else-if",
    "href": "Intermedio_R_C1.html#customize-further-else-if",
    "title": "7  Conditionals and Control Flow",
    "section": "7.11 Customize further: else if",
    "text": "7.11 Customize further: else if\nThe else if statement allows you to further customize your control structure. You can add as many else if statements as you like. Keep in mind that R ignores the remainder of the control structure once a condition has been found that is TRUE and the corresponding expressions have been executed. Here’s an overview of the syntax to freshen your memory:\n\nif (condition1) { expr1 } else if (condition2) { expr2 } else if (condition3) { expr3 } else { expr4 }\n\nAgain, It’s important that the else if keywords comes on the same line as the closing bracket of the previous part of the control construct!"
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp-9",
    "href": "Intermedio_R_C1.html#instructions-100-xp-9",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nAdd code to both control structures such that:\nR prints out “Showing Facebook information” if medium is equal to “Facebook”. Remember that R is case sensitive! “Your number of views is average” is printed if num_views is between 15 (inclusive) and 10 (exclusive). Feel free to change the variables medium and num_views to see how the control structure respond. In both cases, the existing code should be extended in the else if statement. No existing code should be modified.\n\n\nE10.R\n\n# Variables related to your last day of recordings\nmedium <- \"LinkedIn\"\nnum_views <- 14\n\n# Control structure for medium\nif (medium == \"LinkedIn\") {\n  print(\"Showing LinkedIn information\")\n} else if (medium == \"Facebook\") {print(\"Showing Facebook information\")\n  # Add code to print correct string when condition is TRUE\n\n} else {\n  print(\"Unknown medium\")\n}\n\n# Control structure for num_views\nif (num_views > 15) {\n  print(\"You're popular!\")\n} else if (num_views <= 15 & num_views > 10) {print(\"Your number of views is\n            average\")\n  # Add code to print correct string when condition is TRUE\n\n} else {\n  print(\"Try to be more visible!\")\n}"
  },
  {
    "objectID": "Intermedio_R_C1.html#else-if-2.0",
    "href": "Intermedio_R_C1.html#else-if-2.0",
    "title": "7  Conditionals and Control Flow",
    "section": "7.12 Else if 2.0",
    "text": "7.12 Else if 2.0\nYou can do anything you want inside if-else constructs. You can even put in another set of conditional statements. Examine the following code chunk:\n\nif (number < 10) { if (number < 5) { result <- “extra small” } else { result <- “small” } } else if (number < 100) { result <- “medium” } else { result <- “large” }\n\nprint(result) Have a look at the following statements:\n\nIf number is set to 6, “small” gets printed to the console.\nIf number is set to 100, R prints out “medium”.\nIf number is set to 4, “extra small” gets printed out to the console.\nIf number is set to 2500, R will generate an error, as result will not be defined.\n\nSelect the option that lists all the true statements."
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-50-xp",
    "href": "Intermedio_R_C1.html#instructions-50-xp",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 50 XP",
    "text": "Instructions 50 XP\nPossible Answers\n\n2 and 4\n1 and 4\n1 and 3 <- respuesta\n2 and 3"
  },
  {
    "objectID": "Intermedio_R_C1.html#take-control",
    "href": "Intermedio_R_C1.html#take-control",
    "title": "7  Conditionals and Control Flow",
    "section": "7.13 Take control!",
    "text": "7.13 Take control!\nIn this exercise, you will combine everything that you’ve learned so far: relational operators, logical operators and control constructs. You’ll need it all!\nWe’ve pre-defined two values for you: li and fb, denoting the number of profile views your LinkedIn and Facebook profile had on the last day of recordings. Go through the instructions to create R code that generates a ‘social media score’, sms, based on the values of li and fb."
  },
  {
    "objectID": "Intermedio_R_C1.html#instructions-100-xp-10",
    "href": "Intermedio_R_C1.html#instructions-100-xp-10",
    "title": "7  Conditionals and Control Flow",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFinish the control-flow construct with the following behavior:\nIf both li and fb are 15 or higher, set sms equal to double the sum of li and fb. If both li and fb are strictly below 10, set sms equal to half the sum of li and fb. In all other cases, set sms equal to li + fb. Finally, print the resulting sms variable.\n\n\nE11.R\n\n # Variables related to your last day of recordings\nli <- 15\nfb <- 9\n\n# Code the control-flow construct\nif (li>15 & fb>15) {\n  sms <- 2 * (li + fb)\n} else if (li<10 & fb<10){\n  sms <- 0.5 * (li + fb)\n} else {\n  sms <- li + fb\n}\n\n# Print the resulting sms to the console\nprint(sms)"
  },
  {
    "objectID": "Intermedio_R_C2.html#write-a-while-loop",
    "href": "Intermedio_R_C2.html#write-a-while-loop",
    "title": "8  Loops",
    "section": "8.1 Write a while loop",
    "text": "8.1 Write a while loop\nLet’s get you started with building a while loop from the ground up. Have another look at its recipe:\nwhile (condition) { expr } Remember that the condition part of this recipe should become FALSE at some point during the execution. Otherwise, the while loop will go on indefinitely.\nIf your session expires when you run your code, check the body of your while loop carefully.\nHave a look at the sample code provided; it initializes the speed variables and already provides a while loop template to get you started."
  },
  {
    "objectID": "Intermedio_R_C2.html#instructions-100-xp",
    "href": "Intermedio_R_C2.html#instructions-100-xp",
    "title": "8  Loops",
    "section": "8.2 Instructions 100 XP",
    "text": "8.2 Instructions 100 XP\nCode a while loop with the following characteristics:\nThe condition of the while loop should check if speed is higher than 30. Inside the body of the while loop, print out “Slow down!”. Inside the body of the while loop, decrease the speed by 7 units and assign this new value to speed again. This step is crucial; otherwise your while loop will never stop and your session will expire. If your session expires when you run your code, check the body of your while loop carefully: it’s likely that you made a mistake.\n\n\nE1.R\n\n# Initialize the speed variable\nspeed <- 64\n\n# Code the while loop\n\nwhile (speed > 30) {\n  print(paste(\"Slow down!\"))\n  speed <- speed -7\n}\n\n# Print out the speed variable\nprint(speed)"
  },
  {
    "objectID": "Intermedio_R_C2.html#throw-in-more-conditionals",
    "href": "Intermedio_R_C2.html#throw-in-more-conditionals",
    "title": "8  Loops",
    "section": "8.3 Throw in more conditionals",
    "text": "8.3 Throw in more conditionals\nIn the previous exercise, you simulated the interaction between a driver and a driver’s assistant: When the speed was too high, “Slow down!” got printed out to the console, resulting in a decrease of your speed by 7 units.\nThere are several ways in which you could make your driver’s assistant more advanced. For example, the assistant could give you different messages based on your speed or provide you with a current speed at a given moment.\nA while loop similar to the one you’ve coded in the previous exercise is already available for you to use. It prints out your current speed, but there’s no code that decreases the speed variable yet, which is pretty dangerous. Can you make the appropriate changes?"
  },
  {
    "objectID": "Intermedio_R_C2.html#instructions-100-xp-1",
    "href": "Intermedio_R_C2.html#instructions-100-xp-1",
    "title": "8  Loops",
    "section": "8.4 Instructions 100 XP",
    "text": "8.4 Instructions 100 XP\nIf the speed is greater than 48, have R print out “Slow down big time!”, and decrease the speed by 11. Otherwise, have R simply print out “Slow down!”, and decrease the speed by 6. If the session keeps timing out and throwing an error, you are probably stuck in an infinite loop! Check the body of your while loop and make sure you are assigning new values to speed.\n\n\nE2.R\n\n# Initialize the speed variable\nspeed <- 64\n\n# Extend/adapt the while loop\nwhile (speed > 30) {\n  print(paste(\"Your speed is\",speed))\n  if (speed > 48) {\n    print(\"Slow down big time!\")\n    speed <- speed -11\n  } else {\n    print(\"Slow down!\")\n    speed <- speed -6\n  }\n}"
  },
  {
    "objectID": "Intermedio_R_C2.html#stop-the-while-loop-break",
    "href": "Intermedio_R_C2.html#stop-the-while-loop-break",
    "title": "8  Loops",
    "section": "8.5 Stop the while loop: break",
    "text": "8.5 Stop the while loop: break\nThere are some very rare situations in which severe speeding is necessary: what if a hurricane is approaching and you have to get away as quickly as possible? You don’t want the driver’s assistant sending you speeding notifications in that scenario, right?\nThis seems like a great opportunity to include the break statement in the while loop you’ve been working on. Remember that the break statement is a control statement. When R encounters it, the while loop is abandoned completely."
  },
  {
    "objectID": "Intermedio_R_C2.html#instructions-100-xp-2",
    "href": "Intermedio_R_C2.html#instructions-100-xp-2",
    "title": "8  Loops",
    "section": "8.6 Instructions 100 XP",
    "text": "8.6 Instructions 100 XP\nAdapt the while loop such that it is abandoned when the speed of the vehicle is greater than 80. This time, the speed variable has been initialized to 88; keep it that way.\n\n\nE3.R\n\n# Initialize the speed variable\nspeed <- 88\n\nwhile (speed > 30) {\n  print(paste(\"Your speed is\", speed))\n  \n  # Break the while loop when speed exceeds 80\n  if  (speed > 80){  \n    break\n  }\n  \n  if (speed > 48) {\n    print(\"Slow down big time!\")\n    speed <- speed - 11\n  } else {\n    print(\"Slow down!\")\n    speed <- speed - 6\n  }\n}"
  },
  {
    "objectID": "Intermedio_R_C2.html#build-a-while-loop-from-scratch",
    "href": "Intermedio_R_C2.html#build-a-while-loop-from-scratch",
    "title": "8  Loops",
    "section": "8.7 Build a while loop from scratch",
    "text": "8.7 Build a while loop from scratch\nThe previous exercises guided you through developing a pretty advanced while loop, containing a break statement and different messages and updates as determined by control flow constructs. If you manage to solve this comprehensive exercise using a while loop, you’re totally ready for the next topic: the for loop."
  },
  {
    "objectID": "Intermedio_R_C2.html#instructions-100-xp-3",
    "href": "Intermedio_R_C2.html#instructions-100-xp-3",
    "title": "8  Loops",
    "section": "8.8 Instructions 100 XP",
    "text": "8.8 Instructions 100 XP\nFinish the while loop so that it:\nprints out the triple of i, so 3 * i, at each run. is abandoned with a break if the triple of i is divisible by 8, but still prints out this triple before breaking.\n\n\nE4.R\n\n# Initialize i as 1 \ni <- 1\n\n# Code the while loop\nwhile (i <= 10) {\n  print(3 * i)\n  if ((i * 3) %% 8 == 0) {\n    break\n  }\n  i <- i + 1\n}\n\n# Initialize i as 1 \ni <- 1"
  },
  {
    "objectID": "Intermedio_R_C3.html",
    "href": "Intermedio_R_C3.html",
    "title": "9  Functions",
    "section": "",
    "text": "10 Function documentation\nBefore even thinking of using an R function, you should clarify which arguments it expects. All the relevant details such as a description, usage, and arguments can be found in the documentation. To consult the documentation on the sample() function, for example, you can use one of following R commands:\nIf you execute these commands, you’ll be redirected to www.rdocumentation.org.\nA quick hack to see the arguments of the sample() function is the args() function. Try it out in the console:\nIn the next exercises, you’ll be learning how to use the mean() function with increasing complexity. The first thing you’ll have to do is get acquainted with the mean() function."
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp",
    "href": "Intermedio_R_C3.html#instructions-100-xp",
    "title": "9  Functions",
    "section": "10.1 Instructions 100 XP",
    "text": "10.1 Instructions 100 XP\n\nConsult the documentation on the mean() function: ?mean or help(mean).\nInspect the arguments of the mean() function using the args() function.\n\n\n\nE1.R\n\n# Consult the documentation on the mean() function\n?mean\n\n# Inspect the arguments of the mean() function\nargs(mean)"
  },
  {
    "objectID": "Intermedio_R_C3.html#use-a-function",
    "href": "Intermedio_R_C3.html#use-a-function",
    "title": "9  Functions",
    "section": "10.2 Use a function",
    "text": "10.2 Use a function\nThe documentation on the mean() function gives us quite some information:\n\nThe mean() function computes the arithmetic mean.\nThe most general method takes multiple arguments: x and ….\nThe x argument should be a vector containing numeric, logical or time-related information.\n\nRemember that R can match arguments both by position and by name. Can you still remember the difference? You’ll find out in this exercise!\nOnce more, you’ll be working with the view counts of your social network profiles for the past 7 days. These are stored in the linkedin and facebook vectors and have already been created for you."
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp-1",
    "href": "Intermedio_R_C3.html#instructions-100-xp-1",
    "title": "9  Functions",
    "section": "10.3 Instructions 100 XP",
    "text": "10.3 Instructions 100 XP\n\nCalculate the average number of views for both linkedin and facebook and assign the result to avg_li and avg_fb, respectively. Experiment with different types of argument matching!\nPrint out both avg_li and avg_fb.\n\n\n\nE2.R\n\n# The linkedin and facebook vectors have already been created for you\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# Calculate average number of views\navg_li <- mean(linkedin)\navg_fb <- mean(facebook)\n\n# Inspect avg_li and avg_fb\nprint(avg_li)\nprint(avg_fb)"
  },
  {
    "objectID": "Intermedio_R_C3.html#use-a-function-2",
    "href": "Intermedio_R_C3.html#use-a-function-2",
    "title": "9  Functions",
    "section": "10.4 Use a function (2)",
    "text": "10.4 Use a function (2)\nCheck the documentation on the mean() function again:\n\n?mean\n\nThe Usage section of the documentation includes two versions of the mean() function. The first usage,\n\nmean(x, …)\n\nis the most general usage of the mean function. The ‘Default S3 method’, however, is:\n\nmean(x, trim = 0, na.rm = FALSE, …)\n\nThe … is called the ellipsis. It is a way for R to pass arguments along without the function having to name them explicitly. The ellipsis will be treated in more detail in future courses.\nFor the remainder of this exercise, just work with the second usage of the mean function. Notice that both trim and na.rm have default values. This makes them optional arguments."
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp-2",
    "href": "Intermedio_R_C3.html#instructions-100-xp-2",
    "title": "9  Functions",
    "section": "10.5 Instructions 100 XP",
    "text": "10.5 Instructions 100 XP\n\nCalculate the mean of the element-wise sum of linkedin and facebook and store the result in a variable avg_sum.\nCalculate the mean once more, but this time set the trim argument equal to 0.2 and assign the result to avg_sum_trimmed.\nPrint out both avg_sum and avg_sum_trimmed; can you spot the difference?\n\n\n\nE3.R\n\n# The linkedin and facebook vectors have already been created for you\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n\n# Calculate the mean of the sum\nmean(linkedin + facebook)\n\n# Calculate the trimmed mean of the sum\navg_sum_trimmed <- mean(linkedin + facebook, trim = 0.2,na.rm = FALSE,)\n\n\n# Inspect both new variables\nprint(avg_sum)\nprint(avg_sum_trimmed)"
  },
  {
    "objectID": "Intermedio_R_C3.html#use-a-function-3",
    "href": "Intermedio_R_C3.html#use-a-function-3",
    "title": "9  Functions",
    "section": "10.6 Use a function (3)",
    "text": "10.6 Use a function (3)\nIn the video, Filip guided you through the example of specifying arguments of the sd() function. The sd() function has an optional argument, na.rm that specified whether or not to remove missing values from the input vector before calculating the standard deviation.\nIf you’ve had a good look at the documentation, you’ll know by now that the mean() function also has this argument, na.rm, and it does the exact same thing. By default, it is set to FALSE, as the Usage of the default S3 method shows:\nmean(x, trim = 0, na.rm = FALSE, …) Let’s see what happens if your vectors linkedin and facebook contain missing values (NA)."
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp-3",
    "href": "Intermedio_R_C3.html#instructions-100-xp-3",
    "title": "9  Functions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nCalculate the average number of LinkedIn profile views, without specifying any optional arguments. Simply print the result to the console. Calculate the average number of LinkedIn profile views, but this time tell R to strip missing values from the input vector.\n\n\nE4.R\n\n# The linkedin and facebook vectors have already been created for you\nlinkedin <- c(16, 9, 13, 5, NA, 17, 14)\nfacebook <- c(17, NA, 5, 16, 8, 13, 14)\n\n# Basic average of linkedin\nmean(linkedin)\n\n# Advanced average of linkedin\nmean(linkedin, na.rm = TRUE)"
  },
  {
    "objectID": "Intermedio_R_C3.html#functions-inside-functions",
    "href": "Intermedio_R_C3.html#functions-inside-functions",
    "title": "9  Functions",
    "section": "10.7 Functions inside functions",
    "text": "10.7 Functions inside functions\nYou already know that R functions return objects that you can then use somewhere else. This makes it easy to use functions inside functions, as you’ve seen before:\n\nspeed <- 31 print(paste(“Your speed is”, speed))\n\nNotice that both the print() and paste() functions use the ellipsis - … - as an argument. Can you figure out how they’re used?"
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp-4",
    "href": "Intermedio_R_C3.html#instructions-100-xp-4",
    "title": "9  Functions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUse abs() on linkedin - facebook to get the absolute differences between the daily LinkedIn and Facebook profile views. Place the call to abs() inside mean() to calculate the Mean Absolute Deviation. In the mean() call, make sure to specify na.rm to treat missing values correctly!\n\n\nE5.R\n\n# The linkedin and facebook vectors have already been created for you\nlinkedin <- c(16, 9, 13, 5, NA, 17, 14)\nfacebook <- c(17, NA, 5, 16, 8, 13, 14)\n\n# Calculate the mean absolute deviation\nmean(abs(linkedin - facebook), na.rm= TRUE)"
  },
  {
    "objectID": "Intermedio_R_C3.html#required-or-optional",
    "href": "Intermedio_R_C3.html#required-or-optional",
    "title": "9  Functions",
    "section": "10.8 Required, or optional?",
    "text": "10.8 Required, or optional?\nBy now, you will probably have a good understanding of the difference between required and optional arguments. Let’s refresh this difference by having one last look at the mean() function:\nmean(x, trim = 0, na.rm = FALSE, …) x is required; if you do not specify it, R will throw an error. trim and na.rm are optional arguments: they have a default value which is used if the arguments are not explicitly specified.\nWhich of the following statements about the read.table() function are true?\nheader, sep and quote are all optional arguments. row.names and fileEncoding don’t have default values. read.table(“myfile.txt”, “-”, TRUE) will throw an error. read.table(“myfile.txt”, sep = “-”, header = TRUE) will throw an error."
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-50-xp",
    "href": "Intermedio_R_C3.html#instructions-50-xp",
    "title": "9  Functions",
    "section": "Instructions 50 XP",
    "text": "Instructions 50 XP\nPossible Answers\n\nand (3) Respuesta\nand (4)\n\n(1), (2), and (3)\n(1), (2), and (4)"
  },
  {
    "objectID": "Intermedio_R_C3.html#write-your-own-function",
    "href": "Intermedio_R_C3.html#write-your-own-function",
    "title": "9  Functions",
    "section": "10.9 Write your own function",
    "text": "10.9 Write your own function\nWow, things are getting serious… you’re about to write your own function! Before you have a go at it, have a look at the following function template:\n\nmy_fun <- function(arg1, arg2) { body }\n\nNotice that this recipe uses the assignment operator (<-) just as if you were assigning a vector to a variable for example. This is not a coincidence. Creating a function in R basically is the assignment of a function object to a variable! In the recipe above, you’re creating a new R variable my_fun, that becomes available in the workspace as soon as you execute the definition. From then on, you can use the my_fun as a function."
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp-5",
    "href": "Intermedio_R_C3.html#instructions-100-xp-5",
    "title": "9  Functions",
    "section": "10.10 Instructions 100 XP",
    "text": "10.10 Instructions 100 XP\n\nCreate a function pow_two(): it takes one argument and returns that number squared (that number times itself).\nCall this newly defined function with 12 as input.\nNext, create a function sum_abs(), that takes two arguments and returns the sum of the absolute values of both arguments.\nFinally, call the function sum_abs() with arguments -2 and 3 afterwards.\n\n\n\nE6.R\n\n# Create a function pow_two()\npow_two <- function(arg1) {\n  arg1*arg1\n}\n\n# Use the function\npow_two(12)\n\n# Create a function sum_abs()\nsum_abs <- function(arg1, arg2) {\n  sum(abs(arg1), abs(arg2))\n}\n# Use the function\nsum_abs(-2, 3)"
  },
  {
    "objectID": "Intermedio_R_C3.html#write-your-own-function-2",
    "href": "Intermedio_R_C3.html#write-your-own-function-2",
    "title": "9  Functions",
    "section": "10.11 Write your own function (2)",
    "text": "10.11 Write your own function (2)\nThere are situations in which your function does not require an input. Let’s say you want to write a function that gives us the random outcome of throwing a fair die:\n\nthrow_die <- function() { number <- sample(1:6, size = 1) number }\n\n\nthrow_die()\n\nUp to you to code a function that doesn’t take any arguments!"
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp-6",
    "href": "Intermedio_R_C3.html#instructions-100-xp-6",
    "title": "9  Functions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nDefine a function, hello(). It prints out “Hi there!” and returns TRUE. It has no arguments. Call the function hello(), without specifying arguments of course.\n\n\nE7.R\n\n# Define the function hello()\nhello <- function() {\n  print(\"Hi there!\")\n  return(TRUE) \n}\n\n# Call the function hello()\nhello()"
  },
  {
    "objectID": "Intermedio_R_C3.html#write-your-own-function-3",
    "href": "Intermedio_R_C3.html#write-your-own-function-3",
    "title": "9  Functions",
    "section": "10.12 Write your own function (3)",
    "text": "10.12 Write your own function (3)\nDo you still remember the difference between an argument with and without default values? The usage section in the sd() documentation shows the following information:\n\nsd(x, na.rm = FALSE)\n\nThis tells us that x has to be defined for the sd() function to be called correctly, however, na.rm already has a default value. Not specifying this argument won’t cause an error.\nYou can define default argument values in your own R functions as well. You can use the following recipe to do so:\n\nmy_fun <- function(arg1, arg2 = val2) { body }\n\nThe editor on the right already includes an extended version of the pow_two() function from before. Can you finish it?"
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp-7",
    "href": "Intermedio_R_C3.html#instructions-100-xp-7",
    "title": "9  Functions",
    "section": "10.13 Instructions 100 XP",
    "text": "10.13 Instructions 100 XP\n\nAdd an optional argument, named print_info, that is TRUE by default.\nWrap an if construct around the print() function: this function should only be executed if print_info is TRUE.\nFeel free to experiment with the pow_two() function you’ve just coded.\n\n\n\nE8.R\n\n# Finish the pow_two() function\npow_two <- function(x, print_info = TRUE) {\n  y <- x ^ 2\n  if (print_info) {\n    print(paste(x, \"to the power two equals\", y))\n  }\n  return(y)\n}\n\npow_two(5, FALSE)"
  },
  {
    "objectID": "Intermedio_R_C3.html#function-scoping",
    "href": "Intermedio_R_C3.html#function-scoping",
    "title": "9  Functions",
    "section": "10.14 Function scoping",
    "text": "10.14 Function scoping\nAn issue that Filip did not discuss in the video is function scoping. It implies that variables that are defined inside a function are not accessible outside that function. Try running the following code and see if you understand the results:\n\npow_two <- function(x) { y <- x ^ 2 return(y) } pow_two(4) y x\n\ny was defined inside the pow_two() function and therefore it is not accessible outside of that function. This is also true for the function’s arguments of ourse - x in this case.\nWhich statement is correct about the following chunk of code? The function two_dice() is already available in the workspace.\n\ntwo_dice <- function() { possibilities <- 1:6 dice1 <- sample(possibilities, size = 1) dice2 <- sample(possibilities, size = 1) dice1 + dice2 }"
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-50-xp-1",
    "href": "Intermedio_R_C3.html#instructions-50-xp-1",
    "title": "9  Functions",
    "section": "Instructions 50 XP",
    "text": "Instructions 50 XP\nPossible Answers\n\nExecuting two_dice() causes an error.\nExecuting res <- two_dice() makes the contents of dice1 and dice2 available outside the function.\nWhatever the way of calling the two_dice() function, R won’t have access to dice1 and dice2 outside the function. Respuesta"
  },
  {
    "objectID": "Intermedio_R_C3.html#r-passes-arguments-by-value",
    "href": "Intermedio_R_C3.html#r-passes-arguments-by-value",
    "title": "9  Functions",
    "section": "10.15 R passes arguments by value",
    "text": "10.15 R passes arguments by value\nThe title gives it away already: R passes arguments by value. What does this mean? Simply put, it means that an R function cannot change the variable that you input to that function. Let’s look at a simple example (try it in the console):\n\ntriple <- function(x) { x <- 3*x x } a <- 5 triple(a) a\n\nInside the triple() function, the argument x gets overwritten with its value times three. Afterwards this new x is returned. If you call this function with a variable a set equal to 5, you obtain 15. But did the value of a change? If R were to pass a to triple() by reference, the override of the x inside the function would ripple through to the variable a, outside the function. However, R passes by value, so the R objects you pass to a function can never change unless you do an explicit assignment. a remains equal to 5, even after calling triple(a).\nCan you tell which one of the following statements is false about the following piece of code?\n\nincrement <- function(x, inc = 1) { x <- x + inc x } count <- 5 a <- increment(count, 2) b <- increment(count) count <- increment(count, 2)"
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-50-xp-2",
    "href": "Intermedio_R_C3.html#instructions-50-xp-2",
    "title": "9  Functions",
    "section": "Instructions 50 XP",
    "text": "Instructions 50 XP\nPossible Answers\n\na and b equal 7 and 6 respectively after executing this code block.\nAfter the first call of increment(), where a is defined, a equals 7 and count equals 5.\nIn the end, count will equal 10. Respuesta\nIn the last expression, the value of count was actually changed because of the explicit assignment."
  },
  {
    "objectID": "Intermedio_R_C3.html#r-you-functional",
    "href": "Intermedio_R_C3.html#r-you-functional",
    "title": "9  Functions",
    "section": "10.16 R you functional?",
    "text": "10.16 R you functional?\nNow that you’ve acquired some skills in defining functions with different types of arguments and return values, you should try to create more advanced functions. As you’ve noticed in the previous exercises, it’s perfectly possible to add control-flow constructs, loops and even other functions to your function body.\nRemember our social media example? The vectors linkedin and facebook are already defined in the workspace so you can get your hands dirty straight away. As a first step, you will be writing a function that can interpret a single value of this vector. In the next exercise, you will write another function that can handle an entire vector at once."
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp-8",
    "href": "Intermedio_R_C3.html#instructions-100-xp-8",
    "title": "9  Functions",
    "section": "10.17 Instructions 100 XP",
    "text": "10.17 Instructions 100 XP\n\nFinish the function definition for interpret(), that interprets the number of profile views on a single day:\nThe function takes one argument, num_views.\nIf num_views is greater than 15, the function prints out “You’re popular!” to the console and returns num_views.\nElse, the function prints out “Try to be more visible!” and returns 0.\nFinally, call the interpret() function twice: on the first value of the linkedin vector and on the second element of the facebook vector.\n\n\n\nE9.R\n\n# The linkedin and facebook vectors have already been created for you\n\n# Define the interpret function\ninterpret <- function(num_views) {\n  if (num_views > 15) {\n    print(\"You're popular!\")\n     return(num_views)\n  } \n  else {\n    print(\"Try to be more visible!\")\n    return(0)\n  }\n}\n\n\n# Call the interpret function twice\ninterpret(linkedin)\ninterpret(facebook[2])"
  },
  {
    "objectID": "Intermedio_R_C3.html#r-you-functional-2",
    "href": "Intermedio_R_C3.html#r-you-functional-2",
    "title": "9  Functions",
    "section": "10.18 R you functional? (2)",
    "text": "10.18 R you functional? (2)\nA possible implementation of the interpret() function has been provided for you. In this exercise you’ll be writing another function that will use the interpret() function to interpret all the data from your daily profile views inside a vector. Furthermore, your function will return the sum of views on popular days, if asked for. A for loop is ideal for iterating over all the vector elements. The ability to return the sum of views on popular days is something you can code through a function argument with a default value."
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp-9",
    "href": "Intermedio_R_C3.html#instructions-100-xp-9",
    "title": "9  Functions",
    "section": "10.19 Instructions 100 XP",
    "text": "10.19 Instructions 100 XP\nFinish the template for the interpret_all() function:\n\nMake return_sum an optional argument, that is TRUE by default.\nInside the for loop, iterate over all views: on every iteration, add the result of interpret(v) to count. Remember that interpret(v) returns v for popular days,and 0 otherwise. At the same time, interpret(v) will also do some printouts.\nFinish the if construct:\nIf return_sum is TRUE, return count.\nElse, return NULL.\nCall this newly defined function on both linkedin and facebook.\n\n\n\nE10.R\n\n# The linkedin and facebook vectors have already been created for you\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# The interpret() can be used inside interpret_all()\ninterpret <- function(num_views) {\n  if (num_views > 15) {\n    print(\"You're popular!\")\n    return(num_views)\n  } else {\n    print(\"Try to be more visible!\")\n    return(0)\n  }\n}\n\n# Define the interpret_all() function\n# views: vector with data to interpret\n# return_sum: return total number of views on popular days?\ninterpret_all <- function(views, return_sum = TRUE) {\n  count <- 0\n\n  for (v in views) {\n    count <- count + interpret(v)\n    \n  }\n\n  if (return_sum == TRUE) {\n    return(count)\n\n  } else {\n    return(NULL)\n\n  }\n}\n\n# Call the interpret_all() function on both linkedin and facebook\ninterpret_all(linkedin)\ninterpret_all(facebook)"
  },
  {
    "objectID": "Intermedio_R_C3.html#load-an-r-package",
    "href": "Intermedio_R_C3.html#load-an-r-package",
    "title": "9  Functions",
    "section": "10.20 Load an R Package",
    "text": "10.20 Load an R Package\nThere are basically two extremely important functions when it comes down to R packages:\n\ninstall.packages(), which as you can expect, installs a given package.\nlibrary() which loads packages, i.e. attaches them to the search list on your R workspace.\n\nTo install packages, you need administrator privileges. This means that install.packages() will thus not work in the DataCamp interface. However, almost all CRAN packages are installed on our servers. You can load them with library().\nIn this exercise, you’ll be learning how to load the ggplot2 package, a powerful package for data visualization. You’ll use it to create a plot of two variables of the mtcars data frame. The data has already been prepared for you in the workspace.\nBefore starting, execute the following commands in the console:\n\nsearch(), to look at the currently attached packages and\nqplot(mtcars\\(wt, mtcars\\)hp), to build a plot of two variables of the mtcars data frame.\n\nAn error should occur, because you haven’t loaded the ggplot2 package yet!"
  },
  {
    "objectID": "Intermedio_R_C3.html#instructions-100-xp-10",
    "href": "Intermedio_R_C3.html#instructions-100-xp-10",
    "title": "9  Functions",
    "section": "10.21 Instructions 100 XP",
    "text": "10.21 Instructions 100 XP\n\nTo fix the error you saw in the console, load the ggplot2 package. Make sure you are loading (and not installing) the package!\nNow, retry calling the qplot() function with the same arguments.\nFinally, check out the currently attached packages again.\n\n\n\nE11.R\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\n# Retry the qplot() function\nqplot(mtcars$wt, mtcars$hp)\n\n# Check out the currently attached packages again\nsearch()"
  },
  {
    "objectID": "Intermedio_R_C3.html#different-ways-to-load-a-package",
    "href": "Intermedio_R_C3.html#different-ways-to-load-a-package",
    "title": "9  Functions",
    "section": "10.22 Different ways to load a package",
    "text": "10.22 Different ways to load a package\nThe library() and require() functions are not very picky when it comes down to argument types: both library(rjson) and library(“rjson”) work perfectly fine for loading a package.\nHave a look at some more code chunks that (attempt to) load one or more packages:\n\n11 Chunk 1\nlibrary(data.table) require(rjson)\n\n\n12 Chunk 2\nlibrary(“data.table”) require(rjson)\n\n\n13 Chunk 3\nlibrary(data.table) require(rjson, character.only = TRUE)\n\n\n14 Chunk 4\nlibrary(c(“data.table”, “rjson”)) Select the option that lists all of the chunks that do not generate an error. The console is yours to > experiment in.\n\nInstructions 50 XP {.unnumbered}\nPossible Answers\n\nOnly (1)\n\nand (2) Respuesta\n\n(1), (2) and (3)\nAll of them are valid"
  },
  {
    "objectID": "Intermedio_R_C4.html#use-lapply-with-a-built-in-r-function",
    "href": "Intermedio_R_C4.html#use-lapply-with-a-built-in-r-function",
    "title": "10  The apply family",
    "section": "10.1 Use lapply with a built-in R function",
    "text": "10.1 Use lapply with a built-in R function\nBefore you go about solving the exercises below, have a look at the documentation of the lapply() function. The Usage section shows the following expression:\n\nlapply(X, FUN, …)\n\nTo put it generally, lapply takes a vector or list X, and applies the function FUN to each of its members. If FUN requires additional arguments, you pass them after you’ve specified X and FUN (…). The output of lapply() is a list, the same length as X, where each element is the result of applying FUN on the corresponding element of X.\nNow that you are truly brushing up on your data science skills, let’s revisit some of the most relevant figures in data science history. We’ve compiled a vector of famous mathematicians/statisticians and the year they were born. Up to you to extract some information!"
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp",
    "href": "Intermedio_R_C4.html#instructions-100-xp",
    "title": "10  The apply family",
    "section": "10.2 Instructions 100 XP",
    "text": "10.2 Instructions 100 XP\n\nHave a look at the strsplit() calls, that splits the strings in pioneers on the : sign. The result, split_math is a list of 4 character vectors: the first vector element represents the name, the second element the birth year.\nUse lapply() to convert the character vectors in split_math to lowercase letters: apply tolower() on each of the elements in split_math. Assign the result, which is a list, to a new variable split_low.\nFinally, inspect the contents of split_low with str().\n\n\n\nE1.R\n\n# The vector pioneers has already been created for you\npioneers <- c(\"GAUSS:1777\", \"BAYES:1702\", \"PASCAL:1623\", \"PEARSON:1857\")\n\n# Split names from birth year\nsplit_math <- strsplit(pioneers, split = \":\")\n\n# Convert to lowercase strings: split_low\nsplit_low <- lapply(split_math, tolower)\n\n# Take a look at the structure of split_low\nstr(split_low)"
  },
  {
    "objectID": "Intermedio_R_C4.html#use-lapply-with-your-own-function",
    "href": "Intermedio_R_C4.html#use-lapply-with-your-own-function",
    "title": "10  The apply family",
    "section": "10.3 Use lapply with your own function",
    "text": "10.3 Use lapply with your own function\nAs Filip explained in the instructional video, you can use lapply() on your own functions as well. You just need to code a new function and make sure it is available in the workspace. After that, you can use the function inside lapply() just as you did with base R functions.\nIn the previous exercise you already used lapply() once to convert the information about your favorite pioneering statisticians to a list of vectors composed of two character strings. Let’s write some code to select the names and the birth years separately.\nThe sample code already includes code that defined select_first(), that takes a vector as input and returns the first element of this vector."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-1",
    "href": "Intermedio_R_C4.html#instructions-100-xp-1",
    "title": "10  The apply family",
    "section": "10.4 Instructions 100 XP",
    "text": "10.4 Instructions 100 XP\n\nApply select_first() over the elements of split_low with lapply() and assign the result to a new variable names.\nNext, write a function select_second() that does the exact same thing for the second element of an inputted vector.\nFinally, apply the select_second() function over split_low and assign the output to the variable years.\n\n\n\nE2.R\n\n# Code from previous exercise:\npioneers <- c(\"GAUSS:1777\", \"BAYES:1702\", \"PASCAL:1623\", \"PEARSON:1857\")\nsplit <- strsplit(pioneers, split = \":\")\nsplit_low <- lapply(split, tolower)\n\n# Write function select_first()\nselect_first <- function(x) {\n  x[1]\n}\n\n# Apply select_first() over split_low: names\nnames <- lapply(split_low, select_first)\n\n# Write function select_second()\nselect_second <- function(x) {\n  x[2]\n}\n\n# Apply select_second() over split_low: years\nyears <- lapply(split_low, select_second)"
  },
  {
    "objectID": "Intermedio_R_C4.html#lapply-and-anonymous-functions",
    "href": "Intermedio_R_C4.html#lapply-and-anonymous-functions",
    "title": "10  The apply family",
    "section": "10.5 lapply and anonymous functions",
    "text": "10.5 lapply and anonymous functions\nWriting your own functions and then using them inside lapply() is quite an accomplishment! But defining functions to use them only once is kind of overkill, isn’t it? That’s why you can use so-called anonymous functions in R.\nPreviously, you learned that functions in R are objects in their own right. This means that they aren’t automatically bound to a name. When you create a function, you can use the assignment operator to give the function a name. It’s perfectly possible, however, to not give the function a name. This is called an anonymous function:\n\nNamed function\ntriple <- function(x) { 3 * x }\n\n\nAnonymous function with same implementation\nfunction(x) { 3 * x }\n\n\nUse anonymous function inside lapply()\nlapply(list(1,2,3), function(x) { 3 * x }) split_low is defined for you."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-2",
    "href": "Intermedio_R_C4.html#instructions-100-xp-2",
    "title": "10  The apply family",
    "section": "10.6 Instructions 100 XP",
    "text": "10.6 Instructions 100 XP\nTransform the first call of lapply() such that it uses an anonymous function that does the same thing. In a similar fashion, convert the second call of lapply to use an anonymous version of the select_second() function. Remove both the definitions of select_first() and select_second(), as they are no longer useful.\n\n\nE3.R\n\n# split_low has been created for you\nsplit_low <- lapply(split, tolower)\nsplit_low\n\n# Transform: use anonymous function inside lapply\n\nnames <- lapply(split_low, function(x) { x[1] })\n\n# Transform: use anonymous function inside lapply\n\nyears <- lapply(split_low, function(x) { x[2] })"
  },
  {
    "objectID": "Intermedio_R_C4.html#use-lapply-with-additional-arguments",
    "href": "Intermedio_R_C4.html#use-lapply-with-additional-arguments",
    "title": "10  The apply family",
    "section": "10.7 Use lapply with additional arguments",
    "text": "10.7 Use lapply with additional arguments\nIn the video, the triple() function was transformed to the multiply() function to allow for a more generic approach. lapply() provides a way to handle functions that require more than one argument, such as the multiply() function:\nmultiply <- function(x, factor) { x * factor } lapply(list(1,2,3), multiply, factor = 3) On the right we’ve included a generic version of the select functions that you’ve coded earlier: select_el(). It takes a vector as its first argument, and an index as its second argument. It returns the vector’s element at the specified index."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-3",
    "href": "Intermedio_R_C4.html#instructions-100-xp-3",
    "title": "10  The apply family",
    "section": "10.8 Instructions 100 XP",
    "text": "10.8 Instructions 100 XP\nUse lapply() twice to call select_el() over all elements in split_low: once with the index equal to 1 and a second time with the index equal to 2. Assign the result to names and years, respectively.\n\n\nE4.R\n\n# Definition of split_low\npioneers <- c(\"GAUSS:1777\", \"BAYES:1702\", \"PASCAL:1623\", \"PEARSON:1857\")\nsplit <- strsplit(pioneers, split = \":\")\nsplit_low <- lapply(split, tolower)\n\n# Generic select function\nselect_el <- function(x, index) {\n  x[index]\n}\n\n# Use lapply() twice on split_low: names and years\nnames <- lapply(split_low, select_el, index = 1)\nyears <- lapply(split_low, select_el, index = 2)"
  },
  {
    "objectID": "Intermedio_R_C4.html#apply-functions-that-return-null",
    "href": "Intermedio_R_C4.html#apply-functions-that-return-null",
    "title": "10  The apply family",
    "section": "10.9 Apply functions that return NULL",
    "text": "10.9 Apply functions that return NULL\nIn all of the previous exercises, it was assumed that the functions that were applied over vectors and lists actually returned a meaningful result. For example, the tolower() function simply returns the strings with the characters in lowercase. This won’t always be the case. Suppose you want to display the structure of every element of a list. You could use the str() function for this, which returns NULL:\n\nlapply(list(1, “a”, TRUE), str)\n\nThis call actually returns a list, the same size as the input list, containing all NULL values. On the other hand calling\n\nstr(TRUE)\n\non its own prints only the structure of the logical to the console, not NULL. That’s because str() uses invisible() behind the scenes, which returns an invisible copy of the return value, NULL in this case. This prevents it from being printed when the result of str() is not assigned.\nWhat will the following code chunk return (split_low is already available in the workspace)? Try to reason about the result before simply executing it in the console!\n\nlapply(split_low, function(x) { if (nchar(x[1]) > 5) { return(NULL) } else { return(x[2]) } })"
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-50-xp",
    "href": "Intermedio_R_C4.html#instructions-50-xp",
    "title": "10  The apply family",
    "section": "Instructions 50 XP",
    "text": "Instructions 50 XP\nPossible Answers\n\nlist(NULL, NULL, “1623”, “1857”)\nlist(“gauss”, “bayes”, NULL, NULL)\nlist(“1777”, “1702”, NULL, NULL) Respuesta\nlist(“1777”, “1702”)"
  },
  {
    "objectID": "Intermedio_R_C4.html#how-to-use-sapply",
    "href": "Intermedio_R_C4.html#how-to-use-sapply",
    "title": "10  The apply family",
    "section": "10.10 How to use sapply",
    "text": "10.10 How to use sapply\nYou can use sapply() similar to how you used lapply(). The first argument of sapply() is the list or vector X over which you want to apply a function, FUN. Potential additional arguments to this function are specified afterwards (…):\n\nsapply(X, FUN, …)\n\nIn the next couple of exercises, you’ll be working with the variable temp, that contains temperature measurements for 7 days. temp is a list of length 7, where each element is a vector of length 5, representing 5 measurements on a given day. This variable has already been defined in the workspace: type str(temp) to see its structure."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-4",
    "href": "Intermedio_R_C4.html#instructions-100-xp-4",
    "title": "10  The apply family",
    "section": "10.11 Instructions 100 XP",
    "text": "10.11 Instructions 100 XP\nUse lapply() to calculate the minimum (built-in function min()) of the temperature measurements for every day. Do the same thing but this time with sapply(). See how the output differs. Use lapply() to compute the the maximum (max()) temperature for each day. Again, use sapply() to solve the same question and see how lapply() and sapply() differ.\n\n\nE5.R\n\n# temp has already been defined in the workspace\n\n# Use lapply() to find each day's minimum temperature\nlapply(temp, min)\n\n# Use sapply() to find each day's minimum temperature\nsapply(temp, min)\n\n# Use lapply() to find each day's maximum temperature\nlapply(temp, max)\n\n# Use sapply() to find each day's maximum temperature\nsapply(temp, max)"
  },
  {
    "objectID": "Intermedio_R_C4.html#sapply-with-your-own-function",
    "href": "Intermedio_R_C4.html#sapply-with-your-own-function",
    "title": "10  The apply family",
    "section": "10.12 sapply with your own function",
    "text": "10.12 sapply with your own function\nLike lapply(), sapply() allows you to use self-defined functions and apply them over a vector or a list:\n\nsapply(X, FUN, …)\n\nHere, FUN can be one of R’s built-in functions, but it can also be a function you wrote. This self-written function can be defined before hand, or can be inserted directly as an anonymous function."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-5",
    "href": "Intermedio_R_C4.html#instructions-100-xp-5",
    "title": "10  The apply family",
    "section": "10.13 Instructions 100 XP",
    "text": "10.13 Instructions 100 XP\n\nFinish the definition of extremes_avg(): it takes a vector of temperatures and calculates the average of the minimum and maximum temperatures of the vector.\nNext, use this function inside sapply() to apply it over the vectors inside temp.\nUse the same function over temp with lapply() and see how the outputs differ.\n\n\n\nE6.R\n\n# temp is already defined in the workspace\n\n# Finish function definition of extremes_avg\nextremes_avg <- function(x) {\n  ( min(x) + max(x) ) / 2\n}\n\n# Apply extremes_avg() over temp using sapply()\nsapply(temp,extremes_avg)\n\n# Apply extremes_avg() over temp using lapply()\nlapply(temp,extremes_avg)"
  },
  {
    "objectID": "Intermedio_R_C4.html#apply-with-function-returning-vector",
    "href": "Intermedio_R_C4.html#apply-with-function-returning-vector",
    "title": "10  The apply family",
    "section": "10.14 apply with function returning vector",
    "text": "10.14 apply with function returning vector\nIn the previous exercises, you’ve seen how sapply() simplifies the list that lapply() would return by turning it into a vector. But what if the function you’re applying over a list or a vector returns a vector of length greater than 1? If you don’t remember from the video, don’t waste more time in the valley of ignorance and head over to the instructions!"
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-6",
    "href": "Intermedio_R_C4.html#instructions-100-xp-6",
    "title": "10  The apply family",
    "section": "10.15 Instructions 100 XP",
    "text": "10.15 Instructions 100 XP\n\nFinish the definition of the extremes() function. It takes a vector of numerical values and returns a vector containing the minimum and maximum values of a given vector, with the names “min” and “max”, respectively.\nApply this function over the vector temp using sapply().\nFinally, apply this function over the vector temp using lapply() as well.\n\n\n\nE7.R\n\n# temp is already available in the workspace\n\n# Create a function that returns min and max of a vector: extremes\nextremes <- function(x) {\n  c(min = min(x), max = max(x))\n}\n\n# Apply extremes() over temp with sapply()\nsapply(temp,extremes)\n\n# Apply extremes() over temp with lapply()\nlapply(temp,extremes)"
  },
  {
    "objectID": "Intermedio_R_C4.html#sapply-cant-simplify-now-what",
    "href": "Intermedio_R_C4.html#sapply-cant-simplify-now-what",
    "title": "10  The apply family",
    "section": "10.16 sapply can’t simplify, now what?",
    "text": "10.16 sapply can’t simplify, now what?\nIt seems like we’ve hit the jackpot with sapply(). On all of the examples so far, sapply() was able to nicely simplify the rather bulky output of lapply(). But, as with life, there are things you can’t simplify. How does sapply() react?\nWe already created a function, below_zero(), that takes a vector of numerical values and returns a vector that only contains the values that are strictly below zero."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-7",
    "href": "Intermedio_R_C4.html#instructions-100-xp-7",
    "title": "10  The apply family",
    "section": "10.17 Instructions 100 XP",
    "text": "10.17 Instructions 100 XP\n\nApply below_zero() over temp using sapply() and store the result in freezing_s.\nApply below_zero() over temp using lapply(). Save the resulting list in a variable freezing_l.\nCompare freezing_s to freezing_l using the identical() function.\n\n\n\nE8.R\n\n# temp is already prepared for you in the workspace\n\n# Definition of below_zero()\nbelow_zero <- function(x) {\n  return(x[x < 0])\n}\n\n# Apply below_zero over temp using sapply(): freezing_s\nfreezing_s <- sapply(temp,below_zero)\n\n# Apply below_zero over temp using lapply(): freezing_l\nfreezing_l <- lapply(temp,below_zero)\n\n# Are freezing_s and freezing_l identical?\nidentical(freezing_s,freezing_l)"
  },
  {
    "objectID": "Intermedio_R_C4.html#sapply-with-functions-that-return-null",
    "href": "Intermedio_R_C4.html#sapply-with-functions-that-return-null",
    "title": "10  The apply family",
    "section": "10.18 sapply with functions that return NULL",
    "text": "10.18 sapply with functions that return NULL\nYou already have some apply tricks under your sleeve, but you’re surely hungry for some more, aren’t you? In this exercise, you’ll see how sapply() reacts when it is used to apply a function that returns NULL over a vector or a list.\nA function print_info(), that takes a vector and prints the average of this vector, has already been created for you. It uses the cat() function."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-8",
    "href": "Intermedio_R_C4.html#instructions-100-xp-8",
    "title": "10  The apply family",
    "section": "10.19 Instructions 100 XP",
    "text": "10.19 Instructions 100 XP\nApply print_info() over the contents of temp with sapply(). Repeat this process with lapply(). Do you notice the difference?\n\n\nE9.R\n\n# temp is already available in the workspace\n\n# Definition of print_info()\nprint_info <- function(x) {\n  cat(\"The average temperature is\", mean(x), \"\\n\")\n}\n\n# Apply print_info() over temp using sapply()\nsapply(temp,print_info)\n\n# Apply print_info() over temp using lapply()\nlapply(temp,print_info)"
  },
  {
    "objectID": "Intermedio_R_C4.html#reverse-engineering-sapply",
    "href": "Intermedio_R_C4.html#reverse-engineering-sapply",
    "title": "10  The apply family",
    "section": "10.20 Reverse engineering sapply",
    "text": "10.20 Reverse engineering sapply\n\nsapply(list(runif (10), runif (10)), function(x) c(min = min(x), mean = mean(x), max = max(x)))\n\nWithout going straight to the console to run the code, try to reason through which of the following statements are correct and why.\n\nsapply() can’t simplify the result that lapply() would return, and thus returns a list of vectors.\nThis code generates a matrix with 3 rows and 2 columns.\nThe function that is used inside sapply() is anonymous.\nThe resulting data structure does not contain any names.\n\nSelect the option that lists all correct statements."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-50-xp-1",
    "href": "Intermedio_R_C4.html#instructions-50-xp-1",
    "title": "10  The apply family",
    "section": "Instructions 50 XP",
    "text": "Instructions 50 XP\nPossible Answers\n\n\nand (3)\n\n\nand (3) Respuesta\n\n\nand (4)\n\n(2), (3) and (4)"
  },
  {
    "objectID": "Intermedio_R_C4.html#use-vapply",
    "href": "Intermedio_R_C4.html#use-vapply",
    "title": "10  The apply family",
    "section": "10.21 Use vapply",
    "text": "10.21 Use vapply\nBefore you get your hands dirty with the third and last apply function that you’ll learn about in this intermediate R course, let’s take a look at its syntax. The function is called vapply(), and it has the following syntax:\n\nvapply(X, FUN, FUN.VALUE, …, USE.NAMES = TRUE)\n\nOver the elements inside X, the function FUN is applied. The FUN.VALUE argument expects a template for the return argument of this function FUN. USE.NAMES is TRUE by default; in this case vapply() tries to generate a named array, if possible.\nFor the next set of exercises, you’ll be working on the temp list again, that contains 7 numerical vectors of length 5. We also coded a function basics() that takes a vector, and returns a named vector of length 3, containing the minimum, mean and maximum value of the vector respectively."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-9",
    "href": "Intermedio_R_C4.html#instructions-100-xp-9",
    "title": "10  The apply family",
    "section": "10.22 Instructions 100 XP",
    "text": "10.22 Instructions 100 XP\nApply the function basics() over the list of temperatures, temp, using vapply(). This time, you can use numeric(3) to specify the FUN.VALUE argument.\n\n\nE10.R\n\n# temp is already available in the workspace\n\n# Definition of basics()\nbasics <- function(x) {\n  c(min = min(x), mean = mean(x), max = max(x))\n}\n\n# Apply basics() over temp using vapply()\nvapply(temp, basics, numeric(3))"
  },
  {
    "objectID": "Intermedio_R_C4.html#use-vapply-2",
    "href": "Intermedio_R_C4.html#use-vapply-2",
    "title": "10  The apply family",
    "section": "10.23 Use vapply (2)",
    "text": "10.23 Use vapply (2)\nSo far you’ve seen that vapply() mimics the behavior of sapply() if everything goes according to plan. But what if it doesn’t?\nIn the video, Filip showed you that there are cases where the structure of the output of the function you want to apply, FUN, does not correspond to the template you specify in FUN.VALUE. In that case, vapply() will throw an error that informs you about the misalignment between expected and actual output."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-10",
    "href": "Intermedio_R_C4.html#instructions-100-xp-10",
    "title": "10  The apply family",
    "section": "10.24 Instructions 100 XP",
    "text": "10.24 Instructions 100 XP\n\nInspect the pre-loaded code and try to run it. If you haven’t changed anything,an error should pop up. That’s because vapply() still expects basics() to return a vector of length 3. The error message gives you an indication of what’s wrong.\nTry to fix the error by editing the vapply() command.\n\n\n\nE11.R\n\n# temp is already available in the workspace\n\n# Definition of the basics() function\nbasics <- function(x) {\n  c(min = min(x), mean = mean(x), median = median(x), max = max(x))\n}\n\n# Fix the error:\nvapply(temp, basics, numeric(4))"
  },
  {
    "objectID": "Intermedio_R_C4.html#from-sapply-to-vapply",
    "href": "Intermedio_R_C4.html#from-sapply-to-vapply",
    "title": "10  The apply family",
    "section": "10.25 From sapply to vapply",
    "text": "10.25 From sapply to vapply\nAs highlighted before, vapply() can be considered a more robust version of sapply(), because you explicitly restrict the output of the function you want to apply. Converting your sapply() expressions in your own R scripts to vapply() expressions is therefore a good practice (and also a breeze!)."
  },
  {
    "objectID": "Intermedio_R_C4.html#instructions-100-xp-11",
    "href": "Intermedio_R_C4.html#instructions-100-xp-11",
    "title": "10  The apply family",
    "section": "10.26 Instructions 100 XP",
    "text": "10.26 Instructions 100 XP\nConvert all the sapply() expressions on the right to their vapply() counterparts. Their results should be exactly the same; you’re only adding robustness. You’ll need the templates numeric(1) and logical(1).\n\n\nE12.R\n\n# temp is already defined in the workspace\n\n# Convert to vapply() expression\nvapply(temp, max,numeric(1))\n\n# Convert to vapply() expression\nvapply(temp, function(x, y) { mean(x) > y }, y = 5,logical(1))"
  },
  {
    "objectID": "Intermedio_R_C5.html#mathematical-utilities",
    "href": "Intermedio_R_C5.html#mathematical-utilities",
    "title": "11  Utilities",
    "section": "11.1 Mathematical utilities",
    "text": "11.1 Mathematical utilities\nHave another look at some useful math functions that R features:\n\nabs(): Calculate the absolute value.\nsum(): Calculate the sum of all the values in a data structure.\nmean(): Calculate the arithmetic mean.\nround(): Round the values to 0 decimal places by default. Try out ?round in the console for variations of round() and ways to change the number of digits to round to.\n\nAs a data scientist in training, you’ve estimated a regression model on the sales data for the past six months. After evaluating your model, you see that the training error of your model is quite regular, showing both positive and negative values. A vector errors containing the error values has been pre-defined for you."
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp",
    "href": "Intermedio_R_C5.html#instructions-100-xp",
    "title": "11  Utilities",
    "section": "11.2 Instructions 100 XP",
    "text": "11.2 Instructions 100 XP\nCalculate the sum of the absolute rounded values of the training errors. You can work in parts, or with a single one-liner. There’s no need to store the result in a variable, just have R print it.\n\n\nE1.R\n\n# The errors vector has already been defined for you\nerrors <- c(1.9, -2.6, 4.0, -9.5, -3.4, 7.3)\n\n# Sum of absolute rounded values of errors\nsum(abs(round(errors)))"
  },
  {
    "objectID": "Intermedio_R_C5.html#find-the-error",
    "href": "Intermedio_R_C5.html#find-the-error",
    "title": "11  Utilities",
    "section": "11.3 Find the error",
    "text": "11.3 Find the error\nWe went ahead and pre-loaded some code for you, but there’s still an error. Can you trace it and fix it?\nIn times of despair, help with functions such as sum() and rev() are a single command away; simply execute the code ?sum and ?rev."
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-1",
    "href": "Intermedio_R_C5.html#instructions-100-xp-1",
    "title": "11  Utilities",
    "section": "11.4 Instructions 100 XP",
    "text": "11.4 Instructions 100 XP\nFix the error by including code on the last line. Remember: you want to call mean() only once!\n\n\nE2.R\n\n# Don't edit these two lines\nvec1 <- c(1.5, 2.5, 8.4, 3.7, 6.3)\nvec2 <- rev(vec1)\n\n# Fix the error\nmean(abs(vec1)) \nmean(abs(vec2))\n\n# ?mean()\n# ?rev()"
  },
  {
    "objectID": "Intermedio_R_C5.html#data-utilities",
    "href": "Intermedio_R_C5.html#data-utilities",
    "title": "11  Utilities",
    "section": "11.5 Data Utilities",
    "text": "11.5 Data Utilities\nR features a bunch of functions to juggle around with data structures::\n\nseq(): Generate sequences, by specifying the from, to, and by arguments.\nrep(): Replicate elements of vectors and lists.\nsort(): Sort a vector in ascending order. Works on numerics, but also on\ncharacter strings and logicals.\nrev(): Reverse the elements in a data structures for which reversal is defined.\nstr(): Display the structure of any R object.\nappend(): Merge vectors or lists.\nis.*(): Check for the class of an R object.\nas.*(): Convert an R object from one class to another.\nunlist(): Flatten (possibly embedded) lists to produce a vector.\n\nRemember the social media profile views data? Your LinkedIn and Facebook view counts for the last seven days have been pre-defined as lists."
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-2",
    "href": "Intermedio_R_C5.html#instructions-100-xp-2",
    "title": "11  Utilities",
    "section": "11.6 Instructions 100 XP",
    "text": "11.6 Instructions 100 XP\n\nConvert both linkedin and facebook lists to a vector, and store them as li_vec and fb_vec respectively.\nNext, append fb_vec to the li_vec (Facebook data comes last). Save the result as social_vec.\nFinally, sort social_vec from high to low. Print the resulting vector.\n\n\n\nE3.R\n\n# The linkedin and facebook lists have already been created for you\nlinkedin <- list(16, 9, 13, 5, 2, 17, 14)\nfacebook <- list(17, 7, 5, 16, 8, 13, 14)\n\n# Convert linkedin and facebook to a vector: li_vec and fb_vec\nli_vec <- as.vector(linkedin)\nfb_vec <- as.vector(facebook)\n\n# Append fb_vec to li_vec: social_vec\nsocial_vec <-unlist(append(li_vec, fb_vec))\n\n# Sort social_vec\nsort(social_vec,decreasing = TRUE)"
  },
  {
    "objectID": "Intermedio_R_C5.html#find-the-error-2",
    "href": "Intermedio_R_C5.html#find-the-error-2",
    "title": "11  Utilities",
    "section": "11.7 Find the error (2)",
    "text": "11.7 Find the error (2)\nJust as before, let’s switch roles. It’s up to you to see what unforgivable mistakes we’ve made. Go fix them!"
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-3",
    "href": "Intermedio_R_C5.html#instructions-100-xp-3",
    "title": "11  Utilities",
    "section": "11.8 Instructions 100 XP",
    "text": "11.8 Instructions 100 XP\nCorrect the expression. Make sure that your fix still uses the functions rep() and seq().\n\n\nE4.R\n\n# Fix me\nrep(seq(1, 7, by = 2), times = 7)"
  },
  {
    "objectID": "Intermedio_R_C5.html#beat-gauss-using-r",
    "href": "Intermedio_R_C5.html#beat-gauss-using-r",
    "title": "11  Utilities",
    "section": "11.9 Beat Gauss using R",
    "text": "11.9 Beat Gauss using R\nThere is a popular story about young Gauss. As a pupil, he had a lazy teacher who wanted to keep the classroom busy by having them add up the numbers 1 to 100. Gauss came up with an answer almost instantaneously, 5050. On the spot, he had developed a formula for calculating the sum of an arithmetic series. There are more general formulas for calculating the sum of an arithmetic series with different starting values and increments. Instead of deriving such a formula, why not use R to calculate the sum of a sequence?"
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-4",
    "href": "Intermedio_R_C5.html#instructions-100-xp-4",
    "title": "11  Utilities",
    "section": "11.10 Instructions 100 XP",
    "text": "11.10 Instructions 100 XP\nUsing the function seq(), create a sequence that ranges from 1 to 500 in increments of 3. Assign the resulting vector to a variable seq1. Again with the function seq(), create a sequence that ranges from 1200 to 900 in increments of -7. Assign it to a variable seq2. Calculate the total sum of the sequences, either by using the sum() function twice and adding the two results, or by first concatenating the sequences and then using the sum() function once. Print the result to the console.\n\n\nE5.R\n\n# Create first sequence: seq1\nseq1<- seq(from = 1, to = 500, by = 3)\n\n# Create second sequence: seq2\nseq2<- seq(from = 1200, to = 900, by = -7)\n\n# Calculate total sum of the sequences\nsum(seq1, seq2)"
  },
  {
    "objectID": "Intermedio_R_C5.html#grepl-grep",
    "href": "Intermedio_R_C5.html#grepl-grep",
    "title": "11  Utilities",
    "section": "11.11 grepl & grep",
    "text": "11.11 grepl & grep\nIn their most basic form, regular expressions can be used to see whether a pattern exists inside a character string or a vector of character strings. For this purpose, you can use:\n\ngrepl(), which returns TRUE when a pattern is found in the corresponding character string.\ngrep(), which returns a vector of indices of the character strings that contains the pattern.\n\nBoth functions need a pattern and an x argument, where pattern is the regular expression you want to match for, and the x argument is the character vector from which matches should be sought.\nIn this and the following exercises, you’ll be querying and manipulating a character vector of email addresses! The vector emails has been pre-defined so you can begin with the instructions straight away!"
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-5",
    "href": "Intermedio_R_C5.html#instructions-100-xp-5",
    "title": "11  Utilities",
    "section": "11.12 Instructions 100 XP",
    "text": "11.12 Instructions 100 XP\n\nUse grepl() to generate a vector of logicals that indicates whether these email addresses contain “edu”. Print the result to the output.\nDo the same thing with grep(), but this time save the resulting indexes in a variable hits.\nUse the variable hits to select from the emails vector only the emails that contain “edu”.\n\n\n\nE6.R\n\n# The emails vector has already been defined for you\nemails <- c(\"john.doe@ivyleague.edu\", \"education@world.gov\", \n\"dalai.lama@peace.org\",\n            \"invalid.edu\", \"quant@bigdatacollege.edu\",\n            \"cookie.monster@sesame.tv\")\n\n# Use grepl() to match for \"edu\"\ngrepl( \"edu\",  emails)\n\n# Use grep() to match for \"edu\", save result to hits\nhits <- grep(\"edu\", emails)\n\n\n# Subset emails using hits\nemails[hits]"
  },
  {
    "objectID": "Intermedio_R_C5.html#repl-grep-2",
    "href": "Intermedio_R_C5.html#repl-grep-2",
    "title": "11  Utilities",
    "section": "11.13 repl & grep (2)",
    "text": "11.13 repl & grep (2)\nYou can use the caret, ^, and the dollar sign, $ to match the content located in the start and end of a string, respectively. This could take us one step closer to a correct pattern for matching only the “.edu” email addresses from our list of emails. But there’s more that can be added to make the pattern more robust:\n\n@, because a valid email must contain an at-sign.\n., which matches any character (.) zero or more times (). Both the dot and the asterisk are metacharacters. You can use them to match any character between the at-sign and the “.edu” portion of an email address.\n\\.edu$, to match the “.edu” part of the email at the end of the string. The \\ part escapes the dot: it tells R that you want to use the . as an actual character."
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-6",
    "href": "Intermedio_R_C5.html#instructions-100-xp-6",
    "title": "11  Utilities",
    "section": "11.14 Instructions 100 XP",
    "text": "11.14 Instructions 100 XP\n\nUse grepl() with the more advanced regular expression to return a logical vector. Simply print the result.\nDo a similar thing with grep() to create a vector of indices. Store the result in the variable hits.\nUse emails[hits] again to subset the emails vector.\n\n\n\nE7.R\n\n# The emails vector has already been defined for you\nemails <- c(\"john.doe@ivyleague.edu\", \"education@world.gov\",\n\"dalai.lama@peace.org\",\n            \"invalid.edu\", \"quant@bigdatacollege.edu\",\n            \"cookie.monster@sesame.tv\")\n\n# Use grepl() to match for .edu addresses more robustly\ngrepl(pattern=\"@.*\\\\.edu$\",emails)\n\n# Use grep() to match for .edu addresses more robustly, save result to hits\nhits <- grep(\"@.*\\\\.edu$\", emails)\n\n# Subset emails using hits\nemails[hits]"
  },
  {
    "objectID": "Intermedio_R_C5.html#sub-gsub",
    "href": "Intermedio_R_C5.html#sub-gsub",
    "title": "11  Utilities",
    "section": "11.15 sub & gsub",
    "text": "11.15 sub & gsub\nWhile grep() and grepl() were used to simply check whether a regular expression could be matched with a character vector, sub() and gsub() take it one step further: you can specify a replacement argument. If inside the character vector x, the regular expression pattern is found, the matching element(s) will be replaced with replacement. sub() only replaces the first match, whereas gsub() replaces all matches.\nSuppose that emails vector you’ve been working with is an excerpt of DataCamp’s email database. Why not offer the owners of the .edu email addresses a new email address on the datacamp.edu domain? This could be quite a powerful marketing stunt: Online education is taking over traditional learning institutions! Convert your email and be a part of the new generation!"
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-7",
    "href": "Intermedio_R_C5.html#instructions-100-xp-7",
    "title": "11  Utilities",
    "section": "11.16 Instructions 100 XP",
    "text": "11.16 Instructions 100 XP\nWith the advanced regular expression “@.*\\.edu$“, use sub() to replace the match with”(datacamp.edu?)“. Since there will only be one match per character string, gsub() is not necessary here. Inspect the resulting output.\n\n\nE8.R\n\n# The emails vector has already been defined for you\nemails <- c(\"john.doe@ivyleague.edu\", \"education@world.gov\",\n\"global@peace.org\",\n            \"invalid.edu\", \"quant@bigdatacollege.edu\",\n            \"cookie.monster@sesame.tv\")\n\n# Use sub() to convert the email domains to datacamp.edu\nsub(\"@.*\\\\.edu$\", \"@datacamp.edu\", emails)"
  },
  {
    "objectID": "Intermedio_R_C5.html#sub-gsub-2",
    "href": "Intermedio_R_C5.html#sub-gsub-2",
    "title": "11  Utilities",
    "section": "11.17 sub & gsub (2)",
    "text": "11.17 sub & gsub (2)\nRegular expressions are a typical concept that you’ll learn by doing and by seeing other examples. Before you rack your brains over the regular expression in this exercise, have a look at the new things that will be used:\n\n.*: A usual suspect! It can be read as “any character that is matched zero or more times”.\n\\s: Match a space. The “s” is normally a character, escaping it (\\) makes it a metacharacter.\n[0-9]+: Match the numbers 0 to 9, at least once (+).\n([0-9]+): The parentheses are used to make parts of the matching string available to define the replacement. The \\1 in the replacement argument of sub() gets set to the string that is captured by the regular expression [0-9]+.\n\n\nawards <- c(“Won 1 Oscar.”, “Won 1 Oscar. Another 9 wins & 24 nominations.”, “1 win and 2 nominations.”, “2 wins & 3 nominations.”, “Nominated for 2 Golden Globes. 1 more win & 2 nominations.”, “4 wins & 1 nomination.”)\n\nsub(“.\\s([0-9]+)\\snomination.$”, “\\1”, awards) What does this code chunk return? awards is already defined in the workspace so you can start playing in the console straight away."
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-50-xp",
    "href": "Intermedio_R_C5.html#instructions-50-xp",
    "title": "11  Utilities",
    "section": "Instructions 50 XP",
    "text": "Instructions 50 XP\nPossible Answers\n\nA vector of integers containing: 1, 24, 2, 3, 2, 1.\nThe vector awards gets returned as there isn’t a single element in awards that matches the regular expression.\nA vector of character strings containing “1”, “24”, “2”, “3”, “2”, “1”.\nA vector of character strings containing “Won 1 Oscar.”, “24”, “2”, “3”, “2”, “1”. Respuesta"
  },
  {
    "objectID": "Intermedio_R_C5.html#right-here-right-now",
    "href": "Intermedio_R_C5.html#right-here-right-now",
    "title": "11  Utilities",
    "section": "11.18 Right here, right now",
    "text": "11.18 Right here, right now\nIn R, dates are represented by Date objects, while times are represented by POSIXct objects. Under the hood, however, these dates and times are simple numerical values. Date objects store the number of days since the 1st of January in 1970. POSIXct objects on the other hand, store the number of seconds since the 1st of January in 1970.\nThe 1st of January in 1970 is the common origin for representing times and dates in a wide range of programming languages. There is no particular reason for this; it is a simple convention. Of course, it’s also possible to create dates and times before 1970; the corresponding numerical values are simply negative in this case."
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-8",
    "href": "Intermedio_R_C5.html#instructions-100-xp-8",
    "title": "11  Utilities",
    "section": "11.19 Instructions 100 XP",
    "text": "11.19 Instructions 100 XP\n\nAsk R for the current date, and store the result in a variable today.\nTo see what today looks like under the hood, call unclass() on it.\nAsk R for the current time, and store the result in a variable, now.\nTo see the numerical value that corresponds to now, call unclass() on it.\n\n\n\nE9.R\n\n# Get the current date: today\ntoday <- Sys.Date()\ntoday\n\n# See what today looks like under the hood\nunclass(today)\n\n# Get the current time: now\nnow <- Sys.Date()\nnow\n\n# See what now looks like under the hood\nunclass(now)"
  },
  {
    "objectID": "Intermedio_R_C5.html#create-and-format-dates",
    "href": "Intermedio_R_C5.html#create-and-format-dates",
    "title": "11  Utilities",
    "section": "11.20 Create and format dates",
    "text": "11.20 Create and format dates\nTo create a Date object from a simple character string in R, you can use the as.Date() function. The character string has to obey a format that can be defined using a set of symbols (the examples correspond to 13 January, 1982):\n\n%Y: 4-digit year (1982)\n%y: 2-digit year (82)\n%m: 2-digit month (01)\n%d: 2-digit day of the month (13)\n%A: weekday (Wednesday)\n%a: abbreviated weekday (Wed)\n%B: month (January)\n%b: abbreviated month (Jan)\n\nThe following R commands will all create the same Date object for the 13th day in January of 1982:\n\nas.Date(“1982-01-13”) as.Date(“Jan-13-82”, format = “%b-%d-%y”) as.Date(“13 January, 1982”, format = “%d %B, %Y”)\n\nNotice that the first line here did not need a format argument, because by default R matches your character string to the formats “%Y-%m-%d” or “%Y/%m/%d”.\nIn addition to creating dates, you can also convert dates to character strings that use a different date notation. For this, you use the format() function. Try the following lines of code:\n\ntoday <- Sys.Date() format(Sys.Date(), format = “%d %B, %Y”) format(Sys.Date(), format = “Today is a %A!”)"
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-9",
    "href": "Intermedio_R_C5.html#instructions-100-xp-9",
    "title": "11  Utilities",
    "section": "11.21 Instructions 100 XP",
    "text": "11.21 Instructions 100 XP\n\nThree character strings representing dates have been created for you. Convert them to dates using as.Date(), and assign them to date1, date2, and date3 respectively. The code for date1 is already included.\nExtract useful information from the dates as character strings using format(). From the first date, select the weekday. From the second date, select the day of the month. From the third date, you should select the abbreviated month and the 4-digit year, separated by a space.\n\n\n\nE10.R\n\n# Definition of character strings representing dates\nstr1 <- \"May 23, '96\"\nstr2 <- \"2012-03-15\"\nstr3 <- \"30/January/2006\"\n\n# Convert the strings to dates: date1, date2, date3\ndate1 <- as.Date(str1, format = \"%b %d, '%y\")\ndate2 <- as.Date(str2) #, format = \"%y %b, '%d\")\ndate3 <- as.Date(str3, format = \"%d/%B/%Y\")\n\n# Convert dates to formatted strings\nformat(date1, \"%A\")\nformat(date2, \"%d\")\nformat(date3, \"%b %Y\")"
  },
  {
    "objectID": "Intermedio_R_C5.html#create-and-format-times",
    "href": "Intermedio_R_C5.html#create-and-format-times",
    "title": "11  Utilities",
    "section": "11.22 Create and format times",
    "text": "11.22 Create and format times\nSimilar to working with dates, you can use as.POSIXct() to convert from a character string to a POSIXct object, and format() to convert from a POSIXct object to a character string. Again, you have a wide variety of symbols:\n\n%H: hours as a decimal number (00-23)\n%I: hours as a decimal number (01-12)\n%M: minutes as a decimal number\n%S: seconds as a decimal number\n%T: shorthand notation for the typical format %H:%M:%S\n%p: AM/PM indicator\n\nFor a full list of conversion symbols, consult the strptime documentation in the console:\n\n?strptime\n\nAgain,as.POSIXct() uses a default format to match character strings. In this case, it’s %Y-%m-%d %H:%M:%S. In this exercise, abstraction is made of different time zones."
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-10",
    "href": "Intermedio_R_C5.html#instructions-100-xp-10",
    "title": "11  Utilities",
    "section": "11.23 Instructions 100 XP",
    "text": "11.23 Instructions 100 XP\n\nConvert two strings that represent timestamps, str1 and str2, to POSIXct objects called time1 and time2.\nUsing format(), create a string from time1 containing only the minutes.\nFrom time2, extract the hours and minutes as “hours:minutes AM/PM”. Refer to the assignment text above to find the correct conversion symbols!\n\n\n\nE11.R\n\n# Definition of character strings representing times\nstr1 <- \"May 23, '96 hours:23 minutes:01 seconds:45\"\nstr2 <- \"2012-3-12 14:23:08\"\n\n# Convert the strings to POSIXct objects: time1, time2\ntime1 <- as.POSIXct(str1, format = \"%B %d, '%y hours:%H minutes:%M seconds:%S\")\ntime2 <- as.POSIXct(str2)\n\n# Convert times to formatted strings\nformat(time1, \"%M\")\nformat(time2, \"%I:%M %p\")"
  },
  {
    "objectID": "Intermedio_R_C5.html#calculations-with-dates",
    "href": "Intermedio_R_C5.html#calculations-with-dates",
    "title": "11  Utilities",
    "section": "11.24 Calculations with Dates",
    "text": "11.24 Calculations with Dates\nBoth Date and POSIXct R objects are represented by simple numerical values under the hood. This makes calculation with time and date objects very straightforward: R performs the calculations using the underlying numerical values, and then converts the result back to human-readable time information again.\nYou can increment and decrement Date objects, or do actual calculations with them:\n\ntoday <- Sys.Date() today + 1 today - 1\n\n\nas.Date(“2015-03-12”) - as.Date(“2015-02-27”)\n\nTo control your eating habits, you decided to write down the dates of the last five days that you ate pizza. In the workspace, these dates are defined as five Date objects, day1 to day5. A vector pizza containing these 5 Date objects has been pre-defined for you."
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-11",
    "href": "Intermedio_R_C5.html#instructions-100-xp-11",
    "title": "11  Utilities",
    "section": "11.25 Instructions 100 XP",
    "text": "11.25 Instructions 100 XP\n\nCalculate the number of days that passed between the last and the first day you ate pizza. Print the result.\nUse the function diff() on pizza to calculate the differences between consecutive pizza days. Store the result in a new variable day_diff.\nCalculate the average period between two consecutive pizza days. Print the result.\n\n\n\nE12.R\n\n# day1, day2, day3, day4 and day5 are already available in the workspace\n\n# Difference between last and first pizza day\nday5 - day1\n\n# Create vector pizza\npizza <- c(day1, day2, day3, day4, day5)\n\n# Create differences between consecutive pizza days: day_diff\nday_diff <- diff(pizza, lag = 1, differences = 1)\nday_diff\n\n# Average period between two consecutive pizza days\nprint(mean(day_diff))"
  },
  {
    "objectID": "Intermedio_R_C5.html#calculations-with-times",
    "href": "Intermedio_R_C5.html#calculations-with-times",
    "title": "11  Utilities",
    "section": "11.26 Calculations with Times",
    "text": "11.26 Calculations with Times\nCalculations using POSIXct objects are completely analogous to those using Date objects. Try to experiment with this code to increase or decrease POSIXct objects:\n\nnow <- Sys.time() now + 3600 # add an hour now - 3600 * 24 # subtract a day\n\nAdding or subtracting time objects is also straightforward:\n\nbirth <- as.POSIXct(“1879-03-14 14:37:23”) death <- as.POSIXct(“1955-04-18 03:47:12”) einstein <- death - birth einstein\n\nYou’re developing a website that requires users to log in and out. You want to know what is the total and average amount of time a particular user spends on your website. This user has logged in 5 times and logged out 5 times as well. These times are gathered in the vectors login and logout, which are already defined in the workspace."
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-12",
    "href": "Intermedio_R_C5.html#instructions-100-xp-12",
    "title": "11  Utilities",
    "section": "11.27 Instructions 100 XP",
    "text": "11.27 Instructions 100 XP\n\nCalculate the difference between the two vectors logout and login, i.e. the time the user was online in each independent session. Store the result in a variable time_online.\nInspect the variable time_online by printing it.\nCalculate the total time that the user was online. Print the result.\nCalculate the average time the user was online. Print the result.\n\n\n\nE13.R\n\n# login and logout are already defined in the workspace\n# Calculate the difference between login and logout: time_online\ntime_online <- logout- login\n\n# Inspect the variable time_online\ntime_online\n\n# Calculate the total time online\nsum(time_online)\n\n# Calculate the average time online\nmean(time_online)"
  },
  {
    "objectID": "Intermedio_R_C5.html#time-is-of-the-essence",
    "href": "Intermedio_R_C5.html#time-is-of-the-essence",
    "title": "11  Utilities",
    "section": "11.28 Time is of the essence",
    "text": "11.28 Time is of the essence\nThe dates when a season begins and ends can vary depending on who you ask. People in Australia will tell you that spring starts on September 1st. The Irish people in the Northern hemisphere will swear that spring starts on February 1st, with the celebration of St. Brigid’s Day. Then there’s also the difference between astronomical and meteorological seasons: while astronomers are used to equinoxes and solstices, meteorologists divide the year into 4 fixed seasons that are each three months long. (source: www.timeanddate.com)\nA vector astro, which contains character strings representing the dates on which the 4 astronomical seasons start, has been defined on your workspace. Similarly, a vector meteo has already been created for you, with the meteorological beginnings of a season."
  },
  {
    "objectID": "Intermedio_R_C5.html#instructions-100-xp-13",
    "href": "Intermedio_R_C5.html#instructions-100-xp-13",
    "title": "11  Utilities",
    "section": "11.29 Instructions 100 XP",
    "text": "11.29 Instructions 100 XP\n\nUse as.Date() to convert the astro vector to a vector containing Date objects. You will need the %d, %b and %Y symbols to specify the format. Store the resulting vector as astro_dates.\nUse as.Date() to convert the meteo vector to a vector with Date objects. This time, you will need the %B, %d and %y symbols for the format argument. Store the resulting vector as meteo_dates.\nWith a combination of max(), abs() and -, calculate the maximum absolute difference between the astronomical and the meteorological beginnings of a season, i.e. astro_dates and meteo_dates. Simply print this maximum difference to the console output.\n\n\n\nE14.R\n\n# Convert astro to vector of Date objects: astro_dates\nastro_dates <-as.Date(astro, format = \"%d-%b-%Y\")\n\n# Convert meteo to vector of Date objects: meteo_dates\nmeteo_dates <-as.Date(meteo, format = \"%B %d, %y\")\n\n# Calculate the maximum absolute difference between astro_dates and meteo_dates\nmax(abs(astro_dates - meteo_dates))"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#calling-functions",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#calling-functions",
    "title": "12  How to Write a Function",
    "section": "12.1 Calling functions",
    "text": "12.1 Calling functions\nOne way to make your code more readable is to be careful about the order you pass arguments when you call functions, and whether you pass the arguments by position or by name.\ngold_medals, a numeric vector of the number of gold medals won by each country in the 2016 Summer Olympics, is provided.\nFor convenience, the arguments of median() and rank() are displayed using args(). Setting rank()’s na.last argument to “keep” means “keep the rank of NA values as NA”.\nBest practice for calling functions is to include them in the order shown by args(), and to only name rare arguments."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp",
    "title": "12  How to Write a Function",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nThe final line calculates the median number of gold medals each country won.\nRewrite the call to median(), following best practices.\n\n\n\nE1.R\n\n# Look at the gold medals data\ngold_medals\n\n# Note the arguments to median()\nargs(median)\n\n# Rewrite this function call, following best practices\nmedian(gold_medals, na.rm = TRUE)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#the-benefits-of-writing-functions",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#the-benefits-of-writing-functions",
    "title": "12  How to Write a Function",
    "section": "12.2 The benefits of writing functions",
    "text": "12.2 The benefits of writing functions\nThere are lots of great reasons that you should write your own functions.\nWhich of these is not one of them?"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#answer-the-question-50xp",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#answer-the-question-50xp",
    "title": "12  How to Write a Function",
    "section": "Answer the question 50XP",
    "text": "Answer the question 50XP\nPossible Answers\n\nYou can type less code, saving effort and making your analyses more readable. Respuesta\nYou make less “copy and paste”-related errors.\nYou can reuse your code from project to project.\nYou can make your code harder to read, potentially improving your job security because only you can maintain it."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#your-first-function-tossing-a-coin",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#your-first-function-tossing-a-coin",
    "title": "12  How to Write a Function",
    "section": "12.3 Your first function: tossing a coin",
    "text": "12.3 Your first function: tossing a coin\nTime to write your first function! It’s a really good idea when writing functions to start simple. You can always make a function more complicated later if it’s really necessary, so let’s not worry about arguments for now."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp-1",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp-1",
    "title": "12  How to Write a Function",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSimulate a single coin toss by using sample() to sample from coin_sides once.\nWrite a template for your function, naming it toss_coin. The function should take no arguments. Don’t include the body of the function yet.\nCopy your script, and paste it into the function body.\nCall your function.\n\n\n\nE2.R\n\ncoin_sides <- c(\"head\", \"tail\")\n\n# Sample from coin_sides once\nsample(coin_sides,1)\n\n# Write a template for your function, toss_coin()\ntoss_coin <- function() {\n  \n\n  # (Leave the contents of the body for later)\n# Add punctuation to finish the body\n}\n\n# Your script, from a previous step\ncoin_sides <- c(\"head\", \"tail\")\n\n# Paste your script into the function body\ntoss_coin <- function() {\n  sample(coin_sides, 1)\n  \n}\n\n# Your functions, from previous steps\ntoss_coin <- function() {\n  coin_sides <- c(\"head\", \"tail\")\n  sample(coin_sides, 1)\n}\n\n# Call your function\ntoss_coin()"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#inputs-to-functions",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#inputs-to-functions",
    "title": "12  How to Write a Function",
    "section": "12.4 Inputs to functions",
    "text": "12.4 Inputs to functions\nMost functions require some sort of input to determine what to compute. The inputs to functions are called arguments. You specify them inside the parentheses after the word “function.”\nAs mentioned in the video, the following exercises assume that you are using sample() to do random sampling."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp-2",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp-2",
    "title": "12  How to Write a Function",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSample from coin_sides n_flips times with replacement.\nUpdate the definition of toss_coin() to accept a single argument, n_flips. The function should sample coin_sides n_flips times with replacement. Remember to change the signature and the body.\nGenerate 10 coin flips.\n\n\n\nE3.R\n\ncoin_sides <- c(\"head\", \"tail\")\nn_flips <- 10\n\n# Sample from coin_sides n_flips times with replacement\nsample(coin_sides,n_flips,replace = TRUE)\n\n# Update the function to return n coin tosses\ntoss_coin <- function(n_flips) {\n  coin_sides <- c(\"head\", \"tail\")\n  sample(coin_sides, n_flips,replace = TRUE)\n}\n\n# Generate 10 coin tosses\ntoss_coin(10)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#multiple-inputs-to-functions",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#multiple-inputs-to-functions",
    "title": "12  How to Write a Function",
    "section": "12.5 Multiple inputs to functions",
    "text": "12.5 Multiple inputs to functions\nIf a function should have more than one argument, list them in the function signature, separated by commas.\nTo solve this exercise, you need to know how to specify sampling weights to sample(). Set the prob argument to a numeric vector with the same length as x. Each value of prob is the probability of sampling the corresponding element of x, so their values add up to one. In the following example, each sample has a 20% chance of “bat”, a 30% chance of “cat” and a 50% chance of “rat”.\n\nsample(c(“bat”, “cat”, “rat”), 10, replace = TRUE, prob = c(0.2, 0.3, 0.5))\n\n##Instructions 100 XP {.unnumbered}\n\nBias the coin by weighting the sampling. Specify the prob argument so that heads are sampled with probability p_head (and tails are sampled with probability 1 - p_head).\nUpdate the definition of toss_coin() so it accepts an argument, p_head, and weights the samples using the code you wrote in the previous step.\nGenerate 10 coin tosses with an 80% chance of each head.\n\n\n\nE4.R\n\n\ncoin_sides <- c(\"head\", \"tail\")\nn_flips <- 10\np_head <- 0.8\n\n# Define a vector of weights\nweights <- c(p_head, 1 - p_head)\n\n# Update so that heads are sampled with prob p_head\nsample(coin_sides, n_flips, replace = TRUE, prob = weights)\n\n# Update the function so heads have probability p_head\ntoss_coin <- function(n_flips,p_head) {\n  coin_sides <- c(\"head\", \"tail\")\n  # Define a vector of weights\n  weights <- c(p_head,1-p_head)\n  # Modify the sampling to be weighted\n  sample(coin_sides, n_flips, replace = TRUE,prob=weights)\n}\n\n# Generate 10 coin tosses\ntoss_coin(10,0.8)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#renaming-glm",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#renaming-glm",
    "title": "12  How to Write a Function",
    "section": "12.6 Renaming GLM",
    "text": "12.6 Renaming GLM\nR’s generalized linear regression function, glm(), suffers the same usability problems as lm(): its name is an acronym, and its formula and data arguments are in the wrong order.\nTo solve this exercise, you need to know two things about generalized linear regression:\nglm() formulas are specified like lm() formulas: response is on the left, and explanatory variables are added on the right. To model count data, set glm()’s family argument to poisson, making it a Poisson regression. Here you’ll use data on the number of yearly visits to Snake River at Jackson Hole, Wyoming, snake_river_visits."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp-3",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp-3",
    "title": "12  How to Write a Function",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun a generalized linear regression by calling glm(). Model n_visits vs. gender, income, and travel on the snake_river_visits dataset, setting the family to poisson.\nDefine a function, run_poisson_regression(), to run a Poisson regression. This should take two arguments: data and formula, and call glm(), passing those arguments and setting family to poisson.\nRecreate the Poisson regression model from the first step, this time by calling your run_poisson_regression() function.\n\n\n\nE5.R\n\n# Run a generalized linear regression \nglm(\n  # Model no. of visits vs. gender, income, travel\n  n_visits ~ gender + income + travel, \n  # Use the snake_river_visits dataset\n  data = snake_river_visits, \n  # Make it a Poisson regression\n  family = poisson\n)\n\n# Write a function to run a Poisson regression\nrun_poisson_regression <- function(data, formula) {\n    glm(formula, data, family = poisson)\n}\n\n# From previous step\nrun_poisson_regression <- function(data, formula) {\n  glm(formula, data, family = poisson)\n}\n\n# Re-run the Poisson regression, using your function\nmodel <- snake_river_visits %>%\n  run_poisson_regression(n_visits ~ gender + income + travel)\n\n# Run this to see the predictions\nsnake_river_explanatory %>%\n  mutate(predicted_n_visits = predict(model, ., type = \"response\"))%>%\n  arrange(desc(predicted_n_visits))"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#numeric-defaults",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#numeric-defaults",
    "title": "12  How to Write a Function",
    "section": "12.7 Numeric defaults",
    "text": "12.7 Numeric defaults\ncut_by_quantile() converts a numeric vector into a categorical variable where quantiles define the cut points. This is a useful function, but at the moment you have to specify five arguments to make it work. This is too much thinking and typing.\nBy specifying default arguments, you can make it easier to use. Let’s start with n, which specifies how many categories to cut x into.\nA numeric vector of the number of visits to Snake River is provided as n_visits."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp-4",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp-4",
    "title": "12  How to Write a Function",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUpdate the definition of cut_by_quantile() so that the n argument defaults to 5. Remove the n argument from the call to cut_by_quantile().\n\n\nE6.R\n\n# Set the default for n to 5\ncut_by_quantile <- function(x, n=5, na.rm, labels, interval_type) {\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the n argument from the call\ncut_by_quantile(\n  n_visits, \n  na.rm = FALSE, \n  labels = c(\"very low\", \"low\", \"medium\", \"high\", \"very high\"),\n  interval_type = \"(lo, hi]\"\n)\nformals(cut_by_quantile)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#logical-defaults",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#logical-defaults",
    "title": "12  How to Write a Function",
    "section": "12.8 Logical defaults",
    "text": "12.8 Logical defaults\ncut_by_quantile() is now slightly easier to use, but you still always have to specify the na.rm argument. This removes missing values—it behaves the same as the na.rm argument to mean() or sd().\nWhere functions have an argument for removing missing values, the best practice is to not remove them by default (in case you hadn’t spotted that you had missing values). That means that the default for na.rm should be FALSE."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp-5",
    "href": "Introduction_to_Writing_Functions_in_R_C1.html#instructions-100-xp-5",
    "title": "12  How to Write a Function",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUpdate the definition of cut_by_quantile() so that the na.rm argument defaults to FALSE. Remove the na.rm argument from the call to cut_by_quantile().\n\n\nE7.R\n\n# Set the default for na.rm to FALSE\ncut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels, interval_type) {\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the na.rm argument from the call\ncut_by_quantile(\n  n_visits, \n  labels = c(\"very low\", \"low\", \"medium\", \"high\", \"very high\"),\n  interval_type = \"(lo, hi]\"\n)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#returning-early",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#returning-early",
    "title": "13  Return Values and Scope",
    "section": "13.1 Returning early",
    "text": "13.1 Returning early\nSometimes, you don’t need to run through the whole body of a function to get the answer. In that case you can return early from that function using return().\nTo check if x is divisible by n, you can use is_divisible_by(x, n) from assertive.\nAlternatively, use the modulo operator, %%. x %% n gives the remainder when dividing x by n, so x %% n == 0 determines whether x is divisible by n. Try 1:10 %% 3 == 0 in the console.\nTo solve this exercise, you need to know that a leap year is every 400th year (like the year 2000) or every 4th year that isn’t a century (like 1904 but not 1900 or 1905).\nassertive is loaded."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp",
    "title": "13  Return Values and Scope",
    "section": "13.2 Instructions 100 XP",
    "text": "13.2 Instructions 100 XP\nComplete the definition of is_leap_year(), checking for the cases of year being divisible by 400, then 100, then 4, returning early from the function in each case.\n\n\nE1.R\n\nis_leap_year <- function(year) {\n  # If year is div. by 400 return TRUE\n  if(is_divisible_by(year,400)) {\n    return(TRUE)\n  }\n  # If year is div. by 100 return FALSE\n  if(is_divisible_by(year,100)) {\n    return(FALSE)\n  }  \n  # If year is div. by 4 return TRUE\n  if(is_divisible_by(year,4)) {\n    return(TRUE)\n  }\n  \n  # Otherwise return FALSE\n  FALSE\n}"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#returning-invisibly",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#returning-invisibly",
    "title": "13  Return Values and Scope",
    "section": "13.3 Returning invisibly",
    "text": "13.3 Returning invisibly\nWhen the main purpose of a function is to generate output, like drawing a plot or printing something in the console, you may not want a return value to be rinted as well. In that case, the value should be invisibly returned.\nThe base R plot function returns NULL, since its main purpose is to draw a plot. This isn’t helpful if you want to use it in piped code: instead it should invisibly return the plot data to be piped on to the next step.\nRecall that plot() has a formula interface: instead of giving it vectors for x and y, you can specify a formula describing which columns of a data frame go on the x and y axes, and a data argument for the data frame. Note that just like lm(), the arguments are the wrong way round because the detail argument, formula, comes before the data argument.\n\nplot(y ~ x, data = data)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp-1",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp-1",
    "title": "13  Return Values and Scope",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the cars dataset and the formula interface to plot(), draw a scatter plot of dist versus speed.\nGive pipeable_plot() data and formula arguments (in that order) and make it draw the plot, then invisibly return data.\nDraw the scatter plot of dist vs. speed again by calling pipeable_plot()\n\n\n\nE2.R\n\n# Using cars, draw a scatter plot of dist vs. speed\nplt_dist_vs_speed <- plot(dist ~ speed, data = cars)\n\n# Oh no! The plot object is NULL\nplt_dist_vs_speed\n\n# Define a pipeable plot fn with data and formula args\npipeable_plot <- function(data, formula) {\n  # Call plot() with the formula interface\n  plot(formula, data)\n  # Invisibly return the input dataset\n  invisible(data)\n}\n\n# Draw the scatter plot of dist vs. speed again\nplt_dist_vs_speed <- cars %>% \n  pipeable_plot(dist ~ speed)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#returning-many-things",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#returning-many-things",
    "title": "13  Return Values and Scope",
    "section": "13.4 Returning many things",
    "text": "13.4 Returning many things\nFunctions can only return one value. If you want to return multiple things, then you can store them all in a list.\nIf users want to have the list items as separate variables, they can assign each list element to its own variable using zeallot’s multi-assignment operator, %<-%.\nglance(), tidy(), and augment() each take the model object as their only rgument.\nThe Poisson regression model of Snake River visits is available as model. broom and zeallot are loaded."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp-2",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp-2",
    "title": "13  Return Values and Scope",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExamine the structure of model.\nUse broom functions on model to create a list containing the model-, coefficient-, and observation-level parts of model.\nWrap the code into a function, groom_model(), that accepts model as its only argument.\nCall groom_model() on model, multi-assigning the result to three variables at once: mdl, cff, and obs.\n\n\n\nE3.R\n\n# Look at the structure of model (it's a mess!)\nstr(model)\n\n# Use broom tools to get a list of 3 data frames\nlist(\n  # Get model-level values\n  model = glance(model),\n  # Get coefficient-level values\n  coefficients = tidy(model),\n  # Get observation-level values\n  observations = augment(model)\n)\n\n# Wrap this code into a function, groom_model\ngroom_model <- function(model){\n  list(\n    model = glance(model),\n    coefficients = tidy(model),\n    observations = augment(model)\n  )\n}\n\ngroom_model(model)\n\n# From previous step\ngroom_model <- function(model) {\n  list(\n    model = glance(model),\n    coefficients = tidy(model),\n    observations = augment(model)\n  )\n}\n\n# Call groom_model on model, assigning to 3 variables\nc(mdl, cff, obs) %<-% groom_model(model)\n#c(var1, var2, var3) %<-% fn(args)\n\n# See these individual variables\nmdl; cff; obs"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#returning-metadata",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#returning-metadata",
    "title": "13  Return Values and Scope",
    "section": "13.5 Returning metadata",
    "text": "13.5 Returning metadata\nSometimes you want to return multiple things from a function, but you want the result to have a particular class (for example, a data frame or a numeric vector), so returning a list isn’t appropriate. This is common when you have a result plus metadata about the result. (Metadata is “data about the data”. For example, it could be the file a dataset was loaded from, or the username of the person who created the variable, or the number of iterations for an algorithm to converge.)\nIn that case, you can store the metadata in attributes. Recall the syntax for assigning attributes is as follows.\n\nattr(object, “attribute_name”) <- attribute_value"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp-3",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp-3",
    "title": "13  Return Values and Scope",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUpdate pipeable_plot() so the result has an attribute named “formula” with the value of formula.\nplt_dist_vs_speed, that you previously created, is shown. Examine its updated structure.\n\n\n\nE4.R\n\npipeable_plot <- function(data, formula) {\n  plot(formula, data)\n  # Add a \"formula\" attribute to data\n  attr(data, \"formula\") <- formula\n  invisible(data)\n}\n\n# From previous exercise\nplt_dist_vs_speed <- cars %>% \n  pipeable_plot(dist ~ speed)\n\n# Examine the structure of the result\nstr(plt_dist_vs_speed)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#creating-and-exploring-environments",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#creating-and-exploring-environments",
    "title": "13  Return Values and Scope",
    "section": "13.6 Creating and exploring environments",
    "text": "13.6 Creating and exploring environments\nEnvironments are used to store other variables. Mostly, you can think of them as lists, but there’s an important extra property that is relevant to writing functions. Every environment has a parent environment (except the empty environment, at the root of the environment tree). This determines which variables R know about at different places in your code.\nFacts about the Republic of South Africa are contained in capitals, national_parks, and population."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp-4",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp-4",
    "title": "13  Return Values and Scope",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate rsa_lst, a named list from capitals, national_parks, and population. Use those values as the names.\nList the structure of each element of rsa_lst using ls.str().\nConvert the list to an environment, rsa_env, using list2env().\nList the structure of each element of rsa_env\nFind the parent environment of rsa_env and print its name.\n\n\n\nE5.R\n\n# Add capitals, national_parks, & population to a named list\nrsa_lst <- list(\n  capitals = capitals,\n  national_parks = national_parks,\n  population = population\n)\n\n# List the structure of each element of rsa_lst\nls.str(rsa_lst)\n\n# From previous step\nrsa_lst <- list(\n  capitals = capitals,\n  national_parks = national_parks,\n  population = population\n)\n\n# Convert the list to an environment\nrsa_env <- list2env(rsa_lst)\n\n# List the structure of each variable\nls.str(rsa_env)\n\n# From previous steps\nrsa_lst <- list(\n  capitals = capitals,\n  national_parks = national_parks,\n  population = population\n)\nrsa_env <- list2env(rsa_lst)\n\n# Find the parent environment of rsa_env\nparent <- parent.env(rsa_env)\nenvironmentName(parent)\n\n# Print its name\nprint(environmentName)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#do-variables-exist",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#do-variables-exist",
    "title": "13  Return Values and Scope",
    "section": "13.7 Do variables exist?",
    "text": "13.7 Do variables exist?\nIf R cannot find a variable in the current environment, it will look in the parent environment, then the grandparent environment, and so on until it finds it.\nrsa_env has been modified so it includes capitals and national_parks, but not population."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp-5",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#instructions-100-xp-5",
    "title": "13  Return Values and Scope",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCheck if population exists in rsa_env, using default inheritance rules.\nCheck if population exists in rsa_env, ignoring inheritance.\n\n\n\nE6.R\n\n# Compare the contents of the global environment and rsa_env\nls.str(globalenv())\nls.str(rsa_env)\n\n# Does population exist in rsa_env?\nexists(\"population\", envir = rsa_env)\n\n# Does population exist in rsa_env, ignoring inheritance?\nexists(\"population\", envir = rsa_env,inherits = FALSE)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#variable-precedence-1",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#variable-precedence-1",
    "title": "13  Return Values and Scope",
    "section": "13.8 Variable precedence 1",
    "text": "13.8 Variable precedence 1\nConsider this code, run in a fresh R session.\n\nx_plus_y <- function(x) { y <- 3 x + y } y <- 7\n\nIf you now call x_plus_y(5), what is the result?"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#answer-the-question-50xp",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#answer-the-question-50xp",
    "title": "13  Return Values and Scope",
    "section": "13.9 Answer the question 50XP",
    "text": "13.9 Answer the question 50XP\nPossible Answers\n\nRespuesta\n\n\n\nAn error is thrown."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#variable-precedence-2",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#variable-precedence-2",
    "title": "13  Return Values and Scope",
    "section": "13.10 Variable precedence 2",
    "text": "13.10 Variable precedence 2\nConsider this (slightly different) code, run in a fresh R session.\nx_plus_y <- function(x) { x <- 6 y <- 3 x + y } y <- 7 If you now call x_plus_y(5), what is the result?"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C3.html#answer-the-question-50xp-1",
    "href": "Introduction_to_Writing_Functions_in_R_C3.html#answer-the-question-50xp-1",
    "title": "13  Return Values and Scope",
    "section": "13.11 Answer the question 50XP",
    "text": "13.11 Answer the question 50XP\nPossible Answers\n\n\nrespuesta\n\nAn error is thrown."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#converting-areas-to-metric-1",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#converting-areas-to-metric-1",
    "title": "15  Case Study on Grain Yields",
    "section": "15.1 Converting areas to metric 1",
    "text": "15.1 Converting areas to metric 1\nIn this chapter, you’ll be working with grain yield data from the United States Department of Agriculture, National Agricultural Statistics Service. Unfortunately, they report all areas in acres. So, the first thing you need to do is write some utility functions to convert areas in acres to areas in hectares.\nTo solve this exercise, you need to know the following:\nThere are 4840 square yards in an acre. There are 36 inches in a yard and one inch is 0.0254 meters. There are 10000 square meters in a hectare."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp",
    "title": "15  Case Study on Grain Yields",
    "section": "15.2 Instructions 100 XP",
    "text": "15.2 Instructions 100 XP\n\nWrite a function, acres_to_sq_yards(), to convert areas in acres to areas in square yards. This should take a single argument, acres.\nWrite a function, yards_to_meters(), to convert distances in yards to distances in meters. This should take a single argument, yards.\nWrite a function, sq_meters_to_hectares(), to convert areas in square meters to areas in hectares. This should take a single argument, sq_meters.\n\n\n\nE1.R\n\n# Write a function to convert acres to sq. yards\nacres_to_sq_yards <- function(x) {\n  x * 4840\n}\n\n\n# Write a function to convert yards to meters\nyards_to_meters <- function(x) {\n  x * 36*0.0254\n}\n\n# Write a function to convert sq. meters to hectares\nsq_meters_to_hectares <- function(sq_meters) {\n  (sq_meters)/10000\n}"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#converting-areas-to-metric-2",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#converting-areas-to-metric-2",
    "title": "15  Case Study on Grain Yields",
    "section": "15.3 Converting areas to metric 2",
    "text": "15.3 Converting areas to metric 2\nYou’re almost there with creating a function to convert acres to hectares. You need another utility function to deal with getting from square yards to square meters. Then, you can bring everything together to write the overall acres-to-hectares conversion function. Finally, in the next exercise you’ll be calculating area conversions in the denominator of a ratio, so you’ll need a harmonic acre-to-hectare conversion function.\nFree hints: magrittr’s raise_to_power() will be useful here. The last step is similar to Chapter 2’s Harmonic Mean.\nThe three utility functions from the last exercise (acres_to_sq_yards(), yards_to_meters(), and sq_meters_to_hectares()) are available, as is your get_reciprocal() from Chapter 2. magrittr is loaded."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-1",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-1",
    "title": "15  Case Study on Grain Yields",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nWrite a function to convert distance in square yards to square meters. It should take the square root of the input, then convert yards to meters, then square the result.\nWrite a function to convert areas in acres to hectares. The function should convert the input from acres to square yards, then to square meters, then to hectares.\nWrite a function to harmonically convert areas in acres to hectares. The function should get the reciprocal of the input, then convert from acres to hectares, then get the reciprocal again.\n\n\n\nE2.R\n\n# Write a function to convert sq. yards to sq. meters\nsq_yards_to_sq_meters <- function(sq_yards) {\n  sq_yards %>%\n    # Take the square root\n    sqrt() %>%\n    # Convert yards to meters\n    yards_to_meters() %>%\n    # Square it\n    raise_to_power(2)\n     }\n     \n# Load the function from the previous step\nload_step2()\n\n# Write a function to convert acres to hectares\nacres_to_hectares <- function(acres) {\n  acres %>%\n    # Convert acres to sq yards\n    acres_to_sq_yards() %>%\n    # Convert sq yards to sq meters\n    sq_yards_to_sq_meters() %>%\n    # Convert sq meters to hectares\n    sq_meters_to_hectares()\n}\n\n# Load the functions from the previous steps\nload_step3()\n\n# Define a harmonic acres to hectares function\nharmonic_acres_to_hectares <- function(acres) {\n  acres %>% \n    # Get the reciprocal\n    get_reciprocal() %>%\n    # Convert acres to hectares\n    acres_to_hectares() %>% \n    # Get the reciprocal again\n    get_reciprocal()\n}"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#converting-yields-to-metric",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#converting-yields-to-metric",
    "title": "15  Case Study on Grain Yields",
    "section": "15.4 Converting yields to metric",
    "text": "15.4 Converting yields to metric\nThe yields in the NASS corn data are also given in US units, namely bushels per acre. You’ll need to write some more utility functions to convert this unit to the metric unit of kg per hectare.\nBushels historically meant a volume of 8 gallons, but in the context of grain, they are now defined as masses. This mass differs for each grain! To solve this exercise, you need to know these facts.\nOne pound (lb) is 0.45359237 kilograms (kg). One bushel is 48 lbs of barley, 56 lbs of corn, or 60 lbs of wheat. magrittr is loaded."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-2",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-2",
    "title": "15  Case Study on Grain Yields",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nWrite a function to convert masses in lb to kg. This should take a single argument, lbs.\nWrite a function to convert masses in bushels to lbs. This should take two arguments, bushels and crop. It should define a lookup vector of scale factors for each crop (barley, corn, wheat), extract the scale factor for the crop, then multiply this by the number of bushels.\nWrite a function to convert masses in bushels to kgs. This should take two arguments, bushels and crop. It should convert the mass in bushels to lbs then to kgs.\nWrite a function to convert yields in bushels/acre to kg/ha. The arguments should be bushels_per_acre and crop. Three choices of crop should be allowed: “barley”, “corn”, and “wheat”. It should match the crop argument, then convert bushels to kgs, then convert harmonic acres to hectares.\n\n\n\nE3.R\n\n# Write a function to convert lb to kg\nlbs_to_kgs <- function(lbs) {lbs * 0.45359237}\n\n# Write a function to convert bushels to lbs\nbushels_to_lbs <- function(bushels, crop) {\n  # Define a lookup table of scale factors\n  c(barley = 48, corn = 56, wheat = 60) %>%\n    # Extract the value for the crop\n    extract(crop) %>%\n    # Multiply by the no. of bushels\n    multiply_by(bushels)\n}\n\n# Load fns defined in previous steps\nload_step3()\n\n# Write a function to convert bushels to kg\nbushels_to_kgs <- function(bushels, crop) {\n  bushels %>%\n    # Convert bushels to lbs for this crop\n    bushels_to_lbs(crop) %>%\n    # Convert lbs to kgs\n    lbs_to_kgs()\n}\n\n# Load fns defined in previous steps\nload_step4()\n\n# Write a function to convert bushels/acre to kg/ha\nbushels_per_acre_to_kgs_per_hectare <- function(bushels_per_acre, \ncrop = c(\"barley\", \"corn\", \"wheat\")) {\n  # Match the crop argument\n  crop <- match.arg(crop)\n  bushels_per_acre %>%\n    # Convert bushels to kgs for this crop\n    bushels_to_kgs(crop) %>%\n    # Convert harmonic acres to ha\n    harmonic_acres_to_hectares()\n}"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#applying-the-unit-conversion",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#applying-the-unit-conversion",
    "title": "15  Case Study on Grain Yields",
    "section": "15.5 Applying the unit conversion",
    "text": "15.5 Applying the unit conversion\nNow that you’ve written some functions, it’s time to apply them! The NASS corn dataset is available, and you can fortify it (jargon for “adding new columns”) with metrics areas and yields.\nThis fortification process can also be turned into a function, so you’ll define a function for this, and test it on the NASS wheat dataset."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-3",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-3",
    "title": "15  Case Study on Grain Yields",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nLook at the corn dataset. Add two columns: farmed_area_ha should be farmed_area_acres converted to hectares; yield_kg_per_ha should be yield_bushels_per_acre converted to kilograms per hectare.\nWrap the mutation code into a function called fortify_with_metric_units that takes two arguments, data and crop with no defaults. (In the function body, swap corn for the data argument and pass the function’s local crop variable to the crop argument.)\nUse fortify_with_metric_units() on the wheat dataset.\n\n\n\nE4.R\n\n# View the corn dataset\nglimpse(corn)\n\ncorn %>%\n  # Add some columns\n  mutate(\n    # Convert farmed area from acres to ha\n    farmed_area_ha = acres_to_hectares(farmed_area_acres),\n    # Convert yield from bushels/acre to kg/ha\n    yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(\n      yield_bushels_per_acre,\n      crop = \"corn\"\n    )\n  )\n  \n# Wrap this code into a function\nfortify_with_metric_units <- function(data, crop) {\n  data %>%\n    mutate(\n      farmed_area_ha = acres_to_hectares(farmed_area_acres),\n      yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(\n        yield_bushels_per_acre, \n        crop = crop\n      )\n    )\n}\n\n# Try it on the wheat dataset\nfortify_with_metric_units(wheat, crop = \"wheat\")"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#plotting-yields-over-time",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#plotting-yields-over-time",
    "title": "15  Case Study on Grain Yields",
    "section": "15.6 Plotting yields over time",
    "text": "15.6 Plotting yields over time\nNow that the units have been dealt with, it’s time to explore the datasets. An obvious question to ask about each crop is, “how do the yields change over time in each US state?” Let’s draw a line plot to find out!\nggplot2 is loaded, and corn and wheat datasets are available with metric units."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-4",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-4",
    "title": "15  Case Study on Grain Yields",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the corn dataset, plot yield_kg_per_ha versus year. Add a line layer grouped by state and a smooth trend layer.\nTurn the plotting code into a function, plot_yield_vs_year(). This should accept a single argument, data.\n\n\n\nE5.R\n\n\n# Using corn, plot yield (kg/ha) vs. year\nggplot(corn, aes(year, yield_kg_per_ha)) +\n  # Add a line layer, grouped by state\n  geom_line(aes(group = state)) +\n  # Add a smooth trend layer\n  geom_smooth()\n  \n# Wrap this plotting code into a function\nplot_yield_vs_year <- function(data) {\n  ggplot(data, aes(year, yield_kg_per_ha)) +\n    geom_line(aes(group = state)) +\n    geom_smooth()\n}\n\n# Test it on the wheat dataset\nplot_yield_vs_year(wheat)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#a-nation-divided",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#a-nation-divided",
    "title": "15  Case Study on Grain Yields",
    "section": "15.7 A nation divided",
    "text": "15.7 A nation divided\nThe USA has a varied climate, so we might expect yields to differ between states. Rather than trying to reason about 50 states separately, we can use the USA Census Regions to get 9 groups.\nThe “Corn Belt”, where most US corn is grown is in the “West North Central” and “East North Central” regions. The “Wheat Belt” is in the “West South Central” region.\ndplyr is loaded, the corn and wheat datasets are available, as is usa_census_regions."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-5",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-5",
    "title": "15  Case Study on Grain Yields",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nInner join corn to usa_census_regions by “state”.\nTurn the code into a function, fortify_with_census_region(). This should accept a single argument, data.\n\n\n\nE6.R\n\n# Inner join the corn dataset to usa_census_regions by state\ncorn %>%\n  inner_join(usa_census_regions, by = \"state\")\n  \n# Wrap this code into a function\nfortify_with_census_region <- function(data){\n  data %>%\n    inner_join(usa_census_regions, by = \"state\")\n}\n\n# Try it on the wheat dataset\nfortify_with_census_region(wheat)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#plotting-yields-over-time-by-region",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#plotting-yields-over-time-by-region",
    "title": "15  Case Study on Grain Yields",
    "section": "15.8 Plotting yields over time by region",
    "text": "15.8 Plotting yields over time by region\nSo far, you have used a function to plot yields over time for each crop, and you’ve added a census_region column to the crop datasets. Now you are ready to look at how the yields change over time in each region of the USA.\nggplot2 is loaded. corn and wheat have been fortified with census regions. plot_yield_vs_year() is available."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-6",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-6",
    "title": "15  Case Study on Grain Yields",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the function you wrote to plot yield versus year for the corn dataset, then facet the plot, wrapped by census_region.\nTurn the code into a function, plot_yield_vs_year_by_region(), that should take a single argument, data.\n\n\n\nE7.R\n\n# Plot yield vs. year for the corn dataset\nplot_yield_vs_year(corn) +\n  # Facet, wrapped by census region\n  facet_wrap(vars(census_region))\n  \n# Wrap this code into a function\nplot_yield_vs_year_by_region <- function(data) {\n\n  plot_yield_vs_year(data) +\n    facet_wrap(vars(census_region))\n}\n\n# Try it on the wheat dataset\nplot_yield_vs_year_by_region(wheat)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#running-a-model",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#running-a-model",
    "title": "15  Case Study on Grain Yields",
    "section": "15.9 Running a model",
    "text": "15.9 Running a model\nThe smooth trend line you saw in the plots of yield over time use a generalized additive model (GAM) to determine where the line should lie. This sort of model is ideal for fitting nonlinear curves. So we can make predictions about future yields, let’s explicitly run the model. The syntax for running this GAM takes the following form.\n\ngam(response ~ s(explanatory_var1) + explanatory_var2, data = dataset)\n\nHere, s() means “make the variable smooth”, where smooth very roughly means nonlinear.\nmgcv and dplyr are loaded; the corn and wheat datasets are available."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-7",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-7",
    "title": "15  Case Study on Grain Yields",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun a GAM of yield_kg_per_ha versus smoothed year and census region, using the corn dataset.\nWrap the modeling code into a function, run_gam_yield_vs_year_by_region. This should take a single argument, data, with no default.\n\n\n\nE8.R\n\n# Run a generalized additive model of yield vs. smoothed year and census region\n\ngam(yield_kg_per_ha ~ s(year) + census_region, data = corn)\n\n# Wrap the model code into a function\nrun_gam_yield_vs_year_by_region <- function(data){\n  gam(yield_kg_per_ha ~ s(year) + census_region, data = corn)\n}\n\n# Try it on the wheat dataset\nrun_gam_yield_vs_year_by_region(wheat)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#making-yield-predictions",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#making-yield-predictions",
    "title": "15  Case Study on Grain Yields",
    "section": "15.10 Making yield predictions",
    "text": "15.10 Making yield predictions\nThe fun part of modeling is using the models to make predictions. You can do this using a call to predict(), in the following form.\npredict(model, cases_to_predict, type = “response”) mgcv and dplyr are loaded; GAMs of the corn and wheat datasets are available as corn_model and wheat_model. A character vector of census regions is stored as census_regions."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-8",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-8",
    "title": "15  Case Study on Grain Yields",
    "section": "15.11 Instructions 100 XP",
    "text": "15.11 Instructions 100 XP\n\nIn predict_this, set the prediction year to 2050.\nPredict the yield using corn_model and the cases specified in predict_this.\nMutate predict_this to add the prediction vector as a new column named pred_yield_kg_per_ha.\nWrap the script into a function, predict_yields. It should take two arguments, model and year, with no defaults. Remember to update 2050 to the year argument. Try predict_yields() on wheat_model with year set to 2050.\n\n\n\nE9.R\n\n# Make predictions in 2050  \npredict_this <- data.frame(\n  year = 2050,\n  census_region = census_regions\n) \n\n# Predict the yield\npred_yield_kg_per_ha <- predict(corn_model, predict_this, type = \"response\")\n\npredict_this %>%\n  # Add the prediction as a column of predict_this \n    mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)\n\n# Wrap this prediction code into a function\npredict_yields <- function(model, year) {\n  predict_this <- data.frame(\n    year = 2050,\n    census_region = census_regions\n  ) \n  pred_yield_kg_per_ha <- predict(model, predict_this, type = \"response\")\n  predict_this %>%\n    mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)\n}\n\n# Try it on the wheat dataset\npredict_yields(wheat_model,2050)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#do-it-all-over-again",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#do-it-all-over-again",
    "title": "15  Case Study on Grain Yields",
    "section": "15.12 Do it all over again",
    "text": "15.12 Do it all over again\nHopefully, by now, you’ve realized that the real benefit to writing functions is that you can reuse your code easily. Now you are going to rerun the whole analysis from this chapter on a new crop, barley. Since all the infrastructure is in place, that’s less effort than it sounds!\nBarley prefers a cooler climate compared to corn and wheat and is commonly grown in the US mountain states of Idaho and Montana.\ndplyr and ggplot2, and mgcv are loaded; fortify_with_metric_units(), fortify_with_census_region(), plot_yield_vs_year_by_region(), run_gam_yield_vs_year_by_region(), and predict_yields() are available."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-9",
    "href": "Introduction_to_Writing_Functions_in_R_C4.html#instructions-100-xp-9",
    "title": "15  Case Study on Grain Yields",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFortify the barley data with metric units, then with census regions.\nUsing the fortified barley data, plot the yield versus year by census region.\nUsing the fortified barley data, run a GAM of yield versus year by census region, then predict the yields in 2050.\n\n\n\nE10.R\n\nfortified_barley <- barley %>% \n  # Fortify with metric units\n    fortify_with_metric_units() %>%\n  # Fortify with census regions\n  fortify_with_census_region()\n\n# See the result\nglimpse(fortified_barley)\n\n# From previous step\nfortified_barley <- barley %>% \n  fortify_with_metric_units() %>%\n  fortify_with_census_region()\n\n# Plot yield vs. year by region\nplot_yield_vs_year_by_region(fortified_barley)\n\n# From previous step\nfortified_barley <- barley %>% \n  fortify_with_metric_units() %>%\n  fortify_with_census_region()\n\nfortified_barley %>% \n  # Run a GAM of yield vs. year by region\n  run_gam_yield_vs_year_by_region()  %>% \n  # Make predictions of yields in 2050\n  predict_yields(2050)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#read.csv",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#read.csv",
    "title": "16  Importing data from flat files with utils",
    "section": "16.1 read.csv",
    "text": "16.1 read.csv\nThe utils package, which is automatically loaded in your R session on startup, can import CSV files with the read.csv() function.\nIn this exercise, you’ll be working with swimming_pools.csv (view); it contains data on swimming pools in Brisbane, Australia (Source: data.gov.au). The file contains the column names in the first row. It uses a comma to separate values within rows.\nType dir() in the console to list the files in your working directory. You’ll see that it contains swimming_pools.csv, so you can start straight away."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp",
    "title": "16  Importing data from flat files with utils",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse read.csv() to import “swimming_pools.csv” as a data frame with the name pools.\nPrint the structure of pools using str().\n\n\n\nE1.R\n\n# Import swimming_pools.csv: pools\npools <- read.csv(\"swimming_pools.csv\")\n\n# Print the structure of pools\nstr(pools)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#stringsasfactors",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#stringsasfactors",
    "title": "16  Importing data from flat files with utils",
    "section": "16.2 stringsAsFactors",
    "text": "16.2 stringsAsFactors\nWith stringsAsFactors, you can tell R whether it should convert strings in the flat file to factors.\nFor all importing functions in the utils package, this argument is TRUE, which means that you import strings as factors. This only makes sense if the strings you import represent categorical variables in R. If you set stringsAsFactors to FALSE, the data frame columns corresponding to strings in your text file will be character.\nYou’ll again be working with the swimming_pools.csv (view) file. It contains two columns (Name and Address), which shouldn’t be factors."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp-1",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp-1",
    "title": "16  Importing data from flat files with utils",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse read.csv() to import the data in “swimming_pools.csv” as a data frame called pools; make sure that strings are imported as characters, not as factors.\nUsing str(), display the structure of the dataset and check that you indeed get character vectors instead of factors.\n\n\n\nE2.R\n\n# Import swimming_pools.csv correctly: pools\npools <- read.csv(\"swimming_pools.csv\")\n\n# Check the structure of pools\nstr(pools)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#any-changes",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#any-changes",
    "title": "16  Importing data from flat files with utils",
    "section": "16.3 Any changes?",
    "text": "16.3 Any changes?\nConsider the code below that loads data from swimming_pools.csv in two distinct ways:\n\nOption A\npools <- read.csv(“swimming_pools.csv”, stringsAsFactors = TRUE)\n\n\nOption B\npools <- read.csv(“swimming_pools.csv”, stringsAsFactors = FALSE)\n\nHow many variables in the resulting pools data frame have different types if you specify the stringsAsFactors argument differently?\nThe swimming_pools.csv (view) file is available in your current working directory so you can experiment in the console."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#instructions-50-xp",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#instructions-50-xp",
    "title": "16  Importing data from flat files with utils",
    "section": "Instructions 50 XP",
    "text": "Instructions 50 XP\nPossible Answers\nJust one: Name.\nTwo variables: Name and Address. Respuesta\nThree columns: all but Longitude.\nAll four of them!"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#read.delim",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#read.delim",
    "title": "16  Importing data from flat files with utils",
    "section": "16.4 read.delim",
    "text": "16.4 read.delim\nAside from .csv files, there are also the .txt files which are basically text files. You can import these functions with read.delim(). By default, it sets the sep argument to “ (fields in a record are delimited by tabs) and the header argument to TRUE (the first row contains the field names).\nIn this exercise, you will import hotdogs.txt (view), containing information on sodium and calorie levels in different hotdogs (Source: UCLA). The dataset has 3 variables, but the variable names are not available in the first line of the file. The file uses tabs as field separators."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp-2",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp-2",
    "title": "16  Importing data from flat files with utils",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nImport the data in “hotdogs.txt” with read.delim(). Call the resulting data frame hotdogs. The variable names are not on the first line, so make sure to set the header argument appropriately. Call summary() on hotdogs. This will print out some summary statistics about all variables in the data frame.\n\n\nE3.R\n\n# Import hotdogs.txt: hotdogs\nhotdogs <- read.delim(\"hotdogs.txt\", header = FALSE)\n\n# Summarize hotdogs\nsummary(hotdogs)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#read.table",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#read.table",
    "title": "16  Importing data from flat files with utils",
    "section": "16.5 read.table",
    "text": "16.5 read.table\nIf you’re dealing with more exotic flat file formats, you’ll want to use read.table(). It’s the most basic importing function; you can specify tons of different arguments in this function. Unlike read.csv() and read.delim(), the header argument defaults to FALSE and the sep argument is “” by default.\nUp to you again! The data is still hotdogs.txt (view). It has no column names in the first row, and the field separators are tabs. This time, though, the file is in the data folder inside your current working directory. A variable path with the location of this file is already coded for you."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp-3",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp-3",
    "title": "16  Importing data from flat files with utils",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFinish the read.table() call that’s been prepared for you. Use the path variable, and make sure to set sep correctly. Call head() on hotdogs; this will print the first 6 observations in the data frame.\n\n\nE4.R\n\n# Path to the hotdogs.txt file: path\npath <- file.path(\"data\", \"hotdogs.txt\")\n\n# Import the hotdogs.txt file: hotdogs\nhotdogs <- read.table(path, \n                      sep = \"\\t\", \n                      col.names = c(\"type\", \"calories\", \"sodium\"))\n\n# Call head() on hotdogs\nhead(hotdogs)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#arguments",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#arguments",
    "title": "16  Importing data from flat files with utils",
    "section": "16.6 Arguments",
    "text": "16.6 Arguments\nLily and Tom are having an argument because they want to share a hot dog but they can’t seem to agree on which one to choose. After some time, they simply decide that they will have one each. Lily wants to have the one with the fewest calories while Tom wants to have the one with the most sodium.\nNext to calories and sodium, the hotdogs have one more variable: type. This can be one of three things: Beef, Meat, or Poultry, so a categorical variable: a factor is fine."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp-4",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp-4",
    "title": "16  Importing data from flat files with utils",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFinish the read.delim() call to import the data in “hotdogs.txt”. It’s a tab-delimited file without names in the first row.\nThe code that selects the observation with the lowest calorie count and stores it in the variable lily is already available. It uses the function which.min(), that returns the index the smallest value in a vector.\nDo a similar thing for Tom: select the observation with the most sodium and store it in tom. Use which.max() this time.\nFinally, print both the observations lily and tom.\n\n\n\nE5.R\n\n# Finish the read.delim() call\nhotdogs <- read.delim(\"hotdogs.txt\", header = FALSE, col.names = c(\"type\", \n\"calories\", \"sodium\"))\n\n# Select the hot dog with the least calories: lily\nlily <- hotdogs[which.min(hotdogs$calories), ]\n\n# Select the observation with the most sodium: tom\ntom <- hotdogs[which.max(hotdogs$sodium), ]\n\n# Print lily and tom\nprint(lily)\n\nprint(tom)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#column-classes",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#column-classes",
    "title": "16  Importing data from flat files with utils",
    "section": "16.7 Column classes",
    "text": "16.7 Column classes\nNext to column names, you can also specify the column types or column classes of the resulting data frame. You can do this by setting the colClasses argument to a vector of strings representing classes:\n\nread.delim(“my_file.txt”, colClasses = c(“character”, “numeric”, “logical”))\n\nThis approach can be useful if you have some columns that should be factors and others that should be characters. You don’t have to bother with stringsAsFactors anymore; just state for each column what the class should be.\nIf a column is set to “NULL” in the colClasses vector, this column will be skipped and will not be loaded into the data frame."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp-5",
    "href": "Introduction_to_Importing_Data_in_R_C1.html#instructions-100-xp-5",
    "title": "16  Importing data from flat files with utils",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nThe read.delim() call from before is already included and creates the hotdogs data frame. Go ahead and display the structure of hotdogs.\nEdit the second read.delim() call. Assign the correct vector to the colClasses argument. NA should be replaced with a character vector: c(“factor”, “NULL”, “numeric”).\nDisplay the structure of hotdogs2 and look for the difference.\n\n\n\nE6.R\n\n# Previous call to import hotdogs.txt\nhotdogs <- read.delim(\"hotdogs.txt\", header = FALSE, col.names = c(\"type\",\n\"calories\", \"sodium\"))\n\n# Display structure of hotdogs\nstr(hotdogs)\n\n# Edit the colClasses argument to import the data correctly: hotdogs2\nhotdogs2 <- read.delim(\"hotdogs.txt\", header = FALSE, \n                       col.names = c(\"type\", \"calories\", \"sodium\"),\n                       colClasses = c(\"factor\", \"NULL\", \"numeric\"))\n\n\n# Display structure of hotdogs2\nstr(hotdogs2)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#read_csv",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#read_csv",
    "title": "17  readr & data.table",
    "section": "17.1 read_csv",
    "text": "17.1 read_csv\nCSV files can be imported with read_csv(). It’s a wrapper function around read_delim() that handles all the details for you. For example, it will assume that the first row contains the column names.\nThe dataset you’ll be working with here is potatoes.csv (view). It gives information on the impact of storage period and cooking on potatoes’ flavor. It uses commas to delimit fields in a record, and contains column names in the first row. The file is available in your workspace. Remember that you can inspect your workspace with dir()."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp",
    "title": "17  readr & data.table",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nLoad the readr package with library(). You do not need to install the package, it is already installed on DataCamp’s servers. Import “potatoes.csv” using read_csv(). Assign the resulting data frame to the variable potatoes.\n\n\nE1.R\n\n# Load the readr package\nlibrary(readr)\n\n# Import potatoes.csv with read_csv(): potatoes\npotatoes <- read_csv(\"potatoes.csv\")"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#read_tsv",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#read_tsv",
    "title": "17  readr & data.table",
    "section": "17.2 read_tsv",
    "text": "17.2 read_tsv\nWhere you use read_csv() to easily read in CSV files, you use read_tsv() to easily read in TSV files. TSV is short for tab-separated values.\nThis time, the potatoes data comes in the form of a tab-separated values file; potatoes.txt (view) is available in your workspace. In contrast to potatoes.csv, this file does not contain columns names in the first row, though.\nThere’s a vector properties that you can use to specify these column names manually."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-1",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-1",
    "title": "17  readr & data.table",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUse read_tsv() to import the potatoes data from potatoes.txt and store it in the data frame potatoes. In addition to the path to the file, you’ll also have to specify the col_names argument; you can use the properties vector for this. Call head() on potatoes to show the first observations of your dataset.\n\n\nE2.R\n\n# readr is already loaded\n\n# Column names\nproperties <- c(\"area\", \"temp\", \"size\", \"storage\", \"method\",\n                \"texture\", \"flavor\", \"moistness\")\n\n# Import potatoes.txt: potatoes\npotatoes <- read_tsv(\"potatoes.txt\", col_names = properties)\n\n# Call head() on potatoes\nhead(potatoes)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#read_delim",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#read_delim",
    "title": "17  readr & data.table",
    "section": "17.3 read_delim",
    "text": "17.3 read_delim\nJust as read.table() was the main utils function, read_delim() is the main readr function.\nread_delim() takes two mandatory arguments:\nfile: the file that contains the data delim: the character that separates the values in the data file You’ll again be working with potatoes.txt (view); the file uses tabs (“) to delimit values and does not contain column names in its first line. It’s available in your working directory so you can start right away. As before, the vector properties is available to set the col_names."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-2",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-2",
    "title": "17  readr & data.table",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nImport all the data in “potatoes.txt” using read_delim(); store the resulting data frame in potatoes. Print out potatoes.\n\n\nE3.R\n\n# readr is already loaded\n\n# Column names\nproperties <- c(\"area\", \"temp\", \"size\", \"storage\", \"method\",\n                \"texture\", \"flavor\", \"moistness\")\n\n# Import potatoes.txt using read_delim(): potatoes\npotatoes <- read_delim(\"potatoes.txt\", delim = \"\\t\", col_names = properties)\n\n# Print out potatoes\nprint(potatoes)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#skip-and-n_max",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#skip-and-n_max",
    "title": "17  readr & data.table",
    "section": "17.4 skip and n_max",
    "text": "17.4 skip and n_max\nThrough skip and n_max you can control which part of your flat file you’re actually importing into R.\n\nskip specifies the number of lines you’re ignoring in the flat file before actually starting to import data.\nn_max specifies the number of lines you’re actually importing.\n\nSay for example you have a CSV file with 20 lines, and set skip = 2 and n_max = 3, you’re only reading in lines 3, 4 and 5 of the file.\nWatch out: Once you skip some lines, you also skip the first line that can contain column names!\npotatoes.txt (view), a flat file with tab-delimited records and without column names, is available in your workspace."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-3",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-3",
    "title": "17  readr & data.table",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFinish the first read_tsv() call to import observations 7, 8, 9, 10 and 11 from potatoes.txt.\n\n\nE4.R\n\n# readr is already loaded\n\n# Column names\nproperties <- c(\"area\", \"temp\", \"size\", \"storage\", \"method\",\n                \"texture\", \"flavor\", \"moistness\")\n\n# Import 5 observations from potatoes.txt: potatoes_fragment\npotatoes_fragment <- read_tsv(\"potatoes.txt\", skip = 6, n_max = 5,\ncol_names = properties)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#col_types",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#col_types",
    "title": "17  readr & data.table",
    "section": "17.5 col_types",
    "text": "17.5 col_types\nYou can also specify which types the columns in your imported data frame should have. You can do this with col_types. If set to NULL, the default, functions from the readr package will try to find the correct types themselves. You can manually set the types with a string, where each character denotes the class of the column: character, double, integer and logical. _ skips the column as a whole.\npotatoes.txt (view), a flat file with tab-delimited records and without column names, is again available in your workspace."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-4",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-4",
    "title": "17  readr & data.table",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nIn the second read_tsv() call, edit the col_types argument to import all columns as characters (c). Store the resulting data frame in potatoes_char. Print out the structure of potatoes_char and verify whether all column types are chr, short for character.\n\n\nE5.R\n\n# readr is already loaded\n\n# Column names\nproperties <- c(\"area\", \"temp\", \"size\", \"storage\", \"method\",\n                \"texture\", \"flavor\", \"moistness\")\n\n# Import all data, but force all columns to be character: potatoes_char\npotatoes_char <- read_tsv(\"potatoes.txt\", col_types = \"cccccccc\",\ncol_names = properties)\n\n# Print out structure of potatoes_char\nstr(potatoes_char)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#col_types-with-collectors",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#col_types-with-collectors",
    "title": "17  readr & data.table",
    "section": "17.6 col_types with collectors",
    "text": "17.6 col_types with collectors\nAnother way of setting the types of the imported columns is using collectors. Collector functions can be passed in a list() to the col_types argument of read_ functions to tell them how to interpret values in a column.\nFor a complete list of collector functions, you can take a look at the collector documentation. For this exercise you will need two collector functions:\n\ncol_integer(): the column should be interpreted as an integer.\ncol_factor(levels, ordered = FALSE): the column should be interpreted as a factor with levels.\n\nIn this exercise, you will work with hotdogs.txt (view), which is a tab-delimited file without column names in the first row."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-5",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-5",
    "title": "17  readr & data.table",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nhotdogs is created for you without setting the column types. Inspect its summary using the summary() function.\nTwo collector functions are defined for you: fac and int. Have a look at them, do you understand what they’re collecting?\nIn the second read_tsv() call, edit the col_types argument: Pass a list() with the elements fac, int and int, so the first column is imported as a factor, and the second and third column as integers.\nCreate a summary() of hotdogs_factor. Compare this to the summary of hotdogs.\n\n\n\nE6.R\n\n# readr is already loaded\n\n# Import without col_types\nhotdogs <- read_tsv(\"hotdogs.txt\", col_names = c(\"type\", \"calories\", \"sodium\"))\n\n# Display the summary of hotdogs\nsummary(hotdogs)\n\n# The collectors you will need to import the data\nfac <- col_factor(levels = c(\"Beef\", \"Meat\", \"Poultry\"))\nint <- col_integer()\n\n# Edit the col_types argument to import the data correctly: hotdogs_factor\nhotdogs_factor <- read_tsv(\"hotdogs.txt\",\n                           col_names = c(\"type\", \"calories\", \"sodium\"),\n                           col_types = list(fac, int, int))\n\n# Display the summary of hotdogs_factor\nsummary(hotdogs_factor)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#fread",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#fread",
    "title": "17  readr & data.table",
    "section": "17.7 fread",
    "text": "17.7 fread\nYou still remember how to use read.table(), right? Well, fread() is a function that does the same job with very similar arguments. It is extremely easy to use and blazingly fast! Often, simply specifying the path to the file is enough to successfully import your data.\nDon’t take our word for it, try it yourself! You’ll be working with the potatoes.csv (view) file, that’s available in your workspace. Fields are delimited by commas, and the first line contains the column names."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-6",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-6",
    "title": "17  readr & data.table",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse library() to load (NOT install) the data.table package. You do not need to install the package, it is already installed on DataCamp’s servers.\nImport “potatoes.csv” with fread(). Simply pass it the file path and see if it worked. Store the result in a variable potatoes.\nPrint out potatoes.\n\n\n\nE7.R\n\n# load the data.table package using library()\nlibrary(data.table)\n\n# Import potatoes.csv with fread(): potatoes\npotatoes <- fread(\"potatoes.csv\")\n\n# Print out potatoes\nprint(potatoes)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#fread-more-advanced-use",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#fread-more-advanced-use",
    "title": "17  readr & data.table",
    "section": "17.8 fread: more advanced use",
    "text": "17.8 fread: more advanced use\nNow that you know the basics about fread(), you should know about two arguments of the function: drop and select, to drop or select variables of interest.\nSuppose you have a dataset that contains 5 variables and you want to keep the first and fifth variable, named “a” and “e”. The following options will all do the trick:\n\nfread(“path/to/file.txt”, drop = 2:4) fread(“path/to/file.txt”, select = c(1, 5)) fread(“path/to/file.txt”, drop = c(“b”, “c”, “d”)) fread(“path/to/file.txt”, select = c(“a”, “e”))\n\nLet’s stick with potatoes since we’re particularly fond of them here at DataCamp. The data is again available in the file potatoes.csv (view), containing comma-separated records."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-7",
    "href": "Introduction_to_Importing_Data_in_R_C2.html#instructions-100-xp-7",
    "title": "17  readr & data.table",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing fread() and select or drop as arguments, only import the texture and moistness columns of the flat file. They correspond to the columns 6 and 8 in “potatoes.csv”. Store the result in a variable potatoes. plot() 2 columns of the potatoes data frame: texture on the x-axis, moistness on the y-axis. Use the dollar sign notation twice. Feel free to name your axes and plot.\n\n\nE8.R\n\n# fread is already loaded\n\n# Import columns 6 and 8 of potatoes.csv: potatoes\npotatoes<- fread(\"potatoes.csv\", select = c(6, 8))\n\n# Plot texture (x) and moistness (y) of potatoes\nplot(potatoes$texture, potatoes$moistness)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#selecting-columns",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#selecting-columns",
    "title": "18  Importing Excel data",
    "section": "18.1 Selecting columns",
    "text": "18.1 Selecting columns\nUsing the select() verb, we can answer interesting questions about our dataset by focusing in on related groups of verbs. The colon (:) is useful for getting many columns at a time."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp",
    "title": "18  Importing Excel data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse glimpse() to examine all the variables in the counties table.\nSelect the columns for state, county, population, and (using a colon) all five of those industry-related variables; there are five consecutive variables in the table related to the industry of people’s work: professional, service, office, construction, and production.\nArrange the table in descending order of service to find which counties have the highest rates of working in the service industry.\n\n\n\nE1.R\n\n# Glimpse the counties table\nglimpse(counties)\n\ncounties %>%\n  # Select state, county, population, and industry-related columns\n  select(state, county, population,professional:production) %>%\n  # Arrange service in descending order \n   arrange(desc(service))"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#select-helpers",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#select-helpers",
    "title": "18  Importing Excel data",
    "section": "18.2 Select helpers",
    "text": "18.2 Select helpers\nIn the video you learned about the select helper starts_with(). Another select helper is ends_with(), which finds the columns that end with a particular string."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp-1",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp-1",
    "title": "18  Importing Excel data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSelect the columns state, county, population, and all those that end with work.\nFilter just for the counties where at least 50% of the population is engaged in public work.\n\n\n\nE2.R\n\ncounties %>%\n  # Select the state, county, population, and those ending with \"work\"\n  select(state, county, population,ends_with(\"work\")) %>%\n  # Filter for counties that have at least 50% of people engaged in public work\n  filter(public_work  >= 50)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#renaming-a-column-after-count",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#renaming-a-column-after-count",
    "title": "18  Importing Excel data",
    "section": "18.3 Renaming a column after count",
    "text": "18.3 Renaming a column after count\nThe rename() verb is often useful for changing the name of a column that comes out of another verb, such as count(). In this exercise, you’ll rename the default n column generated from count() to something more descriptive."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp-2",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp-2",
    "title": "18  Importing Excel data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse count() to determine how many counties are in each state.\nNotice the n column in the output; use rename() to rename that to num_counties.\n\n\n\nE3.R\n\ncounties %>%\n  # Count the number of counties in each state\n  count(state)\n  \ncounties %>%\n  # Count the number of counties in each state\n  count(state) %>%\n  # Rename the n column to num_counties\n  rename(num_counties = n)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#renaming-a-column-as-part-of-a-select",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#renaming-a-column-as-part-of-a-select",
    "title": "18  Importing Excel data",
    "section": "18.4 Renaming a column as part of a select",
    "text": "18.4 Renaming a column as part of a select\nrename() isn’t the only way you can choose a new name for a column; you can also choose a name as part of a select()."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp-3",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp-3",
    "title": "18  Importing Excel data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSelect the columns state, county, and poverty from the counties dataset; in the same step, rename the poverty column to poverty_rate.\n\n\n\nE4.R\n\ncounties %>%\n  # Select state, county, and poverty as poverty_rate\n   select(state, county, poverty_rate = poverty)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#using-transmute",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#using-transmute",
    "title": "18  Importing Excel data",
    "section": "18.5 Using transmute",
    "text": "18.5 Using transmute\nAs you learned in the video, the transmute verb allows you to control which variables you keep, which variables you calculate, and which variables you drop."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp-4",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp-4",
    "title": "18  Importing Excel data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nKeep only the state, county, and population columns, and add a new column, density, that contains the population per land_area.\nFilter for only counties with a population greater than one million.\nSort the table in ascending order of density.\n\n\n\nE5.R\n\ncounties %>%\n  # Keep the state, county, and populations columns, and add a density column\n transmute(state, county, population, density = population / land_area) %>%\n  # Filter for counties with a population greater than one million \n  filter(population>1000000)%>%\n  # Sort density in ascending order \n  arrange(density)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#choosing-among-the-four-verbs",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#choosing-among-the-four-verbs",
    "title": "18  Importing Excel data",
    "section": "18.6 Choosing among the four verbs",
    "text": "18.6 Choosing among the four verbs\nIn this chapter you’ve learned about the four verbs: select, mutate, transmute, and rename. Here, you’ll choose the appropriate verb for each situation. You won’t need to change anything inside the parentheses."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp-5",
    "href": "Introduction_to_Importing_Data_in_R_C3.html#instructions-100-xp-5",
    "title": "18  Importing Excel data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nChoose the right verb for changing the name of the unemployment column to unemployment_rate\nChoose the right verb for keeping only the columns state, county, and the ones containing poverty.\nCalculate a new column called fraction_women with the fraction of the population made up of women, without dropping any columns.\nKeep only three columns: the state, county, and employed / population, which you’ll call employment_rate.\n\n\n\nE6.R\n\n# Change the name of the unemployment column\ncounties %>%\n  rename(unemployment_rate = unemployment)\n\n# Keep the state and county columns, and the columns containing poverty\ncounties %>%\n  select(state, county, contains(\"poverty\"))\n\n# Calculate the fraction_women column without dropping the other columns\ncounties %>%\n  mutate(fraction_women = women / population)\n\n# Keep only the state, county, and employment_rate columns\ncounties %>%\n  transmute(state, county, employment_rate = employed / population)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#filtering-and-arranging-for-one-year",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#filtering-and-arranging-for-one-year",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "19.1 Filtering and arranging for one year",
    "text": "19.1 Filtering and arranging for one year\nThe dplyr verbs you’ve learned are useful for exploring data. For instance, you could find out the most common names in a particular year."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFilter for only the year 1990.\nSort the table in descending order of the number of babies born.\n\n\n\nE1.R\n\nbabynames %>%\n  # Filter for the year 1990\n  filter(year == 1990) %>%\n  # Sort the number column in descending order \n  arrange(desc(number))"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#finding-the-most-popular-names-each-year",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#finding-the-most-popular-names-each-year",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "19.2 Finding the most popular names each year",
    "text": "19.2 Finding the most popular names each year\nYou saw that you could use filter() and arrange() to find the most common names in one year. However, you could also use group_by() and slice_max() to find the most common name in every year."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-1",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-1",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse group_by() and slice_max() to find the most common name for US babies in each year.\n\n\n\nE2.R\n\nbabynames %>%\n  # Find the most common name in each year\n  group_by(year) %>%\n  slice_max(number, n = 1)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#visualizing-names-with-ggplot2",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#visualizing-names-with-ggplot2",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "19.3 Visualizing names with ggplot2",
    "text": "19.3 Visualizing names with ggplot2\nThe dplyr package is very useful for exploring data, but it’s especially useful when combined with other tidyverse packages like ggplot2."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-2",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-2",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFilter for only the names Steven, Thomas, and Matthew, and assign it to an object called selected_names.\nVisualize those three names as a line plot over time, with each name represented by a different color.\n\n\n\nE3.R\n\nselected_names <- babynames %>%\n  # Filter for the names Steven, Thomas, and Matthew \n  filter(name %in% c(\"Steven\", \"Thomas\", \"Matthew\"))\n  \n\nselected_names <- babynames %>%\n  # Filter for the names Steven, Thomas, and Matthew \n  filter(name %in% c(\"Steven\", \"Thomas\", \"Matthew\"))\n\n# Plot the names using a different color for each name\nggplot(selected_names, aes(x = year, y = number, color = name)) +\n  geom_line()"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#finding-the-year-each-name-is-most-common",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#finding-the-year-each-name-is-most-common",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "19.4 Finding the year each name is most common",
    "text": "19.4 Finding the year each name is most common\nIn an earlier video, you learned how to filter for a particular name to determine the frequency of that name over time. Now, you’re going to explore which year each name was the most common.\nTo do this, you’ll be combining the grouped mutate approach with a slice_max()."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-3",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-3",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFirst, calculate the total number of people born in that year in this dataset as year_total.\nNext, use year_total to calculate the fraction of people born in each year that have each name.\nNow use your newly calculated fraction column, in combination with slice_max(), to identify the year each name was most common.\n\n\n\nE4.R\n\n# Calculate the fraction of people born each year with the same name\nbabynames %>%\n  group_by(year) %>%\n  mutate(year_total = sum(number)) %>%\n  ungroup() %>%\n  mutate(fraction = number / year_total)\n  \n\n# Calculate the fraction of people born each year with the same name\nbabynames %>%\n  group_by(year) %>%\n  mutate(year_total = sum(number)) %>%\n  ungroup() %>%\n  mutate(fraction = number / year_total) %>%\n  # Find the year each name is most common\n  group_by(name) %>%\n  slice_max(fraction, n = 1)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#adding-the-total-and-maximum-for-each-name",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#adding-the-total-and-maximum-for-each-name",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "19.5 Adding the total and maximum for each name",
    "text": "19.5 Adding the total and maximum for each name\nIn the video, you learned how you could group by the year and use mutate() to add a total for that year.\nIn these exercises, you’ll learn to normalize by a different, but also interesting metric: you’ll divide each name by the maximum for that name. This means that every name will peak at 1.\nOnce you add new columns, the result will still be grouped by name. This splits it into 48,000 groups, which actually makes later steps like mutates slower."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-4",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-4",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse a grouped mutate to add two columns:\nname_total: the sum of the number of babies born with that name in the entire dataset.\nname_max: the maximum number of babies born with that name in any year.\nAdd another step to ungroup the table.\nAdd a column called fraction_max containing the number in the year divided by name_max.\n\n\n\nE5.R\n\nbabynames %>%\n  # Add columns name_total and name_max for each name\n  group_by(name) %>%\n  mutate(name_total = sum(number),\n         name_max = max(number))\n         \n\nbabynames %>%\n  # Add columns name_total and name_max for each name\n  group_by(name) %>%\n  mutate(name_total = sum(number),\n         name_max = max(number)) %>%\n  # Ungroup the table \n  ungroup() %>%\n  # Add the fraction_max column containing the number by the name maximum \n  mutate(fraction_max = number / name_max)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#visualizing-the-normalized-change-in-popularity",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#visualizing-the-normalized-change-in-popularity",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "19.6 Visualizing the normalized change in popularity",
    "text": "19.6 Visualizing the normalized change in popularity\nYou picked a few names and calculated each of them as a fraction of their peak. This is a type of “normalizing” a name, where you’re focused on the relative change within each name rather than the overall popularity of the name.\nIn this exercise, you’ll visualize the normalized popularity of each name. Your work from the previous exercise, names_normalized, has been provided for you.\n\nnames_normalized <- babynames %>% group_by(name) %>% mutate(name_total = sum(number), name_max = max(number)) %>% ungroup() %>% mutate(fraction_max = number / name_max)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-5",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-5",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFilter the names_normalized table to limit it to the three names Steven, Thomas, and Matthew.\nCreate a line plot to visualize fraction_max over time, colored by name.\n\n\n\nE6.R\n\nnames_filtered <- names_normalized %>%\n  # Filter for the names Steven, Thomas, and Matthew\n  filter(name %in% c(\"Steven\", \"Thomas\", \"Matthew\"))\n\n# Visualize these names over time\nggplot(names_filtered, aes(x = year, y = fraction_max, color = name)) +\n  geom_line()"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#using-ratios-to-describe-the-frequency-of-a-name",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#using-ratios-to-describe-the-frequency-of-a-name",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "19.7 Using ratios to describe the frequency of a name",
    "text": "19.7 Using ratios to describe the frequency of a name\nIn the video, you learned how to find the difference in the frequency of a baby name between consecutive years. What if instead of finding the difference, you wanted to find the ratio?\nYou’ll start with the babynames_fraction data already, so that you can consider the popularity of each name within each year."
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-6",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-6",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nArrange the data in ascending order of name and then year.\nGroup by name so that your mutate works within each name.\nAdd a column ratio containing the ratio (not difference) of fraction between each year.\n\n\n\nE7.R\n\nbabynames_fraction %>%\n  # Arrange the data in order of name, then year \n  arrange(name, year) %>%\n  # Group the data by name\n  group_by(name) %>%\n  # Add a ratio column that contains the ratio of fraction between each year \n  mutate(ratio = fraction / lag(fraction))"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#biggest-jumps-in-a-name",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#biggest-jumps-in-a-name",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "19.8 Biggest jumps in a name",
    "text": "19.8 Biggest jumps in a name\nPreviously, you added a ratio column to describe the ratio of the frequency of a baby name between consecutive years to describe the changes in the popularity of a name. Now, you’ll look at a subset of that data, called babynames_ratios_filtered, to look further into the names that experienced the biggest jumps in popularity in consecutive years.\n\nbabynames_ratios_filtered <- babynames_fraction %>% arrange(name, year) %>% group_by(name) %>% mutate(ratio = fraction / lag(fraction)) %>% filter(fraction >= 0.00001)"
  },
  {
    "objectID": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-7",
    "href": "Introduction_to_Importing_Data_in_R_C4.html#instructions-100-xp-7",
    "title": "19  Reproducible Excel work with XLConnect",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFrom each name in the data, keep the observation (the year) with the largest ratio; note the data is already grouped by name.\nSort the ratio column in descending order.\nFilter the babynames_ratios_filtered data further by filtering the fraction column to only display results greater than or equal to 0.001.\n\n\n\nE8.R\n\nbabynames_ratios_filtered %>%\n  # Extract the largest ratio from each name \n  slice_max(ratio, n=1) %>%\n  # Sort the ratio column in descending order \n  arrange(desc(ratio)) %>%\n  # Filter for fractions greater than or equal to 0.001\n  filter(fraction >= 0.001)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#selecting-columns",
    "href": "Data_Manipulation_with_dplyr_C1.html#selecting-columns",
    "title": "20  Transforming Data with dplyr",
    "section": "20.1 Selecting columns",
    "text": "20.1 Selecting columns\nSelect the following four columns from the counties variable:\n\nstate county population poverty\n\nYou don’t need to save the result to a variable."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp",
    "href": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp",
    "title": "20  Transforming Data with dplyr",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nSelect the columns listed from the counties variable.\n\n\nE1.R\n\ncounties %>%\n  # Select the columns\n select(state, county, population, poverty)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#arranging-observations",
    "href": "Data_Manipulation_with_dplyr_C1.html#arranging-observations",
    "title": "20  Transforming Data with dplyr",
    "section": "20.2 Arranging observations",
    "text": "20.2 Arranging observations\nHere you see the counties_selected dataset with a few interesting variables selected. These variables: private_work, public_work, self_employed describe whether people work for the government, for private companies, or for themselves.\nIn these exercises, you’ll sort these observations to find the most interesting cases."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp-1",
    "href": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp-1",
    "title": "20  Transforming Data with dplyr",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nAdd a verb to sort the observations of the public_work variable in descending order.\n\n\nE2.R\n\ncounties_selected <- counties %>%\n  select(state, county, population, private_work, public_work, self_employed)\n\ncounties_selected %>%\n  # Add a verb to sort in descending order of public_work\n  arrange(desc(public_work))"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#filtering-for-conditions",
    "href": "Data_Manipulation_with_dplyr_C1.html#filtering-for-conditions",
    "title": "20  Transforming Data with dplyr",
    "section": "20.3 Filtering for conditions",
    "text": "20.3 Filtering for conditions\nYou use the filter() verb to get only observations that match a particular condition, or match multiple conditions."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp-2",
    "href": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp-2",
    "title": "20  Transforming Data with dplyr",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFind only the counties that have a population above one million (1000000).\nFind only the counties in the state of California that also have a population above one million (1000000).\n\n\n\nE3.R\n\ncounties_selected <- counties %>%\n  select(state, county, population)\n\ncounties_selected %>%\n  # Filter for counties with a population above 1000000\n  filter(population > 1000000)\n\n\ncounties_selected <- counties %>%\n  select(state, county, population)\n\ncounties_selected %>%\n  # Filter for counties with a population above 1000000\n  filter(state == \"California\",\n         population > 1000000)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#filtering-and-arranging",
    "href": "Data_Manipulation_with_dplyr_C1.html#filtering-and-arranging",
    "title": "20  Transforming Data with dplyr",
    "section": "20.4 Filtering and arranging",
    "text": "20.4 Filtering and arranging\nWe’re often interested in both filtering and sorting a dataset, to focus on observations of particular interest to you. Here, you’ll find counties that are extreme examples of what fraction of the population works in the private sector."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp-3",
    "href": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp-3",
    "title": "20  Transforming Data with dplyr",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFilter for counties in the state of Texas that have more than ten thousand people (10000), and sort them in descending order of the percentage of people employed in private work.\n\n\nE4.R\n\ncounties_selected <- counties %>%\n  select(state, county, population, private_work, public_work, self_employed)\n\n# Filter for Texas and more than 10000 people; sort in descending order of \n#private_work\ncounties_selected %>%\n  # Filter for Texas and more than 10000 people\n  filter(state == \"Texas\",\n         population > 10000) %>%\n  # Sort in descending order of private_work\n  arrange(desc(private_work))"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#calculating-the-percentage-of-women-in-a-county",
    "href": "Data_Manipulation_with_dplyr_C1.html#calculating-the-percentage-of-women-in-a-county",
    "title": "20  Transforming Data with dplyr",
    "section": "20.5 Calculating the percentage of women in a county",
    "text": "20.5 Calculating the percentage of women in a county\nThe dataset includes columns for the total number (not percentage) of men and women in each county. You could use this, along with the population variable, to compute the fraction of men (or women) within each county.\nIn this exercise, you’ll select the relevant columns yourself."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp-4",
    "href": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp-4",
    "title": "20  Transforming Data with dplyr",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSelect the columns state, county, population, men, and women.\nAdd a new variable called proportion_women with the fraction of the county’s population made up of women.\n\n\n\nE5.R\n\ncounties_selected <- counties %>%\n  # Select the columns state, county, population, men, and women\n  select(state, county, population, men, women)\n\ncounties_selected %>%\n  # Calculate proportion_women as the fraction of the population made up of \n  women\n  mutate(proportion_women= women / population)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#select-mutate-filter-and-arrange",
    "href": "Data_Manipulation_with_dplyr_C1.html#select-mutate-filter-and-arrange",
    "title": "20  Transforming Data with dplyr",
    "section": "20.6 Select, mutate, filter, and arrange",
    "text": "20.6 Select, mutate, filter, and arrange\nIn this exercise, you’ll put together everything you’ve learned in this chapter (select(), mutate(), filter() and arrange()), to find the counties with the highest proportion of men."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp-5",
    "href": "Data_Manipulation_with_dplyr_C1.html#instructions-100-xp-5",
    "title": "20  Transforming Data with dplyr",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSelect only the columns state, county, population, men, and women.\nAdd a variable proportion_men with the fraction of the county’s population made up of men.\nFilter for counties with a population of at least ten thousand (10000).\nArrange counties in descending order of their proportion of men.\n\n\n\nE6.R\n\n\ncounties %>%\n  # Select the five columns \n  select(state, county, population, men, women) %>%\n  # Add the proportion_men variable\n  mutate(proportion_men = men / population) %>%\n  # Filter for population of at least 10,000\n  filter(population >= 10000) %>%\n  # Arrange proportion of men in descending order \n  arrange(desc(proportion_men))"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#counting-by-region",
    "href": "Data_Manipulation_with_dplyr_C2.html#counting-by-region",
    "title": "21  Aggregating Data",
    "section": "21.1 Counting by region",
    "text": "21.1 Counting by region\nThe counties dataset contains columns for region, state, population, and the number of citizens, which we selected and saved as the counties_selected table. In this exercise, you’ll focus on the region column.\n\ncounties_selected <- counties %>% select(county, region, state, population, citizens)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp",
    "href": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp",
    "title": "21  Aggregating Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse count() to find the number of counties in each region, using a second argument to sort in descending order.\n\n\n\nE1.R\n\n# Use count to find the number of counties in each region\ncounties_selected %>%\n  count(region, sort=TRUE)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#counting-citizens-by-state",
    "href": "Data_Manipulation_with_dplyr_C2.html#counting-citizens-by-state",
    "title": "21  Aggregating Data",
    "section": "21.2 Counting citizens by state",
    "text": "21.2 Counting citizens by state\nYou can weigh your count by particular variables rather than finding the number of counties. In this case, you’ll find the number of citizens in each state.\n\ncounties_selected <- counties %>% select(county, region, state, population, citizens)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-1",
    "href": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-1",
    "title": "21  Aggregating Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCount the number of counties in each state, weighted based on the citizens column, and sorted in descending order.\n\n\n\nE2.R\n\n# Find number of counties per state, weighted by citizens, sorted in descending order\ncounties_selected %>%\n    count(state, wt = citizens, sort = TRUE)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#mutating-and-counting",
    "href": "Data_Manipulation_with_dplyr_C2.html#mutating-and-counting",
    "title": "21  Aggregating Data",
    "section": "21.3 Mutating and counting",
    "text": "21.3 Mutating and counting\nYou can combine multiple verbs together to answer increasingly complicated questions of your data. For example: “What are the US states where the most people walk to work?”\nYou’ll use the walk column, which offers a percentage of people in each county that walk to work, to add a new column and count based on it.\n\ncounties_selected <- counties %>% select(county, region, state, population, walk)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-2",
    "href": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-2",
    "title": "21  Aggregating Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse mutate() to calculate and add a column called population_walk, containing the total number of people who walk to work in a county.\nUse a (weighted and sorted) count() to find the total number of people who walk to work in each state.\n\n\n\nE3.R\n\ncounties_selected %>%\n  # Add population_walk containing the total number of people who walk to work \n    mutate(population_walk = population * walk / 100) %>%\n  # Count weighted by the new column, sort in descending order\n    count(state, wt = population_walk, sort = TRUE)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#summarizing",
    "href": "Data_Manipulation_with_dplyr_C2.html#summarizing",
    "title": "21  Aggregating Data",
    "section": "21.4 Summarizing",
    "text": "21.4 Summarizing\nThe summarize() verb is very useful for collapsing a large dataset into a single observation.\n\ncounties_selected <- counties %>% select(county, population, income, unemployment)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-3",
    "href": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-3",
    "title": "21  Aggregating Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSummarize the counties dataset to find the following columns: min_population (with the smallest population), max_unemployment (with the maximum unemployment), and average_income (with the mean of the income variable).\n\n\n\nE4.R\n\ncounties_selected %>%\n  # Summarize to find minimum population, maximum unemployment, and average income\n  summarize(min_population = min(population),\n            max_unemployment = max(unemployment),\n            average_income = mean(income))"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#summarizing-by-state",
    "href": "Data_Manipulation_with_dplyr_C2.html#summarizing-by-state",
    "title": "21  Aggregating Data",
    "section": "21.5 Summarizing by state",
    "text": "21.5 Summarizing by state\nAnother interesting column is land_area, which shows the land area in square miles. Here, you’ll summarize both population and land area by state, with the purpose of finding the density (in people per square miles).\n\ncounties_selected <- counties %>% select(state, county, population, land_area)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-4",
    "href": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-4",
    "title": "21  Aggregating Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGroup the data by state, and summarize to create the columns total_area (with total area in square miles) and total_population (with total population).\nAdd a density column with the people per square mile, then arrange in descending order.\n\n\n\nE5.R\n\ncounties_selected %>%\n  # Group by state \n  group_by(state) %>%\n  # Find the total area and population\n  summarize(total_area = sum(land_area),\n            total_population = sum(population))\n            \n\ncounties_selected %>%\n  group_by(state) %>%\n  summarize(total_area = sum(land_area),\n            total_population = sum(population)) %>%\n  # Add a density column\n  mutate(density = total_population / total_area) %>%\n  # Sort by density in descending order\n  arrange(desc(density))"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#summarizing-by-state-and-region",
    "href": "Data_Manipulation_with_dplyr_C2.html#summarizing-by-state-and-region",
    "title": "21  Aggregating Data",
    "section": "21.6 Summarizing by state and region",
    "text": "21.6 Summarizing by state and region\nYou can group by multiple columns instead of grouping by one. Here, you’ll practice aggregating by state and region, and notice how useful it is for performing multiple aggregations in a row.\n\ncounties_selected <- counties %>% select(region, state, county, population)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-5",
    "href": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-5",
    "title": "21  Aggregating Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSummarize to find the total population, as a column called total_pop, in each combination of region and state, grouped in that order.\nNotice the tibble is still grouped by region; use another summarize() step to calculate two new columns: the average state population in each region (average_pop) and the median state population in each region (median_pop).\n\n\n\nE6.R\n\ncounties_selected %>%\n  # Group and summarize to find the total population\n  group_by(region, state) %>%\n  summarize(total_pop = sum(population))\n  \ncounties_selected %>%\n  # Group and summarize to find the total population\n  group_by(region, state) %>%\n  summarize(total_pop = sum(population)) %>%\n  # Calculate the average_pop and median_pop columns \n  summarize(average_pop = mean(total_pop),\n            median_pop = median(total_pop))"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#selecting-a-county-from-each-region",
    "href": "Data_Manipulation_with_dplyr_C2.html#selecting-a-county-from-each-region",
    "title": "21  Aggregating Data",
    "section": "21.7 Selecting a county from each region",
    "text": "21.7 Selecting a county from each region\nPreviously, you used the walk column, which offers a percentage of people in each county that walk to work, to add a new column and count to find the total number of people who walk to work in each county.\nNow, you’re interested in finding the county within each region with the highest percentage of citizens who walk to work.\n\ncounties_selected <- counties %>% select(region, state, county, metro, population, walk)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-6",
    "href": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-6",
    "title": "21  Aggregating Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFind the county in each region with the highest percentage of citizens who walk to work.\n\n\n\nE7.R\n\ncounties_selected %>%\n  # Group by region\n  group_by(region) %>%\n  # Find the county with the highest percentage of people who walk to work\nslice_max(walk, n = 1)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#finding-the-lowest-income-state-in-each-region",
    "href": "Data_Manipulation_with_dplyr_C2.html#finding-the-lowest-income-state-in-each-region",
    "title": "21  Aggregating Data",
    "section": "21.8 Finding the lowest-income state in each region",
    "text": "21.8 Finding the lowest-income state in each region\nYou’ve been learning to combine multiple dplyr verbs together. Here, you’ll combine group_by(), summarize(), and slice_min() to find the state in each region with the highest income.\nWhen you group by multiple columns and then summarize, it’s important to remember that the summarize “peels off” one of the groups, but leaves the rest on. For example, if you group_by(X, Y) then summarize, the result will still be grouped by X.\ncounties_selected <- counties %>% select(region, state, county, population, income)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-7",
    "href": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-7",
    "title": "21  Aggregating Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the average income (as average_income) of counties within each region and state (notice the group_by() has already been done for you).\nFind the state with the lowest average_income in each region.\n\n\n\nE8.R\n\ncounties_selected %>%\n  group_by(region, state) %>%\n  # Calculate average income\n  summarize(average_income = mean(income)) %>%\n  # Find the lowest income state in each region\n  slice_min(average_income, n=1)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#using-summarize-slice_max-and-count-together",
    "href": "Data_Manipulation_with_dplyr_C2.html#using-summarize-slice_max-and-count-together",
    "title": "21  Aggregating Data",
    "section": "21.9 Using summarize, slice_max, and count together",
    "text": "21.9 Using summarize, slice_max, and count together\nIn this chapter, you’ve learned to use six dplyr verbs related to aggregation: count(), group_by(), summarize(), ungroup(), slice_max(), and slice_min(). In this exercise, you’ll combine them to answer a question:\nIn how many states do more people live in metro areas than non-metro areas?\nRecall that the metro column has one of the two values “Metro” (for high-density city areas) or “Nonmetro” (for suburban and country areas).\n\ncounties_selected <- counties %>% select(state, metro, population)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-8",
    "href": "Data_Manipulation_with_dplyr_C2.html#instructions-100-xp-8",
    "title": "21  Aggregating Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFor each combination of state and metro, find the total population as total_pop.\nExtract the most populated row from each state, which will be either Metro or Nonmetro.\nUngroup, then count how often Metro or Nonmetro appears to see how many states have more people living in those areas.\n\n\n\nE9.R\n\ncounties_selected %>%\n  # Find the total population for each combination of state and metro\n  group_by(state, metro) %>%\n  summarize(total_pop = sum(population))\n  \ncounties_selected %>%\n  # Find the total population for each combination of state and metro\n  group_by(state, metro) %>%\n  summarize(total_pop = sum(population)) %>%\n  # Extract the most populated row for each state\n  slice_max(total_pop, n = 1)\n  \ncounties_selected %>%\n  # Find the total population for each combination of state and metro\n  group_by(state, metro) %>%\n  summarize(total_pop = sum(population)) %>%\n  # Extract the most populated row for each state\n  slice_max(total_pop, n = 1) %>%\n  # Count the states with more people in Metro or Nonmetro areas\n  ungroup() %>% \n  count(metro)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#selecting-columns",
    "href": "Data_Manipulation_with_dplyr_C3.html#selecting-columns",
    "title": "22  Selecting and Transforming Data",
    "section": "22.1 Selecting columns",
    "text": "22.1 Selecting columns\nUsing the select() verb, we can answer interesting questions about our dataset by focusing in on related groups of verbs. The colon (:) is useful for getting many columns at a time."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp",
    "href": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp",
    "title": "22  Selecting and Transforming Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse glimpse() to examine all the variables in the counties table.\nSelect the columns for state, county, population, and (using a colon) all five of those industry-related variables; there are five consecutive variables in the table related to the industry of people’s work: professional, service, office, construction, and production.\nArrange the table in descending order of service to find which counties have the highest rates of working in the service industry.\n\n\n\nE1.R\n\n# Glimpse the counties table\nglimpse(counties)\n\ncounties %>%\n  # Select state, county, population, and industry-related columns\n  select(state, county, population,professional:production) %>%\n  # Arrange service in descending order \n   arrange(desc(service))"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#select-helpers",
    "href": "Data_Manipulation_with_dplyr_C3.html#select-helpers",
    "title": "22  Selecting and Transforming Data",
    "section": "22.2 Select helpers",
    "text": "22.2 Select helpers\nIn the video you learned about the select helper starts_with(). Another select helper is ends_with(), which finds the columns that end with a particular string."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp-1",
    "href": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp-1",
    "title": "22  Selecting and Transforming Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSelect the columns state, county, population, and all those that end with work.\nFilter just for the counties where at least 50% of the population is engaged in public work.\n\n\n\nE2.R\n\ncounties %>%\n  # Select the state, county, population, and those ending with \"work\"\n  select(state, county, population,ends_with(\"work\")) %>%\n  # Filter for counties that have at least 50% of people engaged in public work\n  filter(public_work  >= 50)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#renaming-a-column-after-count",
    "href": "Data_Manipulation_with_dplyr_C3.html#renaming-a-column-after-count",
    "title": "22  Selecting and Transforming Data",
    "section": "22.3 Renaming a column after count",
    "text": "22.3 Renaming a column after count\nThe rename() verb is often useful for changing the name of a column that comes out of another verb, such as count(). In this exercise, you’ll rename the default n column generated from count() to something more descriptive."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp-2",
    "href": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp-2",
    "title": "22  Selecting and Transforming Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse count() to determine how many counties are in each state.\nNotice the n column in the output; use rename() to rename that to num_counties.\n\n\n\nE3.R\n\ncounties %>%\n  # Count the number of counties in each state\n  count(state)\n  \ncounties %>%\n  # Count the number of counties in each state\n  count(state) %>%\n  # Rename the n column to num_counties\n  rename(num_counties = n)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#renaming-a-column-as-part-of-a-select",
    "href": "Data_Manipulation_with_dplyr_C3.html#renaming-a-column-as-part-of-a-select",
    "title": "22  Selecting and Transforming Data",
    "section": "22.4 Renaming a column as part of a select",
    "text": "22.4 Renaming a column as part of a select\nrename() isn’t the only way you can choose a new name for a column; you can also choose a name as part of a select()."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp-3",
    "href": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp-3",
    "title": "22  Selecting and Transforming Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSelect the columns state, county, and poverty from the counties dataset; in the same step, rename the poverty column to poverty_rate.\n\n\n\nE4.R\n\ncounties %>%\n  # Select state, county, and poverty as poverty_rate\n   select(state, county, poverty_rate = poverty)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#using-transmute",
    "href": "Data_Manipulation_with_dplyr_C3.html#using-transmute",
    "title": "22  Selecting and Transforming Data",
    "section": "22.5 Using transmute",
    "text": "22.5 Using transmute\nAs you learned in the video, the transmute verb allows you to control which variables you keep, which variables you calculate, and which variables you drop."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp-4",
    "href": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp-4",
    "title": "22  Selecting and Transforming Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nKeep only the state, county, and population columns, and add a new column, density, that contains the population per land_area.\nFilter for only counties with a population greater than one million.\nSort the table in ascending order of density.\n\n\n\nE5.R\n\ncounties %>%\n  # Keep the state, county, and populations columns, and add a density column\n transmute(state, county, population, density = population / land_area) %>%\n  # Filter for counties with a population greater than one million \n  filter(population>1000000)%>%\n  # Sort density in ascending order \n  arrange(density)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#choosing-among-the-four-verbs",
    "href": "Data_Manipulation_with_dplyr_C3.html#choosing-among-the-four-verbs",
    "title": "22  Selecting and Transforming Data",
    "section": "22.6 Choosing among the four verbs",
    "text": "22.6 Choosing among the four verbs\nIn this chapter you’ve learned about the four verbs: select, mutate, transmute, and rename. Here, you’ll choose the appropriate verb for each situation. You won’t need to change anything inside the parentheses."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp-5",
    "href": "Data_Manipulation_with_dplyr_C3.html#instructions-100-xp-5",
    "title": "22  Selecting and Transforming Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nChoose the right verb for changing the name of the unemployment column to unemployment_rate\nChoose the right verb for keeping only the columns state, county, and the ones containing poverty.\nCalculate a new column called fraction_women with the fraction of the population made up of women, without dropping any columns.\nKeep only three columns: the state, county, and employed / population, which you’ll call employment_rate.\n\n\n\nE6.R\n\n# Change the name of the unemployment column\ncounties %>%\n  rename(unemployment_rate = unemployment)\n\n# Keep the state and county columns, and the columns containing poverty\ncounties %>%\n  select(state, county, contains(\"poverty\"))\n\n# Calculate the fraction_women column without dropping the other columns\ncounties %>%\n  mutate(fraction_women = women / population)\n\n# Keep only the state, county, and employment_rate columns\ncounties %>%\n  transmute(state, county, employment_rate = employed / population)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#filtering-and-arranging-for-one-year",
    "href": "Data_Manipulation_with_dplyr_C4.html#filtering-and-arranging-for-one-year",
    "title": "23  Case Study: The babynames Dataset",
    "section": "23.1 Filtering and arranging for one year",
    "text": "23.1 Filtering and arranging for one year\nThe dplyr verbs you’ve learned are useful for exploring data. For instance, you could find out the most common names in a particular year."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp",
    "href": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp",
    "title": "23  Case Study: The babynames Dataset",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFilter for only the year 1990.\nSort the table in descending order of the number of babies born.\n\n\n\nE1.R\n\nbabynames %>%\n  # Filter for the year 1990\n  filter(year == 1990) %>%\n  # Sort the number column in descending order \n  arrange(desc(number))"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#finding-the-most-popular-names-each-year",
    "href": "Data_Manipulation_with_dplyr_C4.html#finding-the-most-popular-names-each-year",
    "title": "23  Case Study: The babynames Dataset",
    "section": "23.2 Finding the most popular names each year",
    "text": "23.2 Finding the most popular names each year\nYou saw that you could use filter() and arrange() to find the most common names in one year. However, you could also use group_by() and slice_max() to find the most common name in every year."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-1",
    "href": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-1",
    "title": "23  Case Study: The babynames Dataset",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse group_by() and slice_max() to find the most common name for US babies in each year.\n\n\n\nE2.R\n\nbabynames %>%\n  # Find the most common name in each year\n  group_by(year) %>%\n  slice_max(number, n = 1)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#visualizing-names-with-ggplot2",
    "href": "Data_Manipulation_with_dplyr_C4.html#visualizing-names-with-ggplot2",
    "title": "23  Case Study: The babynames Dataset",
    "section": "23.3 Visualizing names with ggplot2",
    "text": "23.3 Visualizing names with ggplot2\nThe dplyr package is very useful for exploring data, but it’s especially useful when combined with other tidyverse packages like ggplot2."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-2",
    "href": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-2",
    "title": "23  Case Study: The babynames Dataset",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFilter for only the names Steven, Thomas, and Matthew, and assign it to an object called selected_names.\nVisualize those three names as a line plot over time, with each name represented by a different color.\n\n\n\nE3.R\n\nselected_names <- babynames %>%\n  # Filter for the names Steven, Thomas, and Matthew \n  filter(name %in% c(\"Steven\", \"Thomas\", \"Matthew\"))\n  \nselected_names <- babynames %>%\n  # Filter for the names Steven, Thomas, and Matthew \n  filter(name %in% c(\"Steven\", \"Thomas\", \"Matthew\"))\n\n# Plot the names using a different color for each name\nggplot(selected_names, aes(x = year, y = number, color = name)) +\n  geom_line()"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#finding-the-year-each-name-is-most-common",
    "href": "Data_Manipulation_with_dplyr_C4.html#finding-the-year-each-name-is-most-common",
    "title": "23  Case Study: The babynames Dataset",
    "section": "23.4 Finding the year each name is most common",
    "text": "23.4 Finding the year each name is most common\nIn an earlier video, you learned how to filter for a particular name to determine the frequency of that name over time. Now, you’re going to explore which year each name was the most common.\nTo do this, you’ll be combining the grouped mutate approach with a slice_max()."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-3",
    "href": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-3",
    "title": "23  Case Study: The babynames Dataset",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFirst, calculate the total number of people born in that year in this dataset as year_total.\nNext, use year_total to calculate the fraction of people born in each year that have each name.\nNow use your newly calculated fraction column, in combination with slice_max(), to identify the year each name was most common.\n\n\n\nE4.R\n\n# Calculate the fraction of people born each year with the same name\nbabynames %>%\n  group_by(year) %>%\n  mutate(year_total = sum(number)) %>%\n  ungroup() %>%\n  mutate(fraction = number / year_total)\n\n# Calculate the fraction of people born each year with the same name\nbabynames %>%\n  group_by(year) %>%\n  mutate(year_total = sum(number)) %>%\n  ungroup() %>%\n  mutate(fraction = number / year_total) %>%\n  # Find the year each name is most common\n  group_by(name) %>%\n  slice_max(fraction, n = 1)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#adding-the-total-and-maximum-for-each-name",
    "href": "Data_Manipulation_with_dplyr_C4.html#adding-the-total-and-maximum-for-each-name",
    "title": "23  Case Study: The babynames Dataset",
    "section": "23.5 Adding the total and maximum for each name",
    "text": "23.5 Adding the total and maximum for each name\nIn the video, you learned how you could group by the year and use mutate() to add a total for that year.\nIn these exercises, you’ll learn to normalize by a different, but also interesting metric: you’ll divide each name by the maximum for that name. This means that every name will peak at 1.\nOnce you add new columns, the result will still be grouped by name. This splits it into 48,000 groups, which actually makes later steps like mutates slower."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-4",
    "href": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-4",
    "title": "23  Case Study: The babynames Dataset",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse a grouped mutate to add two columns:\nname_total: the sum of the number of babies born with that name in the entire dataset.\nname_max: the maximum number of babies born with that name in any year.\nAdd another step to ungroup the table.\nAdd a column called fraction_max containing the number in the year divided by name_max.\n\n\n\nE5.R\n\nbabynames %>%\n  # Add columns name_total and name_max for each name\n  group_by(name) %>%\n  mutate(name_total = sum(number),\n         name_max = max(number))\n         \nbabynames %>%\n  # Add columns name_total and name_max for each name\n  group_by(name) %>%\n  mutate(name_total = sum(number),\n         name_max = max(number)) %>%\n  # Ungroup the table \n  ungroup() %>%\n  # Add the fraction_max column containing the number by the name maximum \n  mutate(fraction_max = number / name_max)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#visualizing-the-normalized-change-in-popularity",
    "href": "Data_Manipulation_with_dplyr_C4.html#visualizing-the-normalized-change-in-popularity",
    "title": "23  Case Study: The babynames Dataset",
    "section": "23.6 Visualizing the normalized change in popularity",
    "text": "23.6 Visualizing the normalized change in popularity\nYou picked a few names and calculated each of them as a fraction of their peak. This is a type of “normalizing” a name, where you’re focused on the relative change within each name rather than the overall popularity of the name.\nIn this exercise, you’ll visualize the normalized popularity of each name. Your work from the previous exercise, names_normalized, has been provided for you.\n\nnames_normalized <- babynames %>% group_by(name) %>% mutate(name_total = sum(number), name_max = max(number)) %>% ungroup() %>% mutate(fraction_max = number / name_max)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-5",
    "href": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-5",
    "title": "23  Case Study: The babynames Dataset",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFilter the names_normalized table to limit it to the three names Steven, Thomas, and Matthew.\nCreate a line plot to visualize fraction_max over time, colored by name.\n\n\n\nE6.R\n\nnames_filtered <- names_normalized %>%\n  # Filter for the names Steven, Thomas, and Matthew\n  filter(name %in% c(\"Steven\", \"Thomas\", \"Matthew\"))\n\n# Visualize these names over time\nggplot(names_filtered, aes(x = year, y = fraction_max, color = name)) +\n  geom_line()"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#using-ratios-to-describe-the-frequency-of-a-name",
    "href": "Data_Manipulation_with_dplyr_C4.html#using-ratios-to-describe-the-frequency-of-a-name",
    "title": "23  Case Study: The babynames Dataset",
    "section": "23.7 Using ratios to describe the frequency of a name",
    "text": "23.7 Using ratios to describe the frequency of a name\nIn the video, you learned how to find the difference in the frequency of a baby name between consecutive years. What if instead of finding the difference, you wanted to find the ratio?\nYou’ll start with the babynames_fraction data already, so that you can consider the popularity of each name within each year."
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-6",
    "href": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-6",
    "title": "23  Case Study: The babynames Dataset",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nArrange the data in ascending order of name and then year.\nGroup by name so that your mutate works within each name.\nAdd a column ratio containing the ratio (not difference) of fraction between each year.\n\n\n\nE7.R\n\nbabynames_fraction %>%\n  # Arrange the data in order of name, then year \n  arrange(name, year) %>%\n  # Group the data by name\n  group_by(name) %>%\n  # Add a ratio column that contains the ratio of fraction between each year \n  mutate(ratio = fraction / lag(fraction))"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#biggest-jumps-in-a-name",
    "href": "Data_Manipulation_with_dplyr_C4.html#biggest-jumps-in-a-name",
    "title": "23  Case Study: The babynames Dataset",
    "section": "23.8 Biggest jumps in a name",
    "text": "23.8 Biggest jumps in a name\nPreviously, you added a ratio column to describe the ratio of the frequency of a baby name between consecutive years to describe the changes in the popularity of a name. Now, you’ll look at a subset of that data, called babynames_ratios_filtered, to look further into the names that experienced the biggest jumps in popularity in consecutive years.\n\nbabynames_ratios_filtered <- babynames_fraction %>% arrange(name, year) %>% group_by(name) %>% mutate(ratio = fraction / lag(fraction)) %>% filter(fraction >= 0.00001)"
  },
  {
    "objectID": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-7",
    "href": "Data_Manipulation_with_dplyr_C4.html#instructions-100-xp-7",
    "title": "23  Case Study: The babynames Dataset",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFrom each name in the data, keep the observation (the year) with the largest ratio; note the data is already grouped by name.\nSort the ratio column in descending order.\nFilter the babynames_ratios_filtered data further by filtering the fraction column to only display results greater than or equal to 0.001.\n\n\n\nE8.R\n\nbabynames_ratios_filtered %>%\n  # Extract the largest ratio from each name \n  slice_max(ratio, n=1) %>%\n  # Sort the ratio column in descending order \n  arrange(desc(ratio)) %>%\n  # Filter for fractions greater than or equal to 0.001\n  filter(fraction >= 0.001)"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "24  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#multiple-variables-per-column",
    "href": "Reshaping_Data_with_tidyr_C1.html#multiple-variables-per-column",
    "title": "28  Tidy Data",
    "section": "28.1 Multiple variables per column",
    "text": "28.1 Multiple variables per column\nBeing a busy person, you don’t want to spend too much time on Netflix, so you decide to crunch some numbers on TV show and movie durations before deciding what to watch. You’ve managed to obtain a dataset named netflix_df, but its duration column has an issue. It contains strings with both a value and unit of duration (“min” or “Season”).\nYou’ll tidy this dataset so that each variable gets its own column.\nAs will always be the case in this course, the tidyr package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp",
    "href": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp",
    "title": "28  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nInspect netflix_df by typing its name directly in the R console and hitting Enter to see what string separates the value from the unit in the duration column.\nSeparate the duration column over two variables named value and unit. Pass the string separating the number from the unit to the sep argument.\n\n\n\nE1.R\n\nnetflix_df %>% \n  # Split the duration column into value and unit columns\n  separate(duration, into = c(\"value\", \"unit\"), sep = \" \", convert = TRUE)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#international-phone-numbers",
    "href": "Reshaping_Data_with_tidyr_C1.html#international-phone-numbers",
    "title": "28  Tidy Data",
    "section": "28.2 International phone numbers",
    "text": "28.2 International phone numbers\nYou work for a multinational company that uses auto-dialer software to contact its customers. When new customers subscribe online they are asked for a phone number but they often forget to add the country code needed for international calls. You were asked to fix this issue in the database. You’ve been given a data frame with national numbers and country codes named phone_nr_df. Now you want to combine the country_code and national_number columns to create valid international numbers."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-1",
    "href": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-1",
    "title": "28  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the unite() function to create a new international_number column, using an empty string as the separator\n\n\n\nE2.R\n\nphone_nr_df %>%\n  # Unite the country_code and national_number columns\n  unite(\"international_number\", country_code, national_number, sep = \" \")"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#extracting-observations-from-values",
    "href": "Reshaping_Data_with_tidyr_C1.html#extracting-observations-from-values",
    "title": "28  Tidy Data",
    "section": "28.3 Extracting observations from values",
    "text": "28.3 Extracting observations from values\nYou’re given a sample of the Netflix dataset containing TV shows and their casts called tvshow_df. You want to learn which six actors have the most appearances.\nHowever, the dataset only has one row per TV show, and multiple actors are listed in the cast column.\nTransform the data so that for each TV show, every actor has a row. The number of appearances will be calculated for you.\nThe dplyr package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#question",
    "href": "Reshaping_Data_with_tidyr_C1.html#question",
    "title": "28  Tidy Data",
    "section": "Question",
    "text": "Question\nInspect tvshow_df in the console. What string separates actors in the cast column?\nPossible Answers\n\n\n< ” ”\n\n\n\n\n“,”\n\n\n\n\n“,” Respuesta"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-2",
    "href": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-2",
    "title": "28  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse separate_rows() on the cast column, using the appropriate separator for the sep argument.\nUse the head() function to keep just the top six.\n\n\n\nE3.R\n\ntvshow_df %>% \n  # Separate the actors in the cast column over multiple rows\n  separate_rows(cast, sep = \", \") %>% \n  rename(actor = cast) %>% \n  count(actor, sort = TRUE) %>% \n  head()"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#separating-into-columns-and-rows",
    "href": "Reshaping_Data_with_tidyr_C1.html#separating-into-columns-and-rows",
    "title": "28  Tidy Data",
    "section": "28.4 Separating into columns and rows",
    "text": "28.4 Separating into columns and rows\nRemember the drink ingredients data from the video? You’ve been given a similar version (drink_df) that also includes quantities and units. Now you want to create an overview of how much of each ingredient you should buy to make these drinks.\nThe dplyr package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-3",
    "href": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-3",
    "title": "28  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nInspect drink_df in the console to find the right separator in the ingredients column.\nSeparate the ingredients column so that for each drink each ingredient gets a row.\nInspect the output of the previous step to find the separator that splits the ingredients column into three columns: ingredient, quantity, and unit.\nMake sure to convert data types to numeric when possible.\nGroup the data by ingredient and unit.\nCalculate the total quantity of each ingredient.\n\n\n\nE4.R\n\ndrink_df %>%\n  # Separate the ingredients over rows\n  separate_rows(ingredients, sep = \"; \")\n  \n  \ndrink_df %>% \n  # Separate the ingredients over rows\n  separate_rows(ingredients, sep = \"; \") %>% \n  # Separate ingredients into three columns\n  separate(\n    ingredients, \n    into = c(\"ingredient\", \"quantity\", \"unit\"), \n    sep = \" \", \n    convert = TRUE\n  ) \n\n\ndrink_df %>% \n  # Separate the ingredients over rows\n  separate_rows(ingredients, sep = \"; \") %>% \n  # Separate ingredients into three columns\n  separate(\n    ingredients, \n    into = c(\"ingredient\", \"quantity\", \"unit\"), \n    sep = \" \", \n    convert = TRUE\n  ) %>% \n  # Group by ingredient and unit\n  group_by(ingredient, unit) %>% \n  # Calculate the total quantity of each ingredient\n  summarize(quantity = sum(quantity))"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#and-the-oscar-for-best-director-goes-to",
    "href": "Reshaping_Data_with_tidyr_C1.html#and-the-oscar-for-best-director-goes-to",
    "title": "28  Tidy Data",
    "section": "28.5 And the Oscar for best director goes to … ",
    "text": "28.5 And the Oscar for best director goes to … \nYou’re working on a sample of the Netflix dataset pre-loaded as director_df. This time, the data frame contains just the directors and movie titles. Your goal is to identify the directors who created the most movies. Since the director column contains multiple names, you’ll first separate its values over multiple rows and then count the directors.\nSince you don’t want movies without directors polluting your overview, you’ll apply the drop_na() function.\nThe dplyr package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-4",
    "href": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-4",
    "title": "28  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nInspect director_df in the console to see what string separates directors in the director column.\nSpread the values in the director column over separate rows.\nCount the number of times each director appears in the data. Make sure to sort the output.\nDrop rows containing NA values in the director column.\n\n\n\nE5.R\n\ndirector_df %>% \n  # Spread the director column over separate rows\n  separate_rows(director, sep = \", \")\n\n\ndirector_df %>% \n  # Spread the director column over separate rows\n  separate_rows(director, sep = \", \") %>% \n  # Count the number of movies per director\n  count(director, sort = TRUE)\n\n\ndirector_df %>% \n  # Drop rows with NA values in the director column\n  drop_na() %>% \n  # Spread the director column over separate rows\n  separate_rows(director, sep = \", \") %>% \n  # Count the number of movies per director\n  count(director, sort = TRUE)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#imputing-sales-data",
    "href": "Reshaping_Data_with_tidyr_C1.html#imputing-sales-data",
    "title": "28  Tidy Data",
    "section": "28.6 Imputing sales data",
    "text": "28.6 Imputing sales data\nYou’ve been asked to create a report that allows management to compare sales figures per quarter for two years. The problem is that the dataset (sales_df) contains missing values. You’ll need to impute the values in the year column so that you can visualize the data.\nThe ggplot2 package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-5",
    "href": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-5",
    "title": "28  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nInspect sales_df in the console, pay attention to the year column.\nUse the fill() function to impute the year column in the correct direction.\nCreate a line plot where each year has a different color.\n\n\n\nE6.R\n\nsales_df %>% \n  # Impute the year column\n  fill(year, .direction = \"up\") %>%\n  # Create a line plot with sales per quarter colored by year.\n  ggplot(aes(x = quarter, y = sales, color = year, group = year)) +\n  geom_line()"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#nuclear-bombs-per-continent",
    "href": "Reshaping_Data_with_tidyr_C1.html#nuclear-bombs-per-continent",
    "title": "28  Tidy Data",
    "section": "28.7 Nuclear bombs per continent",
    "text": "28.7 Nuclear bombs per continent\nSince WWII, a number of nations have been detonating nuclear bombs for military research. A tally of bombs detonated per nation has been calculated from the Nuclear Explosion DataBase (NEDB) and provided as nuke_df. You are interested in finding out how many bombs have been detonated by nations grouped per continent. To achieve this goal, nuke_df will be joined to country_to_continent_df which is a mapping of nation to continent. You will need to overwrite missing values with zeros so that you can create a nice plot.\nThe dplyr and ggplot2 packages have been pre-loaded for you.\nSide note 1: Bombs detonated by the Soviet Union were attributed to the Russian Federation.\nSide note 2: The Russian Federation is solely mapped to Europe for simplicity."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-6",
    "href": "Reshaping_Data_with_tidyr_C1.html#instructions-100-xp-6",
    "title": "28  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nInspect nuke_df and country_to_continent_df in the console.\nReplace the missing values in the n_bombs columns with 0L. Adding the L sets the data type to integer.\nGroup the dataset by continent and aggregate the data by summing the number of bombs.\nPlot the summed number of bombs detonated by nations from each continent.\n\n\n\nE7.R\n\ncountry_to_continent_df %>% \n  left_join(nuke_df, by = \"country_code\") %>% \n  # Impute the missing values in the n_bombs column with 0L\n    replace_na(list(n_bombs = 0L))\n    \n\ncountry_to_continent_df %>% \n  left_join(nuke_df, by = \"country_code\") %>% \n  # Impute the missing values in the n_bombs column with 0L\n  replace_na(list(n_bombs = 0L)) %>% \n  # Group the dataset by continent\n  group_by(continent) %>% \n  # Sum the number of bombs per continent\n  summarize(n_bombs_continent = sum(n_bombs))\n\n\ncountry_to_continent_df %>% \n  left_join(nuke_df, by = \"country_code\") %>%  \n  # Impute the missing values in the n_bombs column with 0L\n  replace_na(list(n_bombs = 0L)) %>% \n  # Group the dataset by continent\n  group_by(continent) %>% \n  # Sum the number of bombs per continent\n  summarize(n_bombs_continent = sum(n_bombs)) %>% \n  # Plot the number of bombs per continent\n  ggplot(aes(x = continent, y = n_bombs_continent)) +\n  geom_col()"
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#converting-data-types",
    "href": "Cleaning_Data_in_R_C1.html#converting-data-types",
    "title": "24  Common Data Problems",
    "section": "24.1 Converting data types",
    "text": "24.1 Converting data types\nThroughout this chapter, you’ll be working with San Francisco bike share ride data called bike_share_rides. It contains information on start and end stations of each trip, the trip duration, and some user information.\nBefore beginning to analyze any dataset, it’s important to take a look at the different types of columns you’ll be working with, which you can do using glimpse().\nIn this exercise, you’ll take a look at the data types contained in bike_share_rides and see how an incorrect data type can flaw your analysis.\ndplyr and assertive are loaded and bike_share_rides is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#instructions-100-xp",
    "href": "Cleaning_Data_in_R_C1.html#instructions-100-xp",
    "title": "24  Common Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExamine the data types of the columns of bike_share_rides.\nGet a summary of the user_birth_year column of bike_share_rides.\nAdd a new column to bike_share_rides called user_birth_year_fct, which contains user_birth_year, converted to a factor.\nAssert that the user_birth_year_fct is a factor to confirm the conversion. ##Question {.unnumbered}\n\nThe summary statistics of user_birth_year don’t seem to offer much useful information about the different birth years in our dataset. Why do you think that is?\nAnswers\nThe user_birth_year column is not of the correct type and should be converted to a character.\nThe user_birth_year column has an infinite set of possible values and should be converted to a factor.\nThe user_birth_year column represents groupings of data and should be converted to a factor. Respuesta\n\n\nE1.R\n\n# Glimpse at bike_share_rides\nglimpse(bike_share_rides)\n\n# Summary of user_birth_year\nsummary(bike_share_rides$user_birth_year)\n\n# Glimpse at bike_share_rides\nglimpse(bike_share_rides)\n\n# Summary of user_birth_year\nsummary(bike_share_rides$user_birth_year)\n\n# Convert user_birth_year to factor: user_birth_year_fct\nbike_share_rides <- bike_share_rides %>%\n  mutate(user_birth_year_fct = as.factor(user_birth_year))\n\n# Assert user_birth_year_fct is a factor\nassert_is_factor(bike_share_rides$user_birth_year_fct)\n\n# Summary of user_birth_year_fct\nsummary(bike_share_rides$user_birth_year_fct)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#trimming-strings",
    "href": "Cleaning_Data_in_R_C1.html#trimming-strings",
    "title": "24  Common Data Problems",
    "section": "24.2 Trimming strings",
    "text": "24.2 Trimming strings\nIn the previous exercise, you were able to identify the correct data type and convert user_birth_year to the correct type, allowing you to extract counts that gave you a bit more insight into the dataset.\nAnother common dirty data problem is having extra bits like percent signs or periods in numbers, causing them to be read in as characters. In order to be able to crunch these numbers, the extra bits need to be removed and the numbers need to be converted from character to numeric. In this exercise, you’ll need to convert the duration column from character to numeric, but before this can appen, the word “minutes” needs to be removed from each value.\ndplyr, assertive, and stringr are loaded and bike_share_rides is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#instructions-100-xp-1",
    "href": "Cleaning_Data_in_R_C1.html#instructions-100-xp-1",
    "title": "24  Common Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse str_remove() to remove “minutes” from the duration column of bike_share_rides. Add this as a new column called duration_trimmed.\nConvert the duration_trimmed column to a numeric type and add this as a new column called duration_mins.\nGlimpse at bike_share_rides and assert that the duration_mins column is numeric.\nCalculate the mean of duration_mins.\n\n\n\nE2.R\n\nbike_share_rides <- bike_share_rides %>%\n  # Remove 'minutes' from duration: duration_trimmed\n  mutate(duration_trimmed = str_remove(duration, \"minutes\"),\n         # Convert duration_trimmed to numeric: duration_mins\n         duration_mins = as.numeric(duration_trimmed))\n\n# Glimpse at bike_share_rides\nglimpse(bike_share_rides)\n\n# Assert duration_mins is numeric\nassert_is_numeric(bike_share_rides$duration_mins)\n\n# Calculate mean duration\nmean(bike_share_rides$duration_mins)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#ride-duration-constraints",
    "href": "Cleaning_Data_in_R_C1.html#ride-duration-constraints",
    "title": "24  Common Data Problems",
    "section": "24.3 Ride duration constraints",
    "text": "24.3 Ride duration constraints\nValues that are out of range can throw off an analysis, so it’s important to catch them early on. In this exercise, you’ll be examining the duration_min column more closely. Bikes are not allowed to be kept out for more than 24 hours, or 1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned.\nIn this exercise, you’ll replace erroneous data with the range limit (1440 minutes), however, you could just as easily replace these values with NAs.\ndplyr, assertive, and ggplot2 are loaded and bike_share_rides is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#instructions-100-xp-2",
    "href": "Cleaning_Data_in_R_C1.html#instructions-100-xp-2",
    "title": "24  Common Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a three-bin histogram of the duration_min column of bike_share_rides using ggplot2 to identify if there is out-of-range data.\nReplace the values of duration_min that are greater than 1440 minutes (24 hours) with 1440. Add this to bike_share_rides as a new column called duration_min_const.\nAssert that all values of duration_min_const are between 0 and 1440.\n\n\n\nE3.R\n\n# Create breaks\nbreaks <- c(min(bike_share_rides$duration_min), 0, 1440, max(bike_share_rides$duration_min))\n\n# Create a histogram of duration_min\nggplot(bike_share_rides, aes(duration_min)) +\n  geom_histogram(breaks = breaks)\n  \n# Create breaks\nbreaks <- c(min(bike_share_rides$duration_min), 0, 1440, max(bike_share_rides$duration_min))\n\n# Create a histogram of duration_min\nggplot(bike_share_rides, aes(duration_min)) +\n  geom_histogram(breaks = breaks)\n\n# duration_min_const: replace vals of duration_min > 1440 with 1440\nbike_share_rides <- bike_share_rides %>%\n  mutate(duration_min_const = replace(duration_min, duration_min > 1440, 1440))\n\n# Make sure all values of duration_min_const are between 0 and 1440\nassert_all_are_in_closed_range(bike_share_rides$duration_min_const, lower = 0, upper = 1440)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#back-to-the-future",
    "href": "Cleaning_Data_in_R_C1.html#back-to-the-future",
    "title": "24  Common Data Problems",
    "section": "24.4 Back to the future",
    "text": "24.4 Back to the future\nSomething has gone wrong and it looks like you have data with dates from the future, which is way outside of the date range you expected to be working with. To fix this, you’ll need to remove any rides from the dataset that have a date in the future. Before you can do this, the date column needs to be converted from a character to a Date. Having these as Date objects will make it much easier to figure out which rides are from the future, since R makes it easy to check if one Date object is before (<) or after (>) another.\ndplyr and assertive are loaded and bike_share_rides is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#instructions-100-xp-3",
    "href": "Cleaning_Data_in_R_C1.html#instructions-100-xp-3",
    "title": "24  Common Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nConvert the date column of bike_share_rides from character to the Date data type.\nAssert that all values in the date column happened sometime in the past and not in the future.\nFilter bike_share_rides to get only the rides from the past or today, and save this as bike_share_rides_past.\nAssert that the dates in bike_share_rides_past occurred only in the past.\n\n\n\nE4.R\n\nlibrary(lubridate)\n# Convert date to Date type\nbike_share_rides <- bike_share_rides %>%\n  mutate(date = as.Date(date))\n\n# Make sure all dates are in the past\nassert_all_are_in_past(bike_share_rides$date)\n\n# Filter for rides that occurred before or on today's date\nbike_share_rides_past <- bike_share_rides %>%\n  filter(date <= today())\n\n# Make sure all dates from bike_share_rides_past are in the past\nassert_all_are_in_past(bike_share_rides_past$date)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#full-duplicates",
    "href": "Cleaning_Data_in_R_C1.html#full-duplicates",
    "title": "24  Common Data Problems",
    "section": "24.5 Full duplicates",
    "text": "24.5 Full duplicates\nYou’ve been notified that an update has been made to the bike sharing data pipeline to make it more efficient, but that duplicates are more likely to be generated as a result. To make sure that you can continue using the same scripts to run your weekly analyses about ride statistics, you’ll need to ensure that any duplicates in the dataset are removed first.\nWhen multiple rows of a data frame share the same values for all columns, they’re full duplicates of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Each ride, including its ride_id should be unique.\ndplyr is loaded and bike_share_rides is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#instructions-100-xp-4",
    "href": "Cleaning_Data_in_R_C1.html#instructions-100-xp-4",
    "title": "24  Common Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the total number of full duplicates in bike_share_rides.\nRemove all full duplicates from bike_share_rides and save the new data frame as bike_share_rides_unique.\nGet the total number of full duplicates in the new bike_share_rides_unique data frame.\n\n\n\nE5.R\n\n# Count the number of full duplicates\nsum(duplicated(bike_share_rides))\n\n# Remove duplicates\nbike_share_rides_unique <- distinct(bike_share_rides)\n\n# Count the full duplicates in bike_share_rides_unique\nsum(duplicated(bike_share_rides_unique))"
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#removing-partial-duplicates",
    "href": "Cleaning_Data_in_R_C1.html#removing-partial-duplicates",
    "title": "24  Common Data Problems",
    "section": "24.6 Removing partial duplicates",
    "text": "24.6 Removing partial duplicates\nNow that you’ve identified and removed the full duplicates, it’s time to check for partial duplicates. Partial duplicates are a bit tricker to deal with than full duplicates. In this exercise, you’ll first identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first.\ndplyr is loaded and bike_share_rides is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#instructions-100-xp-5",
    "href": "Cleaning_Data_in_R_C1.html#instructions-100-xp-5",
    "title": "24  Common Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCount the number of occurrences of each ride_id.\nFilter for ride_ids that occur multiple times.\nRemove full and partial duplicates from bike_share_rides based on ride_id only, keeping all columns.\nStore this as bike_share_rides_unique.\nFind the duplicated ride_ids in bike_share_rides_unique.\n\n\n\nE6.R\n\n# Find duplicated ride_ids\nbike_share_rides %>% \n  # Count the number of occurrences of each ride_id\n  count(ride_id) %>% \n  # Filter for rows with a count > 1\n  filter(n > 1)\n  \n# Find duplicated ride_ids\nbike_share_rides %>% \n  count(ride_id) %>% \n  filter(n > 1)\n\n# Remove full and partial duplicates\nbike_share_rides_unique <- bike_share_rides %>%\n  # Only based on ride_id instead of all cols\n  distinct(ride_id, .keep_all = TRUE)\n  \n\n# Find duplicated ride_ids\nbike_share_rides %>% \n  count(ride_id) %>% \n  filter(n > 1)\n\n# Remove full and partial duplicates\nbike_share_rides_unique <- bike_share_rides %>%\n  # Only based on ride_id instead of all cols\n  distinct(ride_id, .keep_all = TRUE)\n\n# Find duplicated ride_ids in bike_share_rides_unique\nbike_share_rides_unique %>%\n  # Count the number of occurrences of each ride_id\n  count(ride_id) %>%\n  # Filter for rows with a count > 1\n  filter(n > 1)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#aggregating-partial-duplicates",
    "href": "Cleaning_Data_in_R_C1.html#aggregating-partial-duplicates",
    "title": "24  Common Data Problems",
    "section": "24.7 Aggregating partial duplicates",
    "text": "24.7 Aggregating partial duplicates\nAnother way of handling partial duplicates is to compute a summary statistic of the values that differ between partial duplicates, such as mean, median, maximum, or minimum. This can come in handy when you’re not sure how your data was collected and want an average, or if based on domain knowledge, you’d rather have too high of an estimate than too low of an estimate (or vice versa).\ndplyr is loaded and bike_share_rides is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C1.html#instructions-100-xp-6",
    "href": "Cleaning_Data_in_R_C1.html#instructions-100-xp-6",
    "title": "24  Common Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGroup bike_share_rides by ride_id and date.\nAdd a column called duration_min_avg that contains the mean ride duration for the row’s ride_id and date.\nRemove duplicates based on ride_id and date, keeping all columns of the data frame.\nRemove the duration_min column.\n\n\n\nE7.R\n\nbike_share_rides %>%\n  # Group by ride_id and date\n  group_by(ride_id, date) %>%\n  # Add duration_min_avg column\n  mutate(duration_min_avg = mean(duration_min)) %>%\n  # Remove duplicates based on ride_id and date, keep all cols\n  distinct(ride_id, date, .keep_all = TRUE) %>%\n  # Remove duration_min column\n  select(-duration_min)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section",
    "href": "Reshaping_Data_with_tidyr_C2.html#section",
    "title": "29  Tidy Data",
    "section": "29.1 ",
    "text": "29.1"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPivot all columns except for year to a longer format.\nThe country names are now in the name column. Overwrite its name with country. The value column should be named n_bombs.\nReplace the NA values in the n_bombs column with integer zero values (0L).\nCreate a line plot where the number of bombs dropped per country is plotted over time. Use country to color the lines.\n\n\n\nE1.R\n\nnuke_df %>% \n  # Pivot the data to a longer format\n    pivot_longer(-year)\n\n\n\nnuke_df %>% \n  # Pivot the data to a longer format\n  pivot_longer(\n    -year, \n    # Overwrite the names of the two new columns\n    names_to = \"country\", \n    values_to = \"n_bombs\"\n  ) \n\nnuke_df %>% \n  # Pivot the data to a longer format\n  pivot_longer(\n    -year, \n    # Overwrite the names of the two new columns\n    names_to = \"country\", \n    values_to = \"n_bombs\"\n  ) %>% \n  # Replace NA values for n_bombs with 0L\n  replace_na(list(n_bombs = 0L))\n\n\nnuke_df %>% \n  # Pivot the data to a longer format\n  pivot_longer(\n    -year, \n    # Overwrite the names of the two new columns\n    names_to = \"country\", \n    values_to = \"n_bombs\"\n  ) %>% \n  # Replace NA values for n_bombs with 0L\n  replace_na(list(n_bombs = 0L)) %>% \n  # Plot the number of bombs per country over time\n  ggplot(aes(x = year, y = n_bombs, color = country)) +\n  geom_line()"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section-1",
    "href": "Reshaping_Data_with_tidyr_C2.html#section-1",
    "title": "29  Tidy Data",
    "section": "29.2 ",
    "text": "29.2"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-1",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-1",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPivot the male and female columns. The old column names should go in the sex column, the original values should go in the pct_obese column.\nCreate a scatterplot with pct_obese per country colored by sex. The country variable has been ordered by overall obesity % and added for you.\n\n\n\nE2.R\n\nobesity_df %>% \n  # Pivot the male and female columns\n  pivot_longer(c(male, female),\n               names_to = \"sex\",\n               values_to = \"pct_obese\")\n               \n\n\n\nobesity_df %>% \n  # Pivot the male and female columns\n  pivot_longer(c(male, female),\n               names_to = \"sex\",\n               values_to = \"pct_obese\") %>% \n  # Create a scatter plot with pct_obese per country colored by sex\n  ggplot(aes(x = pct_obese, color = sex,\n             y = forcats::fct_reorder(country, both_sexes))) +\n  geom_point() +\n  scale_y_discrete(breaks = c(\"India\", \"Nauru\", \"Cuba\", \"Brazil\",\n                              \"Pakistan\", \"Gabon\", \"Italy\", \"Oman\",\n                              \"China\", \"United States of America\")) +\n  labs(x = \"% Obese\", y = \"Country\")"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section-2",
    "href": "Reshaping_Data_with_tidyr_C2.html#section-2",
    "title": "29  Tidy Data",
    "section": "29.3 ",
    "text": "29.3"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-2",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-2",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPivot all columns except Bond to a longer format and set the names of the newly created columns to decade and n_movies.\nDrop any NA values in the n_movies column while it is created.\nTransform the decade column data type to integer.\n\n\n\nE3.R\n\nbond_df %>% \n  # Pivot the data to long format and set the column names\n  pivot_longer(\n    -Bond, \n    names_to = \"decade\", \n    values_to = \"n_movies\"\n  )\n \n \n  \nbond_df %>% \n  # Pivot the data to long format\n  pivot_longer(\n    -Bond, \n    # Overwrite the names of the two newly created columns\n    names_to = \"decade\", \n    values_to = \"n_movies\", \n    # Drop na values\n    values_drop_na = TRUE\n  )\n\n\nbond_df %>% \n  # Pivot the data to long format\n  pivot_longer(\n    -Bond, \n    # Overwrite the names of the two newly created columns\n    names_to = \"decade\", \n    values_to = \"n_movies\", \n    # Drop na values\n    values_drop_na = TRUE, \n    # Transform the decade column data type to integer\n    names_transform = list(decade = as.integer)\n  ) %>% \n  ggplot(aes(x = decade + 5, y = n_movies, fill = Bond))+\n  geom_col()"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section-3",
    "href": "Reshaping_Data_with_tidyr_C2.html#section-3",
    "title": "29  Tidy Data",
    "section": "29.4 ",
    "text": "29.4"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-3",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-3",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPivot bird_df to longer format so that an integer column points and a character column species are created. Use the names_prefix argument to clean up the points column and make sure no NA values remain.\nCalculate the total_points each species got.\n\n\n\nE4.R\n\nbird_df %>%\n  # Pivot the data to create a two column data frame\n  pivot_longer(\n    starts_with(\"points_\"),\n    names_to = \"points\",\n    names_prefix = \"points_\",\n    names_transform = list(points = as.integer),\n    values_to = \"species\",\n    values_drop_na = TRUE\n  )\n\n\nbird_df %>%\n  # Pivot the data to create a 2 column data frame\n  pivot_longer(\n    starts_with(\"points_\"),\n    names_to = \"points\",\n    names_prefix = \"points_\",\n    names_transform = list(points = as.integer),\n    values_to = \"species\",\n    values_drop_na = TRUE\n  ) %>%\n  group_by(species) %>% \n  summarize(total_points = sum(points)) %>% \n  slice_max(total_points, n = 5)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section-4",
    "href": "Reshaping_Data_with_tidyr_C2.html#section-4",
    "title": "29  Tidy Data",
    "section": "29.5 ",
    "text": "29.5"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-4",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-4",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPivot stock_df so that the integer columns year and week are created from the column names and the original values are moved to the price column. Use the names_sep argument to separate the column names.\nCreate a line plot where the price is shown per week and color by company. The year variable has been dealt with for you.\n\n\n\nE5.R\n\nstock_df %>% \n  # Pivot the data to create 3 new columns: year, week, price\n  pivot_longer(\n    -company,\n    names_to = c(\"year\", \"week\"),\n    values_to = \"price\",\n    names_sep = \"_week\",\n    names_transform = list(\n      year = as.integer,\n      week = as.integer)\n  )\n\n\nstock_df %>% \n  # Pivot the data to create 3 new columns: year, week, price\n  pivot_longer(\n    -company,\n    names_to = c(\"year\", \"week\"),\n    values_to = \"price\",\n    names_sep = \"_week\",\n    names_transform = list(\n      year = as.integer,\n      week = as.integer)\n  ) %>%\n  # Create a line plot with price per week, color by company\n  ggplot(aes(x = week, y = price, color = company)) +\n  geom_line() +\n  facet_grid(. ~ year)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section-5",
    "href": "Reshaping_Data_with_tidyr_C2.html#section-5",
    "title": "29  Tidy Data",
    "section": "29.6 ",
    "text": "29.6"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-5",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-5",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAs the first argument to pivot_longer(), pass the columns to pivot (name_1, name_2, gender_1, and gender_2).\nComplete the names_to argument so that the first part of the column headers are reused.\nMake sure NA values are dropped since not all rockets had two dogs.\n\n\n\nE6.R\n\nspace_dogs_df %>% \n  pivot_longer(\n    # Add the columns to pivot\n    name_1:gender_2,\n    names_sep = \"_\",\n    # Complete the names_to argument to re-use the first part of the column headers\n    names_to = c(\".value\", \"dog_id\"),\n    # Make sure NA values are dropped\n    values_drop_na = TRUE\n  )"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section-6",
    "href": "Reshaping_Data_with_tidyr_C2.html#section-6",
    "title": "29  Tidy Data",
    "section": "29.7 ",
    "text": "29.7"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-6",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-6",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPivot the data so that each variable (year, sex, pct.obese, life.exp) has a column of the correct data type.\nUse ggplot() to create a scatterplot with life.exp over pct.obese. Color the points by sex.\n\n\n\nE7.R\n\nwho_df %>% \n  # Put each variable in its own column\n  pivot_longer(\n    -country,\n    names_to = c(\"year\", \"sex\", \".value\"),\n    names_sep = \"_\", \n    names_transform = list(\"year\" = as.integer)\n  )\n\n\nwho_df %>% \n  # Put each variable in its own column\n  pivot_longer(\n    -country,\n    names_to = c(\"year\", \"sex\", \".value\"),\n    names_sep = \"_\", \n    names_transform = list(\"year\" = as.integer)\n  ) %>%\n  # Create a plot with life expectancy over obesity\n  ggplot(aes(x = pct.obese, y = life.exp, color = sex))+\n  geom_point()"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section-7",
    "href": "Reshaping_Data_with_tidyr_C2.html#section-7",
    "title": "29  Tidy Data",
    "section": "29.8 ",
    "text": "29.8"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-7",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-7",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nInspect the data in the console.\nUncount the data so that per breed, each dog gets a row and an ID. The ID should go in the dog_id column.\n\n\n\nE8.R\n\ndog_df %>% \n  # Create one row for each participant and add the id\n  uncount(n_participants, .id = \"dog_id\")"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section-8",
    "href": "Reshaping_Data_with_tidyr_C2.html#section-8",
    "title": "29  Tidy Data",
    "section": "29.9 ",
    "text": "29.9"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-8",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-8",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPivot the data to a wider format, deriving new column names from the dog_id column and values from the gender column.\nDrop rows that contain NA values.\nCreate a new column same_gender, which has a TRUE value when gender_1 equals gender_2.\n\n\n\nE9.R\n\nspace_dogs_df %>% \n  # Pivot the data to a wider format\n  pivot_wider(names_from = dog_id, values_from = gender, names_prefix = 'gender_')\n\n\n\nspace_dogs_df %>% \n  # Pivot the data to a wider format\n  pivot_wider(names_from = dog_id, values_from = gender, names_prefix = \"gender_\") %>% \n  # Drop rows with NA values\n  drop_na()\n\n\nspace_dogs_df %>% \n  # Pivot the data to a wider format\n  pivot_wider(names_from = dog_id, values_from = gender, names_prefix = \"gender_\") %>% \n  # Drop rows with NA values\n  drop_na() %>% \n  # Create a Boolean column on whether both dogs have the same gender\n  mutate(same_gender = if_else(gender_1 == gender_2, TRUE, FALSE)) %>% \n  summarize(pct_same_gender = mean(same_gender))"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section-9",
    "href": "Reshaping_Data_with_tidyr_C2.html#section-9",
    "title": "29  Tidy Data",
    "section": "29.10 ",
    "text": "29.10"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-9",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-9",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the pivot_wider() function to extract column names from the metric column and values from the value column.\nUse the ggplot() function to create a plot with the temperature over the distance_to_sun.\n\n\n\nE10.R\n\nplanet_df %>% \n  # Give each planet variable its own column\n   pivot_wider(names_from = \"metric\", values_from = \"value\")\n\n\n\nplanet_df %>% \n  # Give each planet variable its own column\n  pivot_wider(names_from = \"metric\", values_from = \"value\") %>% \n  # Plot planet temperature over distance to sun\n  ggplot(aes(x = distance_to_sun, y = temperature)) +\n  geom_point(aes(size = diameter)) +\n  geom_text(aes(label = planet), vjust = -1) +\n  labs(x = \"Distance to sun (million km)\", \n       y = \"Mean temperature (°C)\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#section-10",
    "href": "Reshaping_Data_with_tidyr_C2.html#section-10",
    "title": "29  Tidy Data",
    "section": "29.11 ",
    "text": "29.11"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-10",
    "href": "Reshaping_Data_with_tidyr_C2.html#instructions-100-xp-10",
    "title": "29  From Wide to Long and Back",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPivot the data so that planet names are put in a column named planet.\nPivot the data so that each variable in the metric column gets its own column.\nUse the ggplot() function to create a plot with the number_of_moons over diameter.\n\n\n\nE11.R\n\nplanet_df %>%\n  # Pivot all columns except metric to long format\n    pivot_longer(-metric, names_to = \"planet\")\n  \n\n\nplanet_df %>%\n  # Pivot all columns except metric to long format\n  pivot_longer(-metric, names_to = \"planet\") %>% \n  # Put each metric in its own column\n  pivot_wider(names_from = metric, values_from = value)\n\n\n\nplanet_df %>%\n  # Pivot all columns except metric to long format\n  pivot_longer(-metric, names_to = \"planet\") %>% \n  # Put each metric in its own column\n  pivot_wider(names_from = metric, values_from = value) %>% \n  # Plot the number of moons vs. planet diameter\n  ggplot(aes(x = diameter, y = number_of_moons)) +\n  geom_point(aes(size = diameter)) +\n  geom_text(aes(label = planet), vjust = -1) +\n  labs(x = \"Diameter (km)\", y = \"Number of moons\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section",
    "href": "Reshaping_Data_with_tidyr_C3.html#section",
    "title": "30  Tidy Data",
    "section": "30.1 ",
    "text": "30.1"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp",
    "title": "30  Expanding Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a tibble with three columns called letter1, letter2, and letter3 that holds all possible combinations of the vector letters using expand_grid().\nUse the unite() function from chapter one to merge these three columns into a single column named codon. Use an empty string as the separator.\n\n\n\nE1.R\n\nletters <- c(\"A\", \"C\", \"G\", \"U\")\n\n# Create a tibble with all possible 3 way combinations\ncodon_df <- expand_grid(\n  letter1 = letters,\n  letter2 = letters,\n  letter3 = letters\n)\n\ncodon_df\n\n\n\nletters <- c(\"A\", \"C\", \"G\", \"U\")\n\n# Create a tibble with all possible 3 way combinations\ncodon_df <- expand_grid(\n  letter1 = letters,\n  letter2 = letters,\n  letter3 = letters\n)\n\ncodon_df %>%  \n  # Unite these three columns into a \"codon\" column\n  unite(\"codon\", letter1:letter3, sep= \"\")"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section-1",
    "href": "Reshaping_Data_with_tidyr_C3.html#section-1",
    "title": "30  Tidy Data",
    "section": "30.2 ",
    "text": "30.2"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-1",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-1",
    "title": "30  Expanding Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate full_df, a tibble with all unique combinations of the variables year (from 1951 to 1970) and species (“Human” and “Dog”).\nPerform a right_join() between space_df and full_df on the year and species columns.\nUse the ggplot() function to create a line plot of n_in_space over year, colored by species.\nUse the replace_na() function to overwrite NA values in the n_in_space column with zeros.\n\n\n\nE2.R\n\n# Create a tibble with all combinations of years and species\nfull_df <- expand_grid(\n    year = 1951:1970,\n    species = c(\n        \"Human\",\n        \"Dog\" )\n)\n\nfull_df\n\n\n\n\n# Create a tibble with all combinations of years and species\nfull_df <- expand_grid(\n  year = 1951:1970, \n  species = c(\"Human\", \"Dog\")\n)\n\nspace_df %>% \n  # Join with full_df so that missing values are introduced\n  right_join(full_df, by = c(\"year\", \"species\")) %>% \n  arrange(year)\n\n\n\n# Create a tibble with all combinations of years and species\nfull_df <- expand_grid(\n  year = 1951:1970, \n  species = c(\"Human\", \"Dog\")\n)\n\nspace_df %>% \n  # Join with full_df so that missing values are introduced\n  right_join(full_df, by = c(\"year\", \"species\")) %>% \n  # Create a line plot with n_in_space over year, color by species\n  ggplot(aes(x = year, y = n_in_space, color = species)) +\n  geom_line()\n  \n  \n# Create a tibble with all combinations of years and species\nfull_df <- expand_grid(\n  year = 1951:1970, \n  species = c(\"Human\", \"Dog\")\n)\n\nspace_df %>% \n  # Join with full_df so that missing values are introduced\n  right_join(full_df, by = c(\"year\", \"species\")) %>% \n  # Overwrite NA values for n_in_space with 0L\n  replace_na(list(n_in_space = 0L)) %>% \n  # Create a line plot with n_in_space over year, color by species\n  ggplot(aes(x = year, y = n_in_space, color = species)) +\n  geom_line()"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section-2",
    "href": "Reshaping_Data_with_tidyr_C3.html#section-2",
    "title": "30  Tidy Data",
    "section": "30.3 ",
    "text": "30.3"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-2",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-2",
    "title": "30  Expanding Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the expand_grid() function to create a tibble holding all combinations of the variables date and reactor. Use the dates and reactors vectors created for you.\nPerform an anti-join between full_df and reactor_df on the date and reactor columns.\n\n\n\nE3.R\n\n# Create a tibble with all combinations of dates and reactors\nfull_df <- expand_grid(date = dates, reactor = reactors)\n\n# Find the reactor - date combinations not present in reactor_df\nfull_df %>% \n  anti_join(reactor_df, by = c(\"date\", \"reactor\"))"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section-3",
    "href": "Reshaping_Data_with_tidyr_C3.html#section-3",
    "title": "30  Tidy Data",
    "section": "30.4 ",
    "text": "30.4"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-3",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-3",
    "title": "30  Expanding Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nComplete the planet variable using the planets vector.\nReplace NA values in the n_moons variable with 0L values.\n\n\n\nE4.R\n\nplanets = c(\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\")\n\nplanet_df %>% \n  complete(\n    # Complete the planet variable\n    planet = planets,\n    # Overwrite NA values for n_moons with 0L\n    fill = list(n_moons = 0L)\n  )"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section-4",
    "href": "Reshaping_Data_with_tidyr_C3.html#section-4",
    "title": "30  Tidy Data",
    "section": "30.5 ",
    "text": "30.5"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-4",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-4",
    "title": "30  Expanding Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCount the number of medals won per team and year.\nUse ggplot() to create a line plot with n_medals over year, colored by team.\nComplete the team and year variables, replace NA values in the n_medals column with zeros.\n\n\n\nE5.R\n\nmedal_df %>% \n  # Count the medals won per team and year\n    count(team, year, name = \"n_medals\")\n\n\n\nmedal_df %>% \n  # Count the medals won per team and year\n  count(team, year, name = \"n_medals\") %>% \n  # Plot n_medals over year, colored by team\n  ggplot(aes(x = year, y = n_medals, color = team)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Paired\")\n  \n  \nmedal_df %>% \n  # Count the medals won per team and year\n  count(team, year, name = \"n_medals\") %>% \n  # Complete the team and year variables, fill n_medals with zeros\n  complete(team, year, fill = list(n_medals = 0)) %>% \n  # Plot n_medals over year, colored by team\n  ggplot(aes(x = year, y = n_medals, color = team)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Paired\")"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section-5",
    "href": "Reshaping_Data_with_tidyr_C3.html#section-5",
    "title": "30  Tidy Data",
    "section": "30.6 ",
    "text": "30.6"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-5",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-5",
    "title": "30  Expanding Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse full_seq() to create a sequence with all years from 2020 till 2030.\nUse full_seq() to create a sequence with all decades from 1980 till 2030.\nUse full_seq() to create a sequence with all dates in 1980 using the outer_dates vector.\n\n\n\nE6.R\n\n# Generate all years from 2020 to 2030\nyears <- full_seq(c(2020, 2030), period = 1)\nyears\n\n\n\n# Generate all decades from 1980 to 2030\ndecades <- full_seq(c(1980, 2030), period = 10)\ndecades\n\n\nouter_dates <- c(as.Date(\"1980-01-01\"), as.Date(\"1980-12-31\"))\n\n# Generate the dates for all days in 1980\nfull_seq(outer_dates, period = 1)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section-6",
    "href": "Reshaping_Data_with_tidyr_C3.html#section-6",
    "title": "30  Tidy Data",
    "section": "30.7 ",
    "text": "30.7"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-6",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-6",
    "title": "30  Expanding Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nComplete the dataset so that for each country there is an observation of each date using the full_seq() function.\nGroup the data by country.\nUse the fill() function to overwrite NA values in the total_bombs variable with the last known value.\nUse ggplot() to visualize the total_bombs at any given date, color the line plot by country. Some code has been added for you to visualize the Cuban Missile Crisis.\n\n\n\nE7.R\n\ncumul_nukes_1962_df %>% \n  # Complete the dataset\n  complete(country, date = full_seq(date, period = 1))\n  \n\n\ncumul_nukes_1962_df %>% \n  # Complete the dataset\n  complete(country, date = full_seq(date, period = 1)) %>% \n  # Group the data by country\n  group_by(country) %>% \n  # Impute missing values with the last known observation\n  fill(total_bombs)\n  \n  \ncumul_nukes_1962_df %>% \n  # Complete the dataset\n  complete(country, date = full_seq(date, period = 1)) %>% \n  # Group the data by country\n  group_by(country) %>% \n  # Impute missing values with the last known observation\n  fill(total_bombs) %>% \n  # Plot the number of bombs over time, color by country\n  ggplot(aes(x= date, y = total_bombs, color = country )) +\n  # These two lines will mark the Cuban Missile Crisis \n  geom_rect(xmin = as.Date(\"1962-10-16\"), xmax = as.Date(\"1962-10-29\"), ymin = -Inf, ymax = Inf, color = NA)+ \n  geom_text(x = as.Date(\"1962-10-22\"), y = 15, label = \"Cuban Missile Crisis\", angle = 90, color = \"white\")+\n  geom_line()"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section-7",
    "href": "Reshaping_Data_with_tidyr_C3.html#section-7",
    "title": "30  Tidy Data",
    "section": "30.8 ",
    "text": "30.8"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-7",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-7",
    "title": "30  Expanding Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nComplete the dataset so that each continent has a medals_per_participant value at each Olympic event. Missing values should be filled with zeros.\nNest the season and year variables using the nesting() function, since the summer and winter Olympics don’t occur in the same years.\nUse ggplot() to create a line plot with the medals_per_participant per year, color the plot by continent.\n\n\n\nE8.R\n\nmedal_df %>% \n  # Give each continent an observation at each Olympic event\n  complete(\n    continent, \n    nesting(season, year), \n    fill = list(medals_per_participant = 0)\n  ) %>%\n  # Plot the medals_per_participant over time, colored by continent\n  ggplot(aes(x = year, y = medals_per_participant, color = continent)) +\n  geom_line() +\n  facet_grid(season ~ .)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section-8",
    "href": "Reshaping_Data_with_tidyr_C3.html#section-8",
    "title": "30  Tidy Data",
    "section": "30.9 ",
    "text": "30.9"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-8",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-8",
    "title": "30  Expanding Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPivot the infected and recovered columns to long format, the old column names should go in the status variable, the values to date.\nGroup the data by patient and then complete the date column so that each date between infection and recovery is added using the full_seq() column. At the end, ungroup the data.\nEach date is now a day on which a patient was sick, count the dates and name the new variable n_sick.\n\n\n\nE9.R\n\npatient_df %>% \n  # Pivot the infected and recovered columns to long format\n  pivot_longer(-patient, names_to = \"status\", values_to = \"date\")\n\n\npatient_df %>% \n  # Pivot the infected and recovered columns to long format\n  pivot_longer(-patient, names_to = \"status\", values_to = \"date\") %>% \n  select(-status) %>% \n  # Group by patient\n  group_by(patient) %>% \n  # Complete the date range per patient using full_seq()\n  complete(date = full_seq(date, period = 1)) %>% \n  # Ungroup the data\n  ungroup()\n  \n\npatient_df %>% \n  # Pivot the infected and recovered columns to long format\n  pivot_longer(-patient, names_to = \"status\", values_to = \"date\") %>% \n  select(-status) %>% \n  # Group by patient\n  group_by(patient) %>% \n  # Complete the date range per patient using full_seq()\n  complete(date = full_seq(date, period = 1)) %>% \n  # Ungroup the data\n  ungroup() %>% \n  # Count the dates, the count goes in the n_sick variable\n  count(date, name = \"n_sick\") %>% \n  ggplot(aes(x = date, y = n_sick))+\n  geom_line()"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section-9",
    "href": "Reshaping_Data_with_tidyr_C3.html#section-9",
    "title": "30  Tidy Data",
    "section": "30.10 ",
    "text": "30.10"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-9",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-9",
    "title": "30  Expanding Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nComplete the time variable by using the seq() function to create a sequence between the min and max values with an interval of “20 min”. Fill NA values of enter and exit with 0L.\nCalculate the total_inside variable by taking the cumulative sum of enter plus exit using the cumsum() function.\nPivot the enter and exit columns to long format. The column names should go in the direction variable, the values in n_people.\nUse ggplot() to visualize the n_people in the building over time. Use the fill argument to color the area plot by direction.\n\n\n\nE10.R\n\nsensor_df %>% \n  # Complete the time column with a 20 minute interval\n  complete(time = seq(min(time), max(time), by = \"20 min\"),\n           fill = list(enter = 0L, exit = 0L))\n\n\n\nsensor_df %>% \n  # Complete the time column with a 20 minute interval\n  complete(time = seq(min(time), max(time), by = \"20 min\"),\n           fill = list(enter = 0L, exit = 0L)) %>%\n  # Calculate the total number of people inside\n  mutate(total_inside = cumsum(enter + exit))\n\n\nsensor_df %>% \n  # Complete the time column with a 20 minute interval\n  complete(time = seq(min(time), max(time), by = \"20 min\"),\n           fill = list(enter = 0L, exit = 0L)) %>%\n  # Calculate the total number of people inside\n  mutate(total_inside = cumsum(enter + exit)) %>% \n  # Pivot the enter and exit columns to long format\n  pivot_longer(enter:exit, names_to = \"direction\", values_to = \"n_people\")\n  \n\nsensor_df %>% \n  # Complete the time column with a 20 minute interval\n  complete(time = seq(min(time), max(time), by = \"20 min\"),\n           fill = list(enter = 0L, exit = 0L)) %>%\n  # Calculate the total number of people inside\n  mutate(total_inside = cumsum(enter + exit)) %>% \n  # Pivot the enter and exit columns to long format\n  pivot_longer(enter:exit, names_to = \"direction\", values_to = \"n_people\") %>% \n  # Plot the number of people over time, fill by direction\n  ggplot(aes(x = time, y = n_people, fill = direction)) +\n  geom_area() +\n  geom_line(aes(y = total_inside))"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#section-10",
    "href": "Reshaping_Data_with_tidyr_C3.html#section-10",
    "title": "30  Tidy Data",
    "section": "30.11 ",
    "text": "30.11"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-10",
    "href": "Reshaping_Data_with_tidyr_C3.html#instructions-100-xp-10",
    "title": "30  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE11.R"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section",
    "href": "Reshaping_Data_with_tidyr_C4.html#section",
    "title": "31  Tidy Data",
    "section": "31.1 ",
    "text": "31.1"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp",
    "title": "31  Rectangling Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a tibble with a single column called movie out of the input movie_list.\nWiden the dataset by unnesting the movie column over multiple columns.\nRe-create the tibble with a single column called movie out of movie_planets_list.\nUnnest the planets column to a wider format.\n\n\n\nE1.R\n\n# Create a movie column from the movie_list\ntibble(movie = movie_list)\n\n\n\n# Create a movie column from the movie_list\ntibble(movie = movie_list) %>% \n  # Unnest the movie column\n  unnest_wider(movie)\n\n\n\n# Create a tibble with a movie column\ntibble(movie = movie_planets_list) %>% \n  # Unnest the movie column\n  unnest_wider(movie)\n  \n\n\n# Create a tibble with a movie column\ntibble(movie = movie_planets_list) %>% \n  # Unnest the movie column\n  unnest_wider(movie) %>% \n  # Unnest the planets column\n  unnest_wider(planets)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section-1",
    "href": "Reshaping_Data_with_tidyr_C4.html#section-1",
    "title": "31  Tidy Data",
    "section": "31.2 ",
    "text": "31.2"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-1",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-1",
    "title": "31  Rectangling Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a tibble with a single column called movie out of movie_planets_list.\nUnnest the movie list column which contains named lists of equal length.\nUnnest the planets list column which contains unnamed lists of unequal length.\n\n\n\nE2.R\n\n# Create a tibble from movie_planets_list\ntibble(movie = movie_planets_list)\n\n\n# Create a tibble from movie_planets_list\ntibble(movie = movie_planets_list) %>% \n  # Unnest the movie column in the correct direction\n  unnest_wider(movie)\n  \n\n# Create a tibble from movie_planets_list\ntibble(movie = movie_planets_list) %>% \n  # Unnest the movie column in the correct direction\n  unnest_wider(movie) %>% \n  # Unnest the planets column in the correct direction\n  unnest_longer(planets)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section-2",
    "href": "Reshaping_Data_with_tidyr_C4.html#section-2",
    "title": "31  Tidy Data",
    "section": "31.3 ",
    "text": "31.3"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-2",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-2",
    "title": "31  Rectangling Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUnnest the moons column so that each moon gets an observation.\nUnnest the moons column so that its contents are split over columns.\nUnnest the moon_data column so that its contents are split over columns.\nUse dplyr’s slice_max() function on moon radius to get a top 5 of biggest moons.\n\n\n\nE3.R\n\nplanet_df %>% \n  # Unnest the moons list column over observations\n  unnest_longer(moons)\n\n\n\nplanet_df %>% \n  # Unnest the moons list column over observations\n  unnest_longer(moons) %>% \n  # Further unnest the moons column\n  unnest_wider(moons)\n\n\nplanet_df %>% \n  # Unnest the moons list column over observations\n  unnest_longer(moons) %>% \n  # Further unnest the moons column\n  unnest_wider(moons) %>% \n  # Unnest the moon_data column\n  unnest_wider(moon_data)\n\n\n\nplanet_df %>% \n  # Unnest the moons list column over observations\n  unnest_longer(moons) %>% \n  # Further unnest the moons column\n  unnest_wider(moons) %>% \n  # Unnest the moon_data column\n  unnest_wider(moon_data) %>% \n  # Get the top five largest moons by radius\n  slice_max(radius, n = 5)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section-3",
    "href": "Reshaping_Data_with_tidyr_C4.html#section-3",
    "title": "31  Tidy Data",
    "section": "31.4 ",
    "text": "31.4"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-3",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-3",
    "title": "31  Rectangling Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUnnest the metadata column.\nUnnest the films column.\nAs an alternative approach, use hoist() to select the first film from the films list nested in the metadata column.\n\n\n\nE4.R\n\ncharacter_df %>%\n  # Unnest the metadata column\n  unnest_wider(metadata)\n\n\n\ncharacter_df %>% \n  # Unnest the metadata column\n  unnest_wider(metadata) %>% \n  unnest_longer(films)\n\n\n\ncharacter_df %>% \n  hoist(metadata, first_film = list(\"films\", 1))"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section-4",
    "href": "Reshaping_Data_with_tidyr_C4.html#section-4",
    "title": "31  Tidy Data",
    "section": "31.5 ",
    "text": "31.5"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-4",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-4",
    "title": "31  Rectangling Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUnnest the movie column.\nUnnest the Ratings column.\nUse hoist() on the movie column to extract the Title, Year, and Rotten Tomatoes rating. Note that this rating is nested inside the Ratings column.\n\n\n\nE5.R\n\nmovie_df %>% \n  # Unnest the movie column\n    unnest_wider(movie)\n\n\n\nmovie_df %>% \n  # Unnest the movie column\n  unnest_wider(movie) %>% \n  select(Title, Year, Ratings) %>% \n  # Unnest the Ratings column\n  unnest_wider(Ratings)\n  \nmovie_df %>% \n  hoist(\n    movie,\n    title = \"Title\",\n    year = \"Year\",\n    rating = list(\"Ratings\", \"Rotten Tomatoes\")\n  )"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section-5",
    "href": "Reshaping_Data_with_tidyr_C4.html#section-5",
    "title": "31  Tidy Data",
    "section": "31.6 ",
    "text": "31.6"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-5",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-5",
    "title": "31  Rectangling Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGroup the data by army branch and then nest it.\nGroup the data by both branch and sex, then nest it.\n\n\n\nE6.R\n\nansur_df %>% \n  # Group the data by branch, then nest\n  group_by(branch) %>% \n  nest()\n\n\n\nansur_df %>% \n  # Group the data by branch and sex, then nest\n   group_by(branch,sex) %>% \n  nest()"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section-6",
    "href": "Reshaping_Data_with_tidyr_C4.html#section-6",
    "title": "31  Tidy Data",
    "section": "31.7 ",
    "text": "31.7"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-6",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-6",
    "title": "31  Rectangling Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGroup the data by sex.\nNest the data.\nUnnest the glanced column.\n\n\n\nE7.R\n\nansur_df %>%\n  # Group the data by sex\n  group_by(sex) %>% \n  # Nest the data\n  nest() %>%\n  mutate(\n    fit = map(data, function(df) lm(weight_kg ~ waist_circum_m + stature_m, data = df)),\n    glanced = map(fit, glance)\n  ) %>% \n  # Unnest the glanced column\n  unnest(glanced)"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section-7",
    "href": "Reshaping_Data_with_tidyr_C4.html#section-7",
    "title": "31  Tidy Data",
    "section": "31.8 ",
    "text": "31.8"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-7",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-7",
    "title": "31  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE8.R"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section-8",
    "href": "Reshaping_Data_with_tidyr_C4.html#section-8",
    "title": "31  Tidy Data",
    "section": "31.9 ",
    "text": "31.9"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-8",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-8",
    "title": "31  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE9.R"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section-9",
    "href": "Reshaping_Data_with_tidyr_C4.html#section-9",
    "title": "31  Tidy Data",
    "section": "31.10 ",
    "text": "31.10"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-9",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-9",
    "title": "31  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE10.R"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#section-10",
    "href": "Reshaping_Data_with_tidyr_C4.html#section-10",
    "title": "31  Tidy Data",
    "section": "31.11 ",
    "text": "31.11"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-10",
    "href": "Reshaping_Data_with_tidyr_C4.html#instructions-100-xp-10",
    "title": "31  Tidy Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE11.R"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#nuclear-bombs-per-country",
    "href": "Reshaping_Data_with_tidyr_C2.html#nuclear-bombs-per-country",
    "title": "29  From Wide to Long and Back",
    "section": "29.1 Nuclear bombs per country",
    "text": "29.1 Nuclear bombs per country\nYou’ve been given a version of the Nuclear Explosion DataBase (NEDB) where country names are specified in the column headers (nuke_df). You want to visualize how many nukes were detonated per year per country. You’ll need to pivot the data and replace NA values first.\nThe ggplot2 package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#who-obesity-per-country",
    "href": "Reshaping_Data_with_tidyr_C2.html#who-obesity-per-country",
    "title": "29  From Wide to Long and Back",
    "section": "29.2 WHO obesity per country",
    "text": "29.2 WHO obesity per country\nAccording to the World Health Organization (WHO), worldwide obesity has nearly tripled since 1975. You’re interested in the severity of this global health issue per country and whether males and females are affected differently. You’ll use the WHO’s obesity data (obesity_df) to investigate this issue. The data holds the percentage of females, males, and both sexes combined that are considered obese (BMI > 30) per country.\nYou want to create a scatterplot where, per nation, you can see the obesity data colored differently for females and males. This implies that sex should become a variable with its own column.\nThe ggplot2 package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#bond-james-bond",
    "href": "Reshaping_Data_with_tidyr_C2.html#bond-james-bond",
    "title": "29  From Wide to Long and Back",
    "section": "29.3 Bond… James Bond",
    "text": "29.3 Bond… James Bond\nYou’ve been given a James Bond movie dataset (bond_df) and want to visualize the number of movies that Bond actors have featured in per decade. However, the data is in an untidy format with the decade values captured in the column headers. You’ll tidy this dataset to give each variable its own column.\nThe ggplot2 package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#new-zealands-bird-of-the-year",
    "href": "Reshaping_Data_with_tidyr_C2.html#new-zealands-bird-of-the-year",
    "title": "29  From Wide to Long and Back",
    "section": "29.4 New-Zealand’s bird of the year",
    "text": "29.4 New-Zealand’s bird of the year\nEvery year New Zealanders vote en masse to decide which species gets the bird of the year trophy. The contest is organized by the Forest & Bird agency which allows each person to give points to up to five birds (first pick gets 5 points, second 4, …). Your job is to decide this year’s winner from the messy dataset that’s been pre-loaded for you as bird_df.\nThe dplyr package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#big-tech-stock-prices",
    "href": "Reshaping_Data_with_tidyr_C2.html#big-tech-stock-prices",
    "title": "29  From Wide to Long and Back",
    "section": "29.5 Big tech stock prices",
    "text": "29.5 Big tech stock prices\nYou’re an analyst at an investment firm and want to visualize the weekly closing prices of five big tech firms’ stocks. However, the dataset you’ve been handed (stock_df) is messy and has the year and week variables stored in the column headers. You’ll pivot this data into a tidy format, extract the variables from the headers, and create a line plot.\nThe ggplot2 package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#soviet-space-dogs-the-dog-perspective",
    "href": "Reshaping_Data_with_tidyr_C2.html#soviet-space-dogs-the-dog-perspective",
    "title": "29  From Wide to Long and Back",
    "section": "29.6 Soviet space dogs, the dog perspective",
    "text": "29.6 Soviet space dogs, the dog perspective\nYou’ll be working on an pre-processed sample of the USSR space dogs database compiled by Duncan Geere and pre-loaded for you as space_dogs_df. Each of the 42 rows in this dataset represents a test rocket launch which had one or two very brave dogs on board.\nYour goal is to reshape this dataset so that for each launch, each dog has a row.\nThe challenge is that in the column headers (name_1, name_2, gender_1, and gender_2), the part before the _ separator can point to two different variables (name and gender), while the second part always points to the dog ID (1st or 2nd dog)."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#who-obesity-vs.-life-expectancy",
    "href": "Reshaping_Data_with_tidyr_C2.html#who-obesity-vs.-life-expectancy",
    "title": "29  From Wide to Long and Back",
    "section": "29.7 WHO obesity vs. life expectancy",
    "text": "29.7 WHO obesity vs. life expectancy\nYou’ve been given a sample of WHO data (who_df) with obesity percentages and life expectancy data per country, year, and sex. You want to visually inspect the correlation between obesity and life expectancy.\nHowever, the data is very messy with four variables hidden in the column names. Each column name is made up of three parts separated by underscores: Values for the year, followed by those for sex, and then values for either pct.obese or life.exp. Since the third part of the column name string holds two variables you’ll need to use the special “.value” value in the names_to argument.\nYou’ll pivot the data into a tidy format and create the scatterplot.\nThe ggplot2 package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#uncounting-observations",
    "href": "Reshaping_Data_with_tidyr_C2.html#uncounting-observations",
    "title": "29  From Wide to Long and Back",
    "section": "29.8 Uncounting observations",
    "text": "29.8 Uncounting observations\nYou’ve found the job of your dreams providing technical support for a dog breed beauty contest. The jury members want a spreadsheet with the breed and id of each participating dog so that they can add the scores later on. You’ve only been given the number of participants per dog breed (dog_df) so you decide to use your tidyr skills to create the desired result."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#soviet-space-dogs-the-flight-perspective",
    "href": "Reshaping_Data_with_tidyr_C2.html#soviet-space-dogs-the-flight-perspective",
    "title": "29  From Wide to Long and Back",
    "section": "29.9 Soviet space dogs, the flight perspective",
    "text": "29.9 Soviet space dogs, the flight perspective\nRemember the USSR space dogs dataset1? You changed it to a long format so that for every dog in every rocket launch, there was a row. Suppose you’re given this tidy dataset and are asked to answer the question, “In what percentage of flights were both dogs of the same gender?”\nYou’ll reshape and investigate space_dogs_df to find the answer.\nThe dplyr package has been pre-loaded for you.\n1 Compiled by Duncan Geere."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#planet-temperature-distance-to-the-sun",
    "href": "Reshaping_Data_with_tidyr_C2.html#planet-temperature-distance-to-the-sun",
    "title": "29  From Wide to Long and Back",
    "section": "29.10 Planet temperature & distance to the Sun",
    "text": "29.10 Planet temperature & distance to the Sun\nThe intensity of light radiated by a light source follows an inverse square relationship with the distance it has traveled.\nYou wonder if you could observe this trend in the temperature of the planets in our Solar System given their distance to the Sun. You’ll use the planet_df dataset from the devstronomy project to investigate this."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C2.html#transposing-planet-data",
    "href": "Reshaping_Data_with_tidyr_C2.html#transposing-planet-data",
    "title": "29  From Wide to Long and Back",
    "section": "29.11 Transposing planet data",
    "text": "29.11 Transposing planet data\nYou’re again working on a planet dataset derived from the devstronomy project. This time, you’re interested in the correlation between the diameter of a planet and the number of moons circling it.\nHowever, the dataset (planet_df) has a row for each variable and a column for each planet (observation). You’ll transpose this data in two steps and then create a plot to inspect the correlation.\nThe ggplot2 package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#letters-of-the-genetic-code",
    "href": "Reshaping_Data_with_tidyr_C3.html#letters-of-the-genetic-code",
    "title": "30  Expanding Data",
    "section": "30.1 Letters of the genetic code",
    "text": "30.1 Letters of the genetic code\nThe basic building blocks of RNA are four molecules described by a single letter each: adenine (A), cytosine (C), guanine (G), and uracil (U). The information carried by an RNA strand can be represented as a long sequence of these four letters. To read this code, one has to divide this chain into sequences of three letters each (e.g. GCU, ACG, …). These three letter sequences are known as codons. The concept is illustrated in the image below.\nYour goal for this exercise is to create a data frame with all possible three letter sequences (codons) from a vector with the four letters representing the RNA building blocks."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#when-did-humans-replace-dogs-in-space",
    "href": "Reshaping_Data_with_tidyr_C3.html#when-did-humans-replace-dogs-in-space",
    "title": "30  Expanding Data",
    "section": "30.2 When did humans replace dogs in space?",
    "text": "30.2 When did humans replace dogs in space?\nYou already know that in the early days of spaceflight, the USSR was testing rockets with dogs. You now wonder when exactly humans started replacing dogs on space flight missions. You’ve been given a dataset space_df with the number of both dogs (compiled by Duncan Geere) and humans in space per year from 1951 till 1970 (collected from Wikipedia).\nYour goal is to create a plot that shows you the number of individuals sent into space per species. Before you can create this plot, you’ll first have to introduce zero values for missing combinations of year and species.\nThe dplyr and ggplot2 packages have been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#finding-missing-observations",
    "href": "Reshaping_Data_with_tidyr_C3.html#finding-missing-observations",
    "title": "30  Expanding Data",
    "section": "30.3 Finding missing observations",
    "text": "30.3 Finding missing observations\nYou’re an inspector at a nuclear plant and have to validate whether every reactor has received its daily safety check over the course of a full year. The safety check logs are in reactor_df, a data frame with columns date, reactor, and check.\nTwo vectors, dates and reactors, with all dates of the year and reactors at the plant respectively have been created for you. You’ll use the combination of the expand_grid() and anti_join() functions to find dates where particular reactors were not checked.\nThe dplyr package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#completing-the-solar-system",
    "href": "Reshaping_Data_with_tidyr_C3.html#completing-the-solar-system",
    "title": "30  Expanding Data",
    "section": "30.4 Completing the Solar System",
    "text": "30.4 Completing the Solar System\nYou have been given a data frame (planet_df) from the devstronomy project with the number of moons per planet in our Solar System. However, Mercury and Venus, the two moonless planets, are absent. You want to expand this dataset using the complete() function and a vector planets that contains all eight planet’s names."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#zero-olympic-medals",
    "href": "Reshaping_Data_with_tidyr_C3.html#zero-olympic-medals",
    "title": "30  Expanding Data",
    "section": "30.5 Zero Olympic medals",
    "text": "30.5 Zero Olympic medals\nSince 1896, athletes from all over the world have been competing in the modern Olympic games. You’ve been given a dataset (medal_df) with observations for all medals won by athletes from the 10 most successful countries in Olympic history. You want to create a visual with the number of medals won per country (team) per year. However, since not all countries won medals each year, you’ll have to introduce zero values before you can make an accurate visual.\nOlympic flag\nThe ggplot2 and dplyr packages have been pre-loaded for you. In step 2 and 3 the scale_color_brewer() function is used to color lines in the plot with a palette that makes it easier to distinguish the different countries."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#creating-a-sequence-with-full_seq",
    "href": "Reshaping_Data_with_tidyr_C3.html#creating-a-sequence-with-full_seq",
    "title": "30  Expanding Data",
    "section": "30.6 Creating a sequence with full_seq()",
    "text": "30.6 Creating a sequence with full_seq()\nThe full_seq() function will look for the minimal and maximal values inside the vector you pass it and will then generate a full sequence of numbers with a fixed period in between them. When used inside the complete() function, full_seq() is a handy tool to make sure there are no missing observations in your data. Before combining these two functions you’ll generate a few sequences with full_seq() on its own to get the hang of this function."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#the-cold-wars-hottest-year",
    "href": "Reshaping_Data_with_tidyr_C3.html#the-cold-wars-hottest-year",
    "title": "30  Expanding Data",
    "section": "30.7 The Cold War’s hottest year",
    "text": "30.7 The Cold War’s hottest year\nIn October 1962, during the Cuban missile crisis, the world came close to a full scale nuclear war. Throughout 1962, the USA, USSR, and France together detonated a record 178 nuclear bombs for military power display and research. You’ve been given a sample of the Nuclear Explosion Database (NEDB) for that year (cumul_nukes_1962_df) with an observation for each date on which a bomb was detonated. The total_bombs variable contains the cumulative number of bombs detonated by a country up to that point in time.\nYou’ll complete the dataset to hold the full sequence of dates, and visualize the total number of bombs per country over time. You’ll also use the fill() function from Chapter One to impute missing values.\nThe dplyr and ggplot2 packages have been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#olympic-medals-per-continent",
    "href": "Reshaping_Data_with_tidyr_C3.html#olympic-medals-per-continent",
    "title": "30  Expanding Data",
    "section": "30.8 Olympic medals per continent",
    "text": "30.8 Olympic medals per continent\nYou want to compare Olympic performance of athletes per continent over time, both on the winter and summer Olympics. You’ve been given a dataset medal_df with the average number of medals won per participant of each continent since 1928. You’ll complete this data to introduce zero values for years where a continent did not win any medals.\nOlympic flag\nThe ggplot2 package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#tracking-a-virus-outbreak",
    "href": "Reshaping_Data_with_tidyr_C3.html#tracking-a-virus-outbreak",
    "title": "30  Expanding Data",
    "section": "30.9 Tracking a virus outbreak",
    "text": "30.9 Tracking a virus outbreak\nYou’re a doctor in a remote village confronted with a virus outbreak. You have been collecting data on when your patients got infected and recovered in a data frame named patient_df. Your goal is to create a visual with the number of sick patients over time. You’ll first have to reshape the data so that you can count the number of sick patients per day.\nThe data frame has three columns: patient, infected, and recovered.\nThe dplyr and ggplot2 packages have been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C3.html#counting-office-occupants",
    "href": "Reshaping_Data_with_tidyr_C3.html#counting-office-occupants",
    "title": "30  Expanding Data",
    "section": "30.10 Counting office occupants",
    "text": "30.10 Counting office occupants\nImagine you’re an office facility manager and want to know how many people are present throughout the day. You’ve installed a sensor at the entrance that counts the number of people entering and leaving the building. The sensor sends an update at the end of every 20 minute time slot if at least one person passed.\nTo create a dataset ready for visualization, you’ll combine the different techniques you’ve learned so far.\nThe dplyr and ggplot2 packages have been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#rectangling-star-wars-movies",
    "href": "Reshaping_Data_with_tidyr_C4.html#rectangling-star-wars-movies",
    "title": "31  Rectangling Data",
    "section": "31.1 Rectangling Star Wars movies",
    "text": "31.1 Rectangling Star Wars movies\nLet’s pretend you’re a big Star Wars fan and decided to scrape some data from the Star Wars API. You’ve already loaded the JSON-formatted response into R, and now have two lists of movies named movie_list and movie_planets_list. Your goal is to turn these into rectangular data frames with one row per movie so that you can start crunching those movie stats.\nThe dplyr package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#rectangling-star-wars-planets",
    "href": "Reshaping_Data_with_tidyr_C4.html#rectangling-star-wars-planets",
    "title": "31  Rectangling Data",
    "section": "31.2 Rectangling Star Wars planets",
    "text": "31.2 Rectangling Star Wars planets\nLet’s finish what we started in the last exercise of the previous lesson, exploring Star Wars planets! The movie_planets_list scraped from the Star Wars API has been pre-loaded for you. You’ll need two specific unnesting operations to completely rectangle this data."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#the-solar-systems-biggest-moons",
    "href": "Reshaping_Data_with_tidyr_C4.html#the-solar-systems-biggest-moons",
    "title": "31  Rectangling Data",
    "section": "31.3 The Solar System’s biggest moons",
    "text": "31.3 The Solar System’s biggest moons\nMost planets in our solar system are accompanied by at least one moon. You now wonder which planets are circled by the biggest moons and want to create a top five based on moon radius. However, you’ll first have to unnest the devstronomy project data in planet_df using the unnest_longer() and unnest_wider() functions.\nThe dplyr package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#hoisting-star-wars-films",
    "href": "Reshaping_Data_with_tidyr_C4.html#hoisting-star-wars-films",
    "title": "31  Rectangling Data",
    "section": "31.4 Hoisting Star Wars films",
    "text": "31.4 Hoisting Star Wars films\nYou’ve been given a nested data set on Star Wars characters (character_df) and want to explore the films in which they appeared. You’ll first use the unnest_wider() and unnest_longer() functions to explore the data and will then switch to hoist() to select a specific element in the nested data structure directly."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#hoisting-movie-ratings",
    "href": "Reshaping_Data_with_tidyr_C4.html#hoisting-movie-ratings",
    "title": "31  Rectangling Data",
    "section": "31.5 Hoisting movie ratings",
    "text": "31.5 Hoisting movie ratings\nYou’ve written a script to scrape data on your favorite movies from the Open Movie DataBase API. Now you want to process the JSON data to extract the Rotten Tomatoes rating for each movie. You’ve been given a data frame named movie_df which holds the JSON responses for five movies. You’ll explore this data with unnest_wider() and unnest_longer() before switching to hoist().\nThe dplyr package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#tidy-model-outputs-with-broom",
    "href": "Reshaping_Data_with_tidyr_C4.html#tidy-model-outputs-with-broom",
    "title": "31  Rectangling Data",
    "section": "31.6 Tidy model outputs with broom",
    "text": "31.6 Tidy model outputs with broom\nYou’re trying to predict a person’s weight based on their waist circumference and stature (height). To do so you’re using the US army body measurement dataset ANSUR II. The model has already been trained for you using this code:\n\nmodel <- lm(weight_kg ~ waist_circum_m + stature_m, data = ansur_df)\n\nYou will use the broom package’s glance() and tidy() functions in the console to inspect model outputs in a tidy format."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#question",
    "href": "Reshaping_Data_with_tidyr_C4.html#question",
    "title": "31  Rectangling Data",
    "section": "Question",
    "text": "Question\nWhat is the R2 value?\nPossible Answers\n\n0\n0.894 Respuesta\n5.09"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#question-1",
    "href": "Reshaping_Data_with_tidyr_C4.html#question-1",
    "title": "31  Rectangling Data",
    "section": "Question",
    "text": "Question\nWhat is the standard error on the intercept?\nPossible Answers\n\n-128.\n0.615\n1.25 Respuesta"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#nesting-tibbles",
    "href": "Reshaping_Data_with_tidyr_C4.html#nesting-tibbles",
    "title": "31  Rectangling Data",
    "section": "31.7 Nesting tibbles",
    "text": "31.7 Nesting tibbles\nYou’re pre-processing the US army body measurement dataset ANSUR II to train multiple models in a single pipeline. You’ll experiment with the nest() function to create a list column with nested tibbles containing sub-sets of the data.\nthe dplyr package has been pre-loaded for you."
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#question-2",
    "href": "Reshaping_Data_with_tidyr_C4.html#question-2",
    "title": "31  Rectangling Data",
    "section": "Question",
    "text": "Question\nWhat is the shape of the Combat Arms branch nested dataset?\nPossible Answers\n\n1,312 × 5\n1,582 × 5 Respuesta\n3,174 × 5"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#question-3",
    "href": "Reshaping_Data_with_tidyr_C4.html#question-3",
    "title": "31  Rectangling Data",
    "section": "Question",
    "text": "Question\nWhat is the nested tibble shape for females in the Combat Arms branch?\nPossible Answers\n\n43 × 4 Respuesta\n687 × 4\n1,539 × 4"
  },
  {
    "objectID": "Reshaping_Data_with_tidyr_C4.html#modeling-on-nested-data-frames",
    "href": "Reshaping_Data_with_tidyr_C4.html#modeling-on-nested-data-frames",
    "title": "31  Rectangling Data",
    "section": "31.8 Modeling on nested data frames",
    "text": "31.8 Modeling on nested data frames\nYou’ll be working on the US Army ANSUR II body measurement dataset, which has been pre-loaded as ansur_df. The goal is to nest the data for both sexes so that you can simultaneously train two linear models, one for each sex. These models will derive a person’s weight from their stature (height) and waist circumference. You’ll then unnest the data to inspect the model’s statistics produced by the glance() function from the broom package.\nThe dplyr, broom, and purrr packages have been pre-loaded for you.\nSide note: In the provided code, the purrr package’s map() function applies functions on each nested data frame. Check out this package if you like using functions inside pipes!"
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#date-uniformity",
    "href": "Cleaning_Data_in_R_C3.html#date-uniformity",
    "title": "26  Advanced Data Problems",
    "section": "26.1 Date uniformity",
    "text": "26.1 Date uniformity\nIn this chapter, you work at an asset management company and you’ll be working with the accounts dataset, which contains information about each customer, the amount in their account, and the date their account was opened. Your boss has asked you to calculate some summary statistics about the average value of each account and whether the age of the account is associated with a higher or lower account value. Before you can do this, you need to make sure that the accounts dataset you’ve been given doesn’t contain any uniformity problems. In this exercise, you’ll investigate the date_opened column and clean it up so that all the dates are in the same format.\ndplyr and lubridate are loaded and accounts is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#instructions-100-xp",
    "href": "Cleaning_Data_in_R_C3.html#instructions-100-xp",
    "title": "26  Advanced Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nTake a look at the head of accounts to get a sense of the data you’re working with.\nConvert the dates in the date_opened column to the same format using the formats vector and store this as a new column called date_opened_clean."
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#question",
    "href": "Cleaning_Data_in_R_C3.html#question",
    "title": "26  Advanced Data Problems",
    "section": "Question",
    "text": "Question\nTry running as.Date(accounts$date_opened) in the console and examine the output. Notice that you end up with a lot of NAs. Why is this?\nPossible Answers\n\nas.Date() needs to be explicitly told the formats of every single date, including which dates are in which format.\nBy default, as.Date() can’t convert “Month DD, YYYY” formats. Respuesta\nas.Date() can’t convert characters to Dates.\n\n\n\nE1.R\n\n# Check out the accounts data frame\nhead(accounts)\n\n\n# Check out the accounts data frame\nhead(accounts)\n\n# Define the date formats\nformats <- c(\"%Y-%m-%d\", \"%B %d, %Y\")\n\n# Convert dates to the same format\naccounts %>%\n  mutate(date_opened_clean = parse_date_time(date_opened, orders = formats))"
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#currency-uniformity",
    "href": "Cleaning_Data_in_R_C3.html#currency-uniformity",
    "title": "26  Advanced Data Problems",
    "section": "26.2 Currency uniformity",
    "text": "26.2 Currency uniformity\nNow that your dates are in order, you’ll need to correct any unit differences. When you first plot the data, you’ll notice that there’s a group of very high values, and a group of relatively lower values. The bank has two different offices - one in New York, and one in Tokyo, so you suspect that the accounts managed by the Tokyo office are in Japanese yen instead of U.S. dollars. Luckily, you have a data frame called account_offices that indicates which office manages each customer’s account, so you can use this information to figure out which totals need to be converted from yen to dollars.\nThe formula to convert yen to dollars is USD = JPY / 104.\ndplyr and ggplot2 are loaded and the accounts and account_offices data frames are available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#instructions-100-xp-1",
    "href": "Cleaning_Data_in_R_C3.html#instructions-100-xp-1",
    "title": "26  Advanced Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a scatter plot with date_opened on the x-axis and total on the y-axis.\nLeft join accounts and account_offices by their id columns.\nConvert the totals from the Tokyo office from yen to dollars, and keep the total from the New York office in dollars. Store this as a new column called total_usd.\nCreate a scatter plot of your new uniform data using date_opened on the x-axis and total_usd on the y-axis.\n\n\n\nE2.R\n\n# Scatter plot of opening date and total amount\naccounts %>%\n  ggplot(aes(x = date_opened, y = total)) +\n  geom_point()\n  \n\n# Scatter plot of opening date and total amount\naccounts %>%\n  ggplot(aes(x = date_opened, y = total)) +\n  geom_point()\n\n# Left join accounts and account_offices by id\naccounts %>%\n  left_join(account_offices, by = \"id\")\n  \n\n# Scatter plot of opening date and total amount\naccounts %>%\n  ggplot(aes(x = date_opened, y = total)) +\n  geom_point()\n\n# Left join accounts to account_offices by id\naccounts %>%\n  left_join(account_offices, by = \"id\") %>%\n  # Convert totals from the Tokyo office to USD\n  mutate(total_usd = ifelse(office == \"Tokyo\", total / 104, total))\n  \n  \n# Scatter plot of opening date and total amount\naccounts %>%\n  ggplot(aes(x = date_opened, y = total)) +\n  geom_point()\n\n# Left join accounts to account_offices by id\naccounts %>%\n  left_join(account_offices, by = \"id\") %>%\n  # Convert totals from the Tokyo office to USD\n  mutate(total_usd = ifelse(office == \"Tokyo\", total / 104, total)) %>%\n  # Scatter plot of opening date vs total_usd\n  ggplot(aes(x = date_opened, y = total_usd)) +\n    geom_point()"
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#validating-totals",
    "href": "Cleaning_Data_in_R_C3.html#validating-totals",
    "title": "26  Advanced Data Problems",
    "section": "26.3 Validating totals",
    "text": "26.3 Validating totals\nIn this lesson, you’ll continue to work with the accounts data frame, but this time, you have a bit more information about each account. There are three different funds that account holders can store their money in. In this exercise, you’ll validate whether the total amount in each account is equal to the sum of the amount in fund_A, fund_B, and fund_C. If there are any accounts that don’t match up, you can look into them further to see what went wrong in the bookkeeping that led to inconsistencies.\ndplyr is loaded and accounts is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#instructions-100-xp-2",
    "href": "Cleaning_Data_in_R_C3.html#instructions-100-xp-2",
    "title": "26  Advanced Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a new column called theoretical_total that contains the sum of the amounts in each fund.\nFind the accounts where the total doesn’t match the theoretical_total.\n\n\n\nE3.R\n\n# Find invalid totals\naccounts %>%\n  # theoretical_total: sum of the three funds\n  mutate(theoretical_total = fund_A + fund_B + fund_C) %>%\n  # Find accounts where total doesn't match theoretical_total\n  filter(theoretical_total != total)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#validating-age",
    "href": "Cleaning_Data_in_R_C3.html#validating-age",
    "title": "26  Advanced Data Problems",
    "section": "26.4 Validating age",
    "text": "26.4 Validating age\nNow that you found some inconsistencies in the total amounts, you’re suspicious that there may also be inconsistencies in the acct_agecolumn, and you want to see if these inconsistencies are related. Using the skills you learned from the video exercise, you’ll need to validate the age of each account and see if rows with inconsistent acct_ages are the same ones that had inconsistent totals\ndplyr and lubridate are loaded, and accounts is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#instructions-100-xp-3",
    "href": "Cleaning_Data_in_R_C3.html#instructions-100-xp-3",
    "title": "26  Advanced Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a new column called theoretical_age that contains the age of each account based on the date_opened.\nFind the accounts where the acct_age doesn’t match the theoretical_age.\n\n\n\nE4.R\n\n# Find invalid acct_age\naccounts %>%\n  # theoretical_age: age of acct based on date_opened\n  mutate(theoretical_age = floor(as.numeric(date_opened %--% today(), \"years\"))) %>%\n  # Filter for rows where acct_age is different from theoretical_age\n  filter(acct_age != theoretical_age)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#visualizing-missing-data",
    "href": "Cleaning_Data_in_R_C3.html#visualizing-missing-data",
    "title": "26  Advanced Data Problems",
    "section": "26.5 Visualizing missing data",
    "text": "26.5 Visualizing missing data\nDealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data.\nYou just received a new version of the accounts data frame containing data on the amount held and amount invested for new and existing customers. However, there are rows with missing inv_amount values.\nYou know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. The dplyr and visdat packages have been loaded and accounts is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#instructions-100-xp-4",
    "href": "Cleaning_Data_in_R_C3.html#instructions-100-xp-4",
    "title": "26  Advanced Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nVisualize the missing values in accounts by column using a function from the visdat package.\nAdd a logical column to accounts called missing_inv that indicates whether each row is missing the inv_amount or not.\nGroup by missing_inv.\nCalculate the mean age for each group of missing_inv.\nSort accounts by age.\nVisualize missing data by column."
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#question-1",
    "href": "Cleaning_Data_in_R_C3.html#question-1",
    "title": "26  Advanced Data Problems",
    "section": "Question",
    "text": "Question\nTake a look at the mean age for each group of missing_inv. What’s going on here?\nPossible Answers\nThe data is missing completely at random and there are no drivers behind the missingness.\nSince the average age for TRUE missing_inv is 22 and the average age for FALSE missing_inv is 44, it is likely that the inv_amount variable is missing mostly in young customers. Respuesta\nSince the average age for FALSE missing_inv is 22 and the average age for TRUE missing_inv is 44, it is likely that the inv_amount variable is missing mostly in older customers.\n\n\nE5.R\n\n# Visualize the missing values by column\nvis_miss(accounts)\n\n\n# Visualize the missing values by column\nvis_miss(accounts)\n\naccounts %>%\n  # missing_inv: Is inv_amount missing?\n  mutate(missing_inv = is.na(inv_amount)) %>%\n  # Group by missing_inv\n  group_by(missing_inv) %>%\n  # Calculate mean age for each missing_inv group\n  summarize(avg_age = mean(age))\n\n\n# Visualize the missing values by column\nvis_miss(accounts)\n\naccounts %>%\n  # missing_inv: Is inv_amount missing?\n  mutate(missing_inv = is.na(inv_amount)) %>%\n  # Group by missing_inv\n  group_by(missing_inv) %>%\n  # Calculate mean age for each missing_inv group\n  summarize(avg_age = mean(age))\n\n# Sort by age and visualize missing vals\naccounts %>%\n  arrange(age) %>%\n  vis_miss()"
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#treating-missing-data",
    "href": "Cleaning_Data_in_R_C3.html#treating-missing-data",
    "title": "26  Advanced Data Problems",
    "section": "26.6 Treating missing data",
    "text": "26.6 Treating missing data\nIn this exercise, you’re working with another version of the accounts data that contains missing values for both the cust_id and acct_amount columns.\nYou want to figure out how many unique customers the bank has, as well as the average amount held by customers. You know that rows with missing cust_id don’t really help you, and that on average, the acct_amount is usually 5 times the amount of inv_amount.\nIn this exercise, you will drop rows of accounts with missing cust_ids, and impute missing values of inv_amount with some domain knowledge. dplyr and assertive are loaded and accounts is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C3.html#instructions-100-xp-5",
    "href": "Cleaning_Data_in_R_C3.html#instructions-100-xp-5",
    "title": "26  Advanced Data Problems",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFilter accounts to remove rows with missing cust_ids and save as accounts_clean.\nCreate a new column called acct_amount_filled, which contains the values of acct_amount, except all NA values should be replaced with 5 times the amount in inv_amount.\nAssert that there are no missing values in the cust_id column of accounts_clean.\nAssert that there are no missing values in the acct_amount_filled column of accounts_clean.\n\n\n\nE6.R\n\n# Create accounts_clean\naccounts_clean <- accounts %>%\n  # Filter to remove rows with missing cust_id\n  filter(!is.na(cust_id))\n\naccounts_clean\n\n# Create accounts_clean\naccounts_clean <- accounts %>%\n  # Filter to remove rows with missing cust_id\n  filter(!is.na(cust_id)) %>%\n  # Add new col acct_amount_filled with replaced NAs\n  mutate(acct_amount_filled = ifelse(is.na(acct_amount), inv_amount * 5, acct_amount))\n\naccounts_clean\n\n\n# Create accounts_clean\naccounts_clean <- accounts %>%\n  # Filter to remove rows with missing cust_id\n  filter(!is.na(cust_id)) %>%\n  # Add new col acct_amount_filled with replaced NAs\n  mutate(acct_amount_filled = ifelse(is.na(acct_amount), inv_amount * 5, acct_amount))\n\n# Assert that cust_id has no missing vals\nassert_all_are_not_na(accounts_clean$cust_id)\n\n# Create accounts_clean\naccounts_clean <- accounts %>%\n  # Filter to remove rows with missing cust_id\n  filter(!is.na(cust_id)) %>%\n  # Add new col acct_amount_filled with replaced NAs\n  mutate(acct_amount_filled = ifelse(is.na(acct_amount), inv_amount * 5, acct_amount))\n\n# Assert that cust_id has no missing vals\nassert_all_are_not_na(accounts_clean$cust_id)\n\n# Assert that acct_amount_filled has no missing vals\nassert_all_are_not_na(accounts_clean$acct_amount_filled)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#small-distance-small-difference",
    "href": "Cleaning_Data_in_R_C4.html#small-distance-small-difference",
    "title": "27  Record Linkage",
    "section": "27.1 Small distance, small difference",
    "text": "27.1 Small distance, small difference\nIn the video exercise, you learned that there are multiple ways to calculate how similar or different two strings are. Now you’ll practice using the stringdist package to compute string distances using various methods. It’s important to be familiar with different methods, as some methods work better on certain datasets, while others work better on other datasets.\nThe stringdist package has been loaded for you."
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#instructions-100-xp",
    "href": "Cleaning_Data_in_R_C4.html#instructions-100-xp",
    "title": "27  Record Linkage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the Damerau-Levenshtein distance between “las angelos” and “los angeles”.\nCalculate the Longest Common Substring (LCS) distance between “las angelos” and “los angeles”.\nCalculate the Jaccard distance between “las angelos” and “los angeles”."
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#question",
    "href": "Cleaning_Data_in_R_C4.html#question",
    "title": "27  Record Linkage",
    "section": "Question",
    "text": "Question\nWhy is the LCS distance higher than the Damerau-Levenshtein distance between “las angelos” and “los angeles”?\nPossible Answers\nDamerau-Levenshtein distance is smaller because it’s always a better method.\nLCS distance only uses insertion and deletion, so it takes more operations to change a string to another.\nLCS distance only uses insertion, deletion, and substitution, so it takes more operations to change a string to another\n\n\nE1.R\n\n# Calculate Damerau-Levenshtein distance\nstringdist(\"las angelos\", \"los angeles\", method = \"dl\")\n\n# Calculate LCS distance\nstringdist(\"las angelos\", \"los angeles\", method=\"lcs\")\n\n# Calculate Jaccard distance\nstringdist(\"las angelos\", \"los angeles\", method=\"jaccard\")"
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#fixing-typos-with-string-distance",
    "href": "Cleaning_Data_in_R_C4.html#fixing-typos-with-string-distance",
    "title": "27  Record Linkage",
    "section": "27.2 Fixing typos with string distance",
    "text": "27.2 Fixing typos with string distance\nIn this chapter, one of the datasets you’ll be working with, zagat, is a set of restaurants in New York, Los Angeles, Atlanta, San Francisco, and Las Vegas. The data is from Zagat, a company that collects restaurant reviews, and includes the restaurant names, addresses, phone numbers, as well as other restaurant information.\nThe city column contains the name of the city that the restaurant is located in. However, there are a number of typos throughout the column. Your task is to map each city to one of the five correctly-spelled cities contained in the cities data frame.\ndplyr and fuzzyjoin are loaded, and zagat and cities are available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#instructions-100-xp-1",
    "href": "Cleaning_Data_in_R_C4.html#instructions-100-xp-1",
    "title": "27  Record Linkage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCount the number of each variation of city name in zagat\nLeft join zagat and cities based on string distance using the city and city_actual columns.\nSelect the name, city, and city_actual columns.\n\n\n\nE2.R\n\n# Count the number of each city variation\nzagat %>%\n  count(city)\n  \n# Count the number of each city variation\nzagat %>%\n  count(city)\n\n# Join and look at results\nzagat %>%\n  # Left join based on stringdist using city and city_actual cols\n  stringdist_left_join(cities, by = c(\"city\" = \"city_actual\")) %>%\n  # Select the name, city, and city_actual cols\n  select(name, city, city_actual)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#pair-blocking",
    "href": "Cleaning_Data_in_R_C4.html#pair-blocking",
    "title": "27  Record Linkage",
    "section": "27.3 Pair blocking",
    "text": "27.3 Pair blocking\nZagat and Fodor’s are both companies that gather restaurant reviews. The zagat and fodors datasets both contain information about various restaurants, including addresses, phone numbers, and cuisine types. Some restaurants appear in both datasets, but don’t necessarily have the same exact name or phone number written down. In this chapter, you’ll work towards figuring out which restaurants appear in both datasets.\nThe first step towards this goal is to generate pairs of records so that you can compare them. In this exercise, you’ll first generate all possible pairs, and then use your newly-cleaned city column as a blocking variable.\nzagat and fodors are available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#instructions-100-xp-2",
    "href": "Cleaning_Data_in_R_C4.html#instructions-100-xp-2",
    "title": "27  Record Linkage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nLoad the reclin package.\nGenerate all possible pairs of records between the zagat and fodors datasets.\nUse pair blocking to generate only pairs that have matching values in the city column.\n\n\n\nE3.R\n\n# Load reclin\nlibrary(reclin)\n\n# Generate all possible pairs\npair_blocking(zagat, fodors)\n\n\n# Load reclin\nlibrary(reclin)\n\n# Generate pairs with same city\npair_blocking(zagat, fodors, blocking_var =\"city\")"
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#comparing-pairs",
    "href": "Cleaning_Data_in_R_C4.html#comparing-pairs",
    "title": "27  Record Linkage",
    "section": "27.4 Comparing pairs",
    "text": "27.4 Comparing pairs\nNow that you’ve generated the pairs of restaurants, it’s time to compare them. You can easily customize how you perform your comparisons using the by and default_comparator arguments. There’s no right answer as to what each should be set to, so in this exercise, you’ll try a couple options out.\ndplyr and reclin are loaded and zagat and fodors are available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#instructions-100-xp-3",
    "href": "Cleaning_Data_in_R_C4.html#instructions-100-xp-3",
    "title": "27  Record Linkage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCompare pairs by name using lcs() distance.\nCompare pairs by name, phone, and addr using jaro_winkler().\n\n\n\nE4.R\n\n# Generate pairs\npair_blocking(zagat, fodors, blocking_var = \"city\") %>%\n  # Compare pairs by name using lcs()\n  compare_pairs(by = \"name\",\n                default_comparator = lcs())\n                \n\n# Generate pairs\npair_blocking(zagat, fodors, blocking_var = \"city\") %>%\n  # Compare pairs by name, phone, addr\n  compare_pairs(by = c(\"name\", \"phone\", \"addr\"),\n                default_comparator = jaro_winkler())"
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#putting-it-together",
    "href": "Cleaning_Data_in_R_C4.html#putting-it-together",
    "title": "27  Record Linkage",
    "section": "27.5 Putting it together",
    "text": "27.5 Putting it together\nDuring this chapter, you’ve cleaned up the city column of zagat using string similarity, as well as generated and compared pairs of restaurants from zagat and fodors. The end is near - all that’s left to do is score and select pairs and link the data together, and you’ll be able to begin your analysis in no time!\nreclin and dplyr are loaded and zagat and fodors are available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C4.html#instructions-100-xp-4",
    "href": "Cleaning_Data_in_R_C4.html#instructions-100-xp-4",
    "title": "27  Record Linkage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nScore the pairs of records probabilistically.\nSelect the pairs that are considered matches.\nLink the two data frames together.\n\n\n\nE5.R\n\n# Create pairs\npair_blocking(zagat, fodors, blocking_var = \"city\") %>%\n  # Compare pairs\n  compare_pairs(by = c(\"name\", \"addr\"), default_comparator = jaro_winkler()) %>%\n  # Score pairs\n  score_problink()\n  \n  \n# Create pairs\npair_blocking(zagat, fodors, blocking_var = \"city\") %>%\n  # Compare pairs\n  compare_pairs(by = c(\"name\", \"addr\"), default_comparator = jaro_winkler()) %>%\n  # Score pairs\n  score_problink() %>%\n  # Select pairs\n  select_n_to_m()\n  \n# Create pairs\npair_blocking(zagat, fodors, blocking_var = \"city\") %>%\n  # Compare pairs\n  compare_pairs(by = c(\"name\", \"addr\"), default_comparator = jaro_winkler()) %>%\n  # Score pairs\n  score_problink() %>%\n  # Select pairs\n  select_n_to_m() %>%\n  # Link data \n  link()"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#using-and-finding-missing-values",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#using-and-finding-missing-values",
    "title": "32  Why care about missing data?",
    "section": "32.1 Using and finding missing values",
    "text": "32.1 Using and finding missing values\nWhen working with missing data, there are a couple of commands that you should be familiar with - firstly, you should be able to identify if there are any missing values, and where these are.\nUsing the any_na() and are_na() tools, identify which values are missing."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp",
    "title": "32  Why care about missing data?",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a vector, x, which has the values NA, NaN, Inf, “.”, and “missing”.\nPass a vector x to any_na() and are_na() to find and explore which ones are missing.\n\n\n\nE1.R\n\n# Create x, a vector, with values NA, NaN, Inf, \".\", and \"missing\"\nx <- c(NA, NaN, Inf, \".\", \"missing\")\n\n# Use any_na() and are_na() on to explore the missings\nany_na(x)\nare_na(x)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#how-many-missing-values-are-there",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#how-many-missing-values-are-there",
    "title": "32  Why care about missing data?",
    "section": "32.2 How many missing values are there?",
    "text": "32.2 How many missing values are there?\nOne of the first things that you will want to check with a new dataset is if there are any missing missing values, and how many there are.\nYou could use are_na() to and count up the missing values, but the most efficient way to count missings is to use the n_miss() function. This will tell you the total number of missing values in the data.\nYou can then find the percent of missing values in the data with the pct_miss function. This will tell you the percentage of missing values in the data.\nYou can also find the complement to these - how many complete values there are - using n_complete and pct_complete."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-1",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-1",
    "title": "32  Why care about missing data?",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the example dataframe of heights and weights dat_hw:\n\nUse n_miss() on the dataframe dat_hw to count the total number of missing values the dataframe.\nUse n_miss() on the variable dat_hw$weight to count the total number of missing values it.\nSimilarly, use prop_miss(), n_complete(), and prop_complete() to get the proportion of missings, and the number and proportion of complete values for the dataframe and the variables.\n\n\n\nE2.R\n\n# Use n_miss() to count the total number of missing values in dat_hw\nn_miss(dat_hw)\n\n# Use n_miss() on dat_hw$weight to count the total number of missing values\nn_miss(dat_hw$weight)\n\n# Use n_complete() on dat_hw to count the total number of complete values\nn_complete(dat_hw)\n\n# Use n_complete() on dat_hw$weight to count the total number of complete values\nn_complete(dat_hw$weight)\n\n# Use prop_miss() and prop_complete() on dat_hw to count the total number of missing values in each of the variables\nprop_miss(dat_hw)\nprop_complete(dat_hw)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#summarizing-missingness",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#summarizing-missingness",
    "title": "32  Why care about missing data?",
    "section": "32.3 Summarizing missingness",
    "text": "32.3 Summarizing missingness\nNow that you understand the behavior of missing values in R, and how to count them, let’s scale up our summaries for cases (rows) and variables, using miss_var_summary() and miss_case_summary(), and also explore how they can be applied for groups in a dataframe, using the group_by function from dplyr."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-2",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-2",
    "title": "32  Why care about missing data?",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate summaries of missingness in the airquality dataset for variables using the miss_var_summary() function.\nCalculate summaries of missingness in the airquality dataset for the cases using the miss_case_summary() function.\nUsing the airquality dataset, use group_by() to create summaries for each variable and case, by each Month.\n\n\n\nE3.R\n\n# Summarize missingness in each variable of the `airquality` dataset\nmiss_var_summary(airquality)\n\n\n# Summarize missingness in each case of the `airquality` dataset\nmiss_case_summary(airquality)\n\n\n# Return the summary of missingness in each variable, \n# grouped by Month, in the `airquality` dataset\nairquality %>% group_by(Month) %>% miss_var_summary()\n\n# Return the summary of missingness in each case, \n# grouped by Month, in the `airquality` dataset\nairquality %>% group_by(Month) %>% miss_case_summary()"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#tabulating-missingness",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#tabulating-missingness",
    "title": "32  Why care about missing data?",
    "section": "32.4 Tabulating Missingness",
    "text": "32.4 Tabulating Missingness\nThe summaries of missingness we just calculated give us the number and percentage of missing observations for the cases and variables.\nAnother way to summarize missingness is by tabulating the number of times that there are 0, 1, 2, 3, missings in a variable, or in a case.\nIn this exercise we are going to tabulate the number of missings in each case and variable using miss_var_table() and miss_case_table(), and also combine these summaries with the the group_by operator from dplyr. to explore the summaries over a grouping variable in the dataset."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-3",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-3",
    "title": "32  Why care about missing data?",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFor the airquality dataset:\n\nTabulate missingness for each variable using miss_var_table().\nTabulate missingness for every case using miss_case_table().\nCombine previous tabulations with dplyr’s group_by() function to create tabulations for each variable and case, by each Month.\n\n\n\nE4.R\n\n# Tabulate missingness in each variable and case of the `airquality` dataset\nmiss_var_table(airquality)\nmiss_case_table(airquality)\n\n# Tabulate the missingness in each variable, grouped by Month, in the `airquality` dataset\nairquality %>% group_by(Month) %>% miss_var_table()\n\n# Tabulate of missingness in each case, grouped by Month, in the `airquality` dataset\nairquality %>% group_by() %>% miss_case_table()"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#other-summaries-of-missingness",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#other-summaries-of-missingness",
    "title": "32  Why care about missing data?",
    "section": "32.5 Other summaries of missingness",
    "text": "32.5 Other summaries of missingness\nSome summaries of missingness are particularly useful for different types of data. For example, miss_var_span() and miss_var_run().\nmiss_var_span() calculates the number of missing values in a specified variable for a repeating span. This is really useful in time series data, to look for weekly (7 day) patterns of missingness.\nmiss_var_run() calculates the number of “runs” or “streaks” of missingness. This is useful to find unusual patterns of missingness, for example, you might find a repeating pattern of 5 complete and 5 missings.\nBoth miss_var_span() and miss_var_run() work with the group_by operator from dplyr."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-4",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-4",
    "title": "32  Why care about missing data?",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the pedestrian dataset from naniar:\n\nCalculate summaries of missingness for the variables in datasets using miss_var_span(), for a span of 4000.\nCalculate summaries of missingness for the cases in datasets using miss_var_run().\nCombine with dplyr’s group_by operator for month.\n\n\n\nE5.R\n\n# Calculate the summaries for each run of missingness for the variable, hourly_counts\nmiss_var_run(pedestrian, var = hourly_counts)\n\n# Calculate the summaries for each span of missingness, \n# for a span of 4000, for the variable hourly_counts\nmiss_var_span(pedestrian, var = hourly_counts, span_every = 4000)\n\n# For each `month` variable, calculate the run of missingness for hourly_counts\npedestrian %>% group_by(month) %>% miss_var_run(var = hourly_counts)\n\n# For each `month` variable, calculate the span of missingness \n# of a span of 2000, for the variable hourly_counts\npedestrian %>% group_by(month) %>% miss_var_span(var = hourly_counts, span_every = 2000)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#your-first-missing-data-visualizations",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#your-first-missing-data-visualizations",
    "title": "32  Why care about missing data?",
    "section": "32.6 Your first missing data visualizations",
    "text": "32.6 Your first missing data visualizations\nIt can be difficult to get a handle on where the missing values are in your data, and here is where visualization can really help.\nThe function vis_miss() creates an overview visualization of the missingness in the data. It also has options to cluster rows based on missingness, using cluster = TRUE; as well as options for sorting the columns, from most missing to least missing (sort_miss = TRUE)."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-5",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-5",
    "title": "32  Why care about missing data?",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the riskfactors dataset from naniar:\n\nUse vis_miss() to visualize the missingness in the data.\nUse vis_miss() with cluster = TRUE to explore some clusters of missingness.\nUse vis_miss() and sort the missings with sort_miss to arrange the columns by missingness.\n\n\n\nE6.R\n\n# Visualize all of the missingness in the `riskfactors`  dataset\nvis_miss(riskfactors)\n\n# Visualize and cluster all of the missingness in the `riskfactors` dataset\nvis_miss(riskfactors, cluster = TRUE)\n\n# Visualize and sort the columns by missingness in the `riskfactors` dataset\nvis_miss(riskfactors, sort_miss = TRUE)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#visualizing-missing-cases-and-variables",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#visualizing-missing-cases-and-variables",
    "title": "32  Why care about missing data?",
    "section": "32.7 Visualizing missing cases and variables",
    "text": "32.7 Visualizing missing cases and variables\nTo get a clear picture of the missingness across variables and cases, use gg_miss_var() and gg_miss_case(). These are the visual counterpart to miss_var_summary() and miss_case_summary().\nThese can be split up into multiple plots with one for each category by choosing a variable to facet by."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-6",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-6",
    "title": "32  Why care about missing data?",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the riskfactors dataset:\n\nVisualize the number of missings in cases using gg_miss_case().\nExplore the number of missings in cases using gg_miss_case() and facet by the variable education.\nVisualize the number of missings in variables using gg_miss_var().\nExplore the number of missings in variables using gg_miss_var() and facet by the variable education.\nWhat do you notice in the visualizations of the whole data compared to the faceting?\n\n\n\nE7.R\n\n# Visualize the number of missings in cases using `gg_miss_case()`\ngg_miss_case(riskfactors)\n\n# Explore the number of missings in cases using `gg_miss_case()` \n# and facet by the variable `education`\ngg_miss_case(riskfactors, facet = education)\n\n# Visualize the number of missings in variables using `gg_miss_var()`\ngg_miss_var(riskfactors)\n\n# Explore the number of missings in variables using `gg_miss_var()` \n# and facet by the variable `education`\ngg_miss_var(riskfactors, facet = education)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#visualizing-missingness-patterns",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#visualizing-missingness-patterns",
    "title": "32  Why care about missing data?",
    "section": "32.8 Visualizing missingness patterns",
    "text": "32.8 Visualizing missingness patterns\nLet’s practice a few different ways to visualize patterns of missingness using:\n\n\ngg_miss_upset() to give an overall pattern of missingness.\ngg_miss_fct() for a dataset that has a factor of interest: marriage.\nand gg_miss_span() to explore the missingness in a time series dataset.\n\n\nWhat do you notice with the missingness and the faceting in the data?"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-7",
    "href": "Dealing_With_Missing_Data_in_R_C1.html#instructions-100-xp-7",
    "title": "32  Why care about missing data?",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExplore missingness pattern of the airquality dataset with gg_miss_upset().\nExplore how the missingness changes in the riskfactors dataset across the marital variable using gg_miss_fct()\nExplore how the missingness changes in the pedestrian dataset across the hourly_counts variable over a span of 3000 (you can also try different spans from 2000-5000).\nExplore the impact of month on hourly_counts by including it in the facet argument, with a span of 1000.\n\n\n\nE8.R\n\n# Using the airquality dataset, explore the missingness pattern using gg_miss_upset()\ngg_miss_upset(airquality)\n\n# With the riskfactors dataset, explore how the missingness changes across the marital variable using gg_miss_fct()\ngg_miss_fct(x = riskfactors, fct = marital)\n\n# Using the pedestrian dataset, explore how the missingness of hourly_counts changes over a span of 3000 \ngg_miss_span(pedestrian, var = hourly_counts, span_every = 3000)\n\n# Using the pedestrian dataset, explore the impact of month by faceting by month\n# and explore how missingness changes for a span of 1000\ngg_miss_span(pedestrian, var = hourly_counts, span_every = 1000, facet = month)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#r",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#r",
    "title": "33  Importing data from flat files with utils",
    "section": "33.1 r",
    "text": "33.1 r"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp",
    "title": "33  Wrangling and tidying up missing values",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFor the pacman dataset, use miss_scan_count() to search for strange missing values:\n\n“N/A”, “missing”, “na”, and ” ” (a single space).\nTo search for strange missing values all at once.\n\n\n\nE1.R\n\n# Explore the strange missing values \"N/A\"\nmiss_scan_count(data = pacman, search = list(\"N/A\"))\n\n# Explore the strange missing values \"missing\"\nmiss_scan_count(data = pacman, search = list(\"missing\"))\n\n# Explore the strange missing values \"na\"\nmiss_scan_count(data = pacman, search = list(\"na\"))\n\n# Explore the strange missing values \" \" (a single space)\nmiss_scan_count(data = pacman, search = list(\" \"))\n\n# Explore all of the strange missing values, \"N/A\", \"missing\", \"na\", \" \"\nmiss_scan_count(data = pacman, search = list(\"N/A\", \"missing\",\"na\", \" \"))"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#section",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#section",
    "title": "33  Importing data from flat files with utils",
    "section": "33.2 ",
    "text": "33.2"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-1",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-1",
    "title": "33  Wrangling and tidying up missing values",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFor the pacman dataset:\n\nUsing replace_with_na(), replace missing values “N/A”, “na”, and “missing” with NA for the year, and score variables.\nTest to see if you have removed all the missing values using miss_scan_count() at the end.\n\n\n\nE2.R\n\n# Print the top of the pacman data using `head()`\nhead(pacman)\n\n# Replace the strange missing values \"N/A\", \"na\", and \n# \"missing\" with `NA` for the variables, year, and score\npacman_clean <- replace_with_na(pacman, replace = list(year = c(\"N/A\", \"na\", \"missing\"),\n                                score = c(\"N/A\", \"na\", \"missing\")))\n                                        \n# Test if `pacman_clean` still has these values in it?\nmiss_scan_count(pacman_clean, search = list(\"N/A\", \"na\", \"missing\"))"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#section-1",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#section-1",
    "title": "33  Importing data from flat files with utils",
    "section": "33.3 ",
    "text": "33.3"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-2",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-2",
    "title": "33  Wrangling and tidying up missing values",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFor the dataset pacman replace the same special missing values, “N/A”, “missing”, “na”, and ” “:\n\nyear, month, and day, using replace_with_na_at().\nOnly character variables using replace_with_na_if().\nAll variables using replace_with_na_all().\n\n\n\nE3.R\n\n# Use `replace_with_na_at()` to replace with NA\nreplace_with_na_at(pacman,\n                   .vars = c(\"year\", \"month\", \"day\"), \n                   ~.x %in% c(\"N/A\", \"missing\", \"na\", \" \"))\n\n# Use `replace_with_na_if()` to replace with NA the character values using `is.character`\nreplace_with_na_if(pacman,\n                   .predicate = is.character, \n                   ~.x %in% c(\"N/A\", \"missing\", \"na\", \" \"))\n\n# Use `replace_with_na_all()` to replace with NA\nreplace_with_na_all(pacman, ~.x %in% c(\"N/A\", \"missing\", \"na\", \" \"))"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#section-2",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#section-2",
    "title": "33  Importing data from flat files with utils",
    "section": "33.4 ",
    "text": "33.4"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-3",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-3",
    "title": "33  Wrangling and tidying up missing values",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFor the frogger dataset:\n\nUse complete() from tidyr on the time and name variables to make implicit missing values explicit.\n\n\n\nE4.R\n\n# Print the frogger data to have a look at it\nfrogger\n\n# Use `complete()` on the `time` and `name` variables to  \n# make implicit missing values explicit\nfrogger_tidy <- frogger %>% complete(time, name)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#section-3",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#section-3",
    "title": "33  Importing data from flat files with utils",
    "section": "33.5 ",
    "text": "33.5"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-4",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-4",
    "title": "33  Wrangling and tidying up missing values",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUse fill() to fill down the “name” observations in the frogger dataset.\n\n\nE5.R\n\n# Print the frogger data to have a look at it\nfrogger\n\n# Use `fill()` to fill down the name variable in the frogger dataset\nfrogger %>% fill(name)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#section-4",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#section-4",
    "title": "33  Importing data from flat files with utils",
    "section": "33.6 ",
    "text": "33.6"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-5",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-5",
    "title": "33  Wrangling and tidying up missing values",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUse fill() and complete() to correctly fill and complete the values in the data so we went up with a dataset that has 5 names, all completed, each with 4 times filled in.\n\n\nE6.R\n\n# Print the frogger data to have a look at it\nfrogger\n\n# Correctly fill() and complete() missing values so that our dataset becomes sensible\nfrogger %>%\n  fill(name) %>%\n  complete(name, time)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#r",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#r",
    "title": "34  Importing data from flat files with utils",
    "section": "34.1 r",
    "text": "34.1 r"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp",
    "title": "34  Testing missing relationships",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the oceanbuoys dataset:\n\nCreate shadow matrix data with as_shadow()\nCreate nabular data by binding the shadow to the data with bind_shadow()\nBind only the variables with missing values by using bind_shadow(only_miss = TRUE)\n\n\n\nE1.R\n\n# Create shadow matrix data with `as_shadow()`\nas_shadow(oceanbuoys)\n\n# Create nabular data by binding the shadow to the data with `bind_shadow()`\nbind_shadow(oceanbuoys)\n\n# Bind only the variables with missing values by using bind_shadow(only_miss = TRUE)\n\nbind_shadow(oceanbuoys, only_miss = TRUE)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#section",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#section",
    "title": "34  Importing data from flat files with utils",
    "section": "34.2 ",
    "text": "34.2"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-1",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-1",
    "title": "34  Testing missing relationships",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFor the oceanbuoys dataset:\nbind_shadow(), then group_by() for the missingness of humidity (humidity_NA) and calculate the means and standard deviations for wind east west (wind_ew) using summarize() from dplyr.\nRepeat this, but calculating summaries for wind north south (wind_ns).\n\n\n\nE2.R\n\n# `bind_shadow()` and `group_by()` humidity missingness (`humidity_NA`)\noceanbuoys %>%\n  bind_shadow() %>%\n  group_by(humidity_NA) %>%\n  summarize(wind_ew_mean = mean(wind_ew), # calculate mean of wind_ew\n            wind_ew_sd = sd(wind_ew)) # calculate standard deviation of wind_ew\n  \n# Repeat this, but calculating summaries for wind north south (`wind_ns`)\noceanbuoys %>%\n  bind_shadow() %>%\n  group_by(humidity_NA) %>%\n  summarize(wind_ns_mean = mean(wind_ns),\n            wind_ns_sd = sd(wind_ns))"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#section-1",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#section-1",
    "title": "34  Importing data from flat files with utils",
    "section": "34.3 ",
    "text": "34.3"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-2",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-2",
    "title": "34  Testing missing relationships",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing group_by() and summarize() on wind_ew: * Summarize by the missingness of air_temp_c_NA. * Summarize by missingness of air_temp_c_NA and humidity_NA.\n\n\nE3.R\n\n# Summarize wind_ew by the missingness of `air_temp_c_NA`\noceanbuoys %>% \n  bind_shadow() %>%\n  group_by(air_temp_c_NA) %>%\n  summarize(wind_ew_mean = mean(wind_ew),\n            wind_ew_sd = sd(wind_ew),\n            n_obs = n())\n\n# Summarize wind_ew by missingness of `air_temp_c_NA` and `humidity_NA`\noceanbuoys %>% \n  bind_shadow() %>%\n  group_by(air_temp_c_NA, humidity_NA) %>%\n  summarize(wind_ew_mean = mean(wind_ew),\n            wind_ew_sd = sd(wind_ew),\n            n_obs = n())"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#section-2",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#section-2",
    "title": "34  Importing data from flat files with utils",
    "section": "34.4 ",
    "text": "34.4"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-3",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-3",
    "title": "34  Testing missing relationships",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFirst explore the missingness structure of oceanbuoys using vis_miss().\nExplore the distribution of wind east west (wind_ew) for the missingness of air temperature using geom_density().\nExplore the distribution of sea temperature for the missingness of humidity using geom_density().\n\n\n\nE4.R\n\n# First explore the missingness structure of `oceanbuoys` using `vis_miss()`\nvis_miss(oceanbuoys)\n\n# Explore the distribution of `wind_ew` for the missingness  \n# of `air_temp_c_NA` using  `geom_density()`\nbind_shadow(oceanbuoys) %>%\n  ggplot(aes(x = wind_ew, \n             color = air_temp_c_NA)) + \n  geom_density()\n\n# Explore the distribution of sea temperature for the  \n# missingness of humidity (humidity_NA) using  `geom_density()`\nbind_shadow(oceanbuoys) %>%\n  ggplot(aes(x = sea_temp_c,\n             color = humidity_NA)) + \n  geom_density()"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#section-3",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#section-3",
    "title": "34  Importing data from flat files with utils",
    "section": "34.5 ",
    "text": "34.5"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-4",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-4",
    "title": "34  Testing missing relationships",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExplore the distribution of wind east west (wind_ew) for the missingness of air temperature using geom_density() and faceting by the missingness of air temperature (air_temp_c_NA).\nBuild upon this visualization by filling by the missingness of humidity (humidity_NA).\n\n\n\nE5.R\n\n# Explore the distribution of wind east west (wind_ew) for the missingness of air temperature \n# using geom_density() and faceting by the missingness of air temperature (air_temp_c_NA).\noceanbuoys %>%\n  bind_shadow() %>%\n  ggplot(aes(x = wind_ew)) + \n  geom_density() + \n  facet_wrap(~air_temp_c_NA)\n\n# Build upon this visualization by filling by the missingness of humidity (humidity_NA).\noceanbuoys %>%\n  bind_shadow() %>%\n  ggplot(aes(x = wind_ew,\n             color = humidity_NA)) + \n  geom_density() + \n  facet_wrap(~air_temp_c_NA)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#section-4",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#section-4",
    "title": "34  Importing data from flat files with utils",
    "section": "34.6 ",
    "text": "34.6"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-5",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-5",
    "title": "34  Testing missing relationships",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExplore the distribution of wind east west (wind_ew) for the missingness of air temperature using geom_boxplot()\nBuild upon this visualization by faceting by the missingness of humidity (humidity_NA).\n\n\n\nE6.R\n\n# Explore the distribution of wind east west (`wind_ew`) for  \n# the missingness of air temperature using  `geom_boxplot()`\noceanbuoys %>%\n  bind_shadow() %>%\n  ggplot(aes(x = air_temp_c_NA,\n             y = wind_ew)) + \n  geom_boxplot()\n\n# Build upon this visualization by faceting by the missingness of humidity (`humidity_NA`).\noceanbuoys %>%\n  bind_shadow() %>%\n  ggplot(aes(x = air_temp_c_NA,\n             y = wind_ew)) + \n  geom_boxplot() + \n  facet_wrap(~humidity_NA)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#r",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#r",
    "title": "35  Importing data from flat files with utils",
    "section": "35.1 r",
    "text": "35.1 r"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the oceanbuoys data:\n\nImpute below the range using impute_below_all().\nVisualize the new missing values for wind_ew on the x-axis and air_temp_c on the y-axis.\nImpute and track data with bind_shadow(), impute_below_all(), and add_label_shadow().\nShow the plot and inspect the imputed values.\n\n\n\nE1.R\n\n# Impute the data below the range using `impute_below`.\nocean_imp <- impute_below_all(oceanbuoys)\n\n# Visualize the new missing values\nggplot(ocean_imp,\n       aes(x = wind_ew, y = air_temp_c)) + \n  geom_point()\n\n# Impute and track data with `bind_shadow`, `impute_below`, and `add_label_shadow`\nocean_imp_track <- bind_shadow(oceanbuoys) %>% \n  impute_below_all() %>%\n  add_label_shadow()\n\n# Look at the imputed values\nocean_imp_track"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#section",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#section",
    "title": "35  Importing data from flat files with utils",
    "section": "35.2 ",
    "text": "35.2"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-1",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-1",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the oceanbuoys data:\n\nImpute and track the missing values using bind_shadow() and impute_below_all(), and add_label_shadow().\nVisualize the missingness in wind and air temperature on the x and y-axis respectively, coloring missing air temp values with air_temp_c_NA.\nVisualize humidity and air temp on the x and y-axis respectively, coloring any missing cases using the variable any_missing.\n\n\n\nE2.R\n\n# Impute and track the missing values\nocean_imp_track <- bind_shadow(oceanbuoys) %>% \n  impute_below_all() %>%\n  add_label_shadow()\n\n# Visualize the missingness in wind and air temperature,  \n# coloring missing air temp values with air_temp_c_NA\nggplot(ocean_imp_track,\n       aes(x = wind_ew, y = air_temp_c, color = air_temp_c_NA)) + \n  geom_point()\n\n# Visualize humidity and air temp, coloring any missing cases using the variable any_missing\nggplot(ocean_imp_track,\n       aes(x = humidity, y = air_temp_c, color = any_missing)) + \n  geom_point()"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#section-1",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#section-1",
    "title": "35  Importing data from flat files with utils",
    "section": "35.3 ",
    "text": "35.3"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-2",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-2",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the imputed and tracked data, ocean_imp_track:\n\nExplore the values of air_temp_c, visualizing the amount of missings with air_temp_c_NA.\nExplore the missings in humidity using humidity_NA.\nExplore the missings in air_temp_c according to year, using facet_wrap(~year).\nexplore the missings in humidity according to year, using facet_wrap(~year).\n\n\n\nE3.R\n\n# Explore the values of air_temp_c, visualizing the amount of missings with `air_temp_c_NA`.\np <- ggplot(ocean_imp_track, aes(x = air_temp_c, fill = air_temp_c_NA)) + geom_histogram()\n\n# Expore the missings in humidity using humidity_NA\np2 <- ggplot(ocean_imp_track, aes(x = humidity, fill = humidity_NA)) + geom_histogram()\n\n# Explore the missings in air_temp_c according to year, using `facet_wrap(~year)`.\np + facet_wrap(~year)\n\n# Explore the missings in humidity according to year, using `facet_wrap(~year)`.\np2 + facet_wrap(~year)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#section-2",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#section-2",
    "title": "35  Importing data from flat files with utils",
    "section": "35.4 ",
    "text": "35.4"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-3",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-3",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nFor the oceanbuoys dataset:\n\nImpute the mean value with impute_mean_all(), and track these imputations with add_label_shadow().\nExplore the imputed values in humidity (humidity) using a box plot.\nExplore the imputed values in air temperature (air_temp_c) using a box plot.\n\n\n\nE4.R\n\n# Impute the mean value and track the imputations \nocean_imp_mean <- bind_shadow(oceanbuoys) %>% \n  impute_mean_all() %>% \n  add_label_shadow()\n\n# Explore the mean values in humidity in the imputed dataset\nggplot(ocean_imp_mean, \n       aes(x = humidity_NA, y = humidity)) + \n  geom_boxplot()\n\n# Explore the values in air temperature in the imputed dataset\nggplot(ocean_imp_mean, \n       aes(x = air_temp_c_NA, y = air_temp_c)) + \n  geom_boxplot()"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#section-3",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#section-3",
    "title": "35  Importing data from flat files with utils",
    "section": "35.5 ",
    "text": "35.5"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-4",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-4",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the data with already imputed values, ocean_imp_mean:\n\nExplore imputations in air temperature (on the x-axis) and humidity (on the y-axis) using a scatter plot, remembering to use color = any_missing.\nBuild upon this previous visualization by faceting by year.\n\n\n\nE5.R\n\n# Explore imputations in air temperature and humidity,  \n# coloring by the variable, any_missing\nggplot(ocean_imp_mean, \n       aes(x = air_temp_c, y = humidity, color = any_missing)) + \n  geom_point()\n\n# Explore imputations in air temperature and humidity,  \n# coloring by the variable, any_missing, and faceting by year\nggplot(ocean_imp_mean, \n       aes(x = air_temp_c, y = humidity, color = any_missing)) +\n  geom_point() + \n  facet_wrap(~year)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#section-4",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#section-4",
    "title": "35  Importing data from flat files with utils",
    "section": "35.6 ",
    "text": "35.6"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-5",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-5",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the shadow_long() to gather the imputed data ocean_imp_mean, focussing on humidity and air_temp_c.\nPrint the data and inspect it.\nExplore the imputations in a histogram using geom_histogram(), placing the values on the x-axis, filling by their missingness and faceting by variable\n\n\n\nE6.R\n\n# Gather the imputed data \nocean_imp_mean_gather <- shadow_long(ocean_imp_mean,\n                                     humidity,\n                                     air_temp_c)\n# Inspect the data\nocean_imp_mean_gather\n\n# Explore the imputations in a histogram \nggplot(ocean_imp_mean_gather, \n       aes(x = value, fill = value_NA)) + \n  geom_histogram() + \n  facet_wrap(~variable)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#using-miss_scan_count",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#using-miss_scan_count",
    "title": "33  Wrangling and tidying up missing values",
    "section": "33.1 Using miss_scan_count",
    "text": "33.1 Using miss_scan_count\nYou have a dataset with missing values coded as “N/A”, “missing”, and “na”. But before we go ahead and start replacing these with NA, we should get an idea of how big the problem is.\nUse miss_scan_count to count the possible missings in the dataset, pacman, a dataset of pacman scores, containing three columns:\n\nyear: the year that person made that score. initial: the initials of the person. *score: the scores of that person."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#using-replace_with_na",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#using-replace_with_na",
    "title": "33  Wrangling and tidying up missing values",
    "section": "33.2 Using replace_with_na",
    "text": "33.2 Using replace_with_na\nFollowing on from the previous dataset, we now know that we have a few strange missing values.\nNow, we are going to do something about it, and replace these values with missings (e.g. NA) using the function replace_with_na()."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#using-replace_with_na-scoped-variants",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#using-replace_with_na-scoped-variants",
    "title": "33  Wrangling and tidying up missing values",
    "section": "33.3 Using replace_with_na scoped variants",
    "text": "33.3 Using replace_with_na scoped variants\nTo reduce code repetition when replacing values with NA, use the “scoped variants” of replace_with_na():\n\n\nreplace_with_na_at()\nreplace_with_na_if()\nreplace_with_na_all()\n\n\nThe syntax of replacement looks like this:\n\n~.x == “N/A”\n\nThis replaces all cases that are equal to “N/A”.\n\n~.x %in% c(“N/A”, “missing”, “na”, ” “)\n\nReplaces all cases that have “N/A”, “missing”, “na”, or ” “."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#fix-implicit-missings-using-complete",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#fix-implicit-missings-using-complete",
    "title": "33  Wrangling and tidying up missing values",
    "section": "33.4 Fix implicit missings using complete()",
    "text": "33.4 Fix implicit missings using complete()\nWe are going to explore a new dataset, frogger.\nThis dataset contains 4 scores per player recorded at different times: morning, afternoon, evening, and late_night.\nEvery player should have played 4 games, one at each of these times, but it looks like not every player completed all of these games.\nUse the complete() function to make these implicit missing values explicit"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#fix-explicit-missings-using-fill",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#fix-explicit-missings-using-fill",
    "title": "33  Wrangling and tidying up missing values",
    "section": "33.5 Fix explicit missings using fill()",
    "text": "33.5 Fix explicit missings using fill()\nOne type of missing value that can be obvious to deal with is where the first entry of a group is given, but subsequent entries are marked NA.\nThese missing values often result from empty values in spreadsheets to avoid entering multiple names multiple times; as well as for “human readability”.\nThis type of problem can be solved by using the fill() function from the tidyr package."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#using-complete-and-fill-together",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#using-complete-and-fill-together",
    "title": "33  Wrangling and tidying up missing values",
    "section": "33.6 Using complete() and fill() together",
    "text": "33.6 Using complete() and fill() together\nNow let’s put it together!\nUse complete() and fill() together to fix explicit and implicitly missing values in the frogger dataset."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#exploring-missingness-dependence",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#exploring-missingness-dependence",
    "title": "33  Wrangling and tidying up missing values",
    "section": "33.7 Exploring missingness dependence",
    "text": "33.7 Exploring missingness dependence\nTo learn about the structure of the missingness in data, you can explore how sorting changes how missingness is presented.\nFor the oceanbuoys dataset, explore the missingness with vis_miss(), and then arrange by a few different variables\nThis is not a definitive process, but it will get you started to ask the right questions of your data. We explore more powerful techniques in the next chapter."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-6",
    "href": "Dealing_With_Missing_Data_in_R_C2.html#instructions-100-xp-6",
    "title": "33  Wrangling and tidying up missing values",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the dataset, oceanbuoys:\n\nArrange by the variables, year, latitude, and wind_ew using vis_miss().\n\n\n\nE6.R\n\n# Arrange by year\noceanbuoys %>% arrange(year) %>% vis_miss()\n\n# Arrange by latitude\noceanbuoys %>% arrange(latitude) %>% vis_miss()\n\n# Arrange by wind_ew (wind east west)\noceanbuoys %>% arrange(wind_ew) %>% vis_miss()"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#creating-shadow-matrix-data",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#creating-shadow-matrix-data",
    "title": "34  Testing missing relationships",
    "section": "34.1 Creating shadow matrix data",
    "text": "34.1 Creating shadow matrix data\nMissing data can be tricky to think about, as they don’t usually proclaim themselves for you, and instead hide amongst the weeds of the data.\nOne way to help expose missing values is to change the way we think about the data - by thinking about every single data value being missing or not missing.\nThe as_shadow() function in R transforms a dataframe into a shadow matrix, a special data format where the values are either missing (NA), or Not Missing (!NA).\nThe column names of a shadow matrix are the same as the data, but have a suffix added _NA.\nTo keep track of and compare data values to their missingness state, use the bind_shadow() function. Having data in this format, with the shadow matrix column bound to the regular data is called nabular data."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#performing-grouped-summaries-of-missingness",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#performing-grouped-summaries-of-missingness",
    "title": "34  Testing missing relationships",
    "section": "34.2 Performing grouped summaries of missingness",
    "text": "34.2 Performing grouped summaries of missingness\nNow that you can create nabular data, let’s use it to explore the data. Let’s calculate summary statistics based on the missingness of another variable.\nTo do this we are going to use the following steps:\nFirst, bind_shadow() turns the data into nabular data.\nNext, perform some summaries on the data using group_by() and summarize() to calculate the mean and standard deviation, using the mean() and sd() functions."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#further-exploring-more-combinations-of-missingness",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#further-exploring-more-combinations-of-missingness",
    "title": "34  Testing missing relationships",
    "section": "34.3 Further exploring more combinations of missingness",
    "text": "34.3 Further exploring more combinations of missingness\nIt can be useful to get a bit of extra information about the number of cases in each missing condition.\nIn this exercise, we are going to add information about the number of observed cases using n() inside the summarize() function.\nWe will then add an additional level of grouping by looking at the combination of humidity being missing (humidity_NA) and air temperature being missing (air_temp_c_NA)."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#nabular-data-and-filling-by-missingness",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#nabular-data-and-filling-by-missingness",
    "title": "34  Testing missing relationships",
    "section": "34.4 Nabular data and filling by missingness",
    "text": "34.4 Nabular data and filling by missingness\nSummary statistics are useful to calculate, but as they say, a picture tells you a thousand words.\nIn this exercise, we are going to explore how you can use nabular data to explore the variation in a variable by the missingness of another.\nWe are going to use the oceanbuoys dataset from naniar."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#nabular-data-and-summarising-by-missingness",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#nabular-data-and-summarising-by-missingness",
    "title": "34  Testing missing relationships",
    "section": "34.5 Nabular data and summarising by missingness",
    "text": "34.5 Nabular data and summarising by missingness\nIn this exercise, we are going to explore how to use nabular data to explore the variation in a variable by the missingness of another.\nWe are going to use the oceanbuoys dataset from naniar, and then create multiple plots of the data using facets.\nThis allows you to explore different layers of missingness."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#explore-variation-by-missingness-box-plots",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#explore-variation-by-missingness-box-plots",
    "title": "34  Testing missing relationships",
    "section": "34.6 Explore variation by missingness: box plots",
    "text": "34.6 Explore variation by missingness: box plots\nPrevious exercises use nabular data along with density plots to explore the variation in a variable by the missingness of another.\nWe are going to use the oceanbuoys dataset from naniar, using box plots instead of facets or others to explore different layers of missingness."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#exploring-missing-data-with-scatter-plots",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#exploring-missing-data-with-scatter-plots",
    "title": "34  Testing missing relationships",
    "section": "34.7 Exploring missing data with scatter plots",
    "text": "34.7 Exploring missing data with scatter plots\nMissing values in a scatter plot in ggplot2 are removed by default, with a warning.\nWe can display missing values in a scatter plot, using geom_miss_point() - a special ggplot2 geom that shifts the missing values into the plot, displaying them 10% below the minimum of the variable.\nLet’s practice using this visualization with the oceanbuoys dataset."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-6",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-6",
    "title": "34  Testing missing relationships",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExplore the missingness in wind east west (wind_ew) and air temperature, and display the missingness using geom_miss_point().\nExplore the missingness in humidity and air temperature, and display the missingness using geom_miss_point().\n\n\n\nE7.R\n\n# Explore the missingness in wind and air temperature, and  \n# display the missingness using `geom_miss_point()`\nggplot(oceanbuoys,\n       aes(x = wind_ew,\n           y = air_temp_c)) + \n  geom_miss_point()\n\n# Explore the missingness in humidity and air temperature,  \n# and display the missingness using `geom_miss_point()`\nggplot(oceanbuoys,\n       aes(x = humidity,\n           y = air_temp_c)) + \n  geom_miss_point()"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#using-facets-to-explore-missingness",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#using-facets-to-explore-missingness",
    "title": "34  Testing missing relationships",
    "section": "34.8 Using facets to explore missingness",
    "text": "34.8 Using facets to explore missingness\nBecause geom_miss_point() is a ggplot geom, you can use it with ggplot2 features like faceting.\nThis means we can rapidly explore the missingness and stay within the familiar bounds of ggplot2."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-7",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-7",
    "title": "34  Testing missing relationships",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExplore the missingness in wind and air temperature, and display the missingness using geom_miss_point(). Facet by year to explore this further.\nExplore the missingness in humidity and air temperature, and display the missingness using geom_miss_point(). Facet by year to explore this further.\n\n\n\nE8.R\n\n# Explore the missingness in wind and air temperature, and display the \n# missingness using `geom_miss_point()`. Facet by year to explore this further.\nggplot(oceanbuoys,\n       aes(x = wind_ew,\n           y = air_temp_c)) + \n  geom_miss_point() + \n  facet_wrap(~year)\n\n# Explore the missingness in humidity and air temperature, and display the \n# missingness using `geom_miss_point()` Facet by year to explore this further.\nggplot(oceanbuoys,\n       aes(x = humidity,\n           y = air_temp_c)) + \n  geom_miss_point() + \n  facet_wrap(~year)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#faceting-to-explore-missingness-multiple-plots",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#faceting-to-explore-missingness-multiple-plots",
    "title": "34  Testing missing relationships",
    "section": "34.9 Faceting to explore missingness (multiple plots)",
    "text": "34.9 Faceting to explore missingness (multiple plots)\nAnother useful technique with geommisspoint() is to explore the missingness by creating multiple plots.\nJust as we have done in the previous exercises, we can use the nabular data to help us create additional faceted plots.\nWe can even create multiple faceted plots according to values in the data, such as year, and features of the data, such as missingness."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-8",
    "href": "Dealing_With_Missing_Data_in_R_C3.html#instructions-100-xp-8",
    "title": "34  Testing missing relationships",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse geom_miss_point() and facet_wrap() to explore how the missingness in wind_ew and air_temp_c is different for missingness of humidity.\nUse geom_miss_point() and facet_grid() to explore how the missingness in wind_ew and air_temp_c is different for missingness of humidity and by year.\n\n\n\nE9.R\n\n# Use geom_miss_point() and facet_wrap to explore how the missingness \n# in wind_ew and air_temp_c is different for missingness of humidity\nbind_shadow(oceanbuoys) %>%\n  ggplot(aes(x = wind_ew,\n           y = air_temp_c)) + \n  geom_miss_point() + \n  facet_wrap(~humidity_NA)\n\n# Use geom_miss_point() and facet_grid to explore how the missingness in wind_ew and air_temp_c \n# is different for missingness of humidity AND by year - by using `facet_grid(humidity_NA ~ year)`\nbind_shadow(oceanbuoys) %>%\n  ggplot(aes(x = wind_ew,\n             y = air_temp_c)) + \n  geom_miss_point() + \n  facet_grid(humidity_NA~year)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#impute-data-below-range-with-nabular-data",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#impute-data-below-range-with-nabular-data",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.1 Impute data below range with nabular data",
    "text": "35.1 Impute data below range with nabular data\nWe want to keep track of values we imputed. If we don’t, it is very difficult to assess how good the imputed values are.\nWe are going to practice imputing data and recreate visualizations in the previous set of exercises by imputing values below the range of the data.\nThis is a very useful way to help further explore missingness, and also provides the framework for imputing missing values.\nFirst, we are going to impute the data below the range using impute_below_all(), and then visualize the data. We notice that although we can see where the missing values are in this instance, we need some way to track them. The track missing data programming pattern can help with this."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#visualize-imputed-values-in-a-scatter-plot",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#visualize-imputed-values-in-a-scatter-plot",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.2 Visualize imputed values in a scatter plot",
    "text": "35.2 Visualize imputed values in a scatter plot\nNow, let’s recreate one of the previous plots we saw in chapter three that used geom_miss_point().\nTo do this, we need to impute the data below the range of the data. This is a special kind of imputation to explore the data. This imputation will illustrate what we need to practice: how to track missing values. To impute the data below the range of the data, we use the function impute_below_all()."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#create-histogram-of-imputed-data",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#create-histogram-of-imputed-data",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.3 Create histogram of imputed data",
    "text": "35.3 Create histogram of imputed data\nNow that we can recreate the first visualization of geom_miss_point(), let’s explore how we can apply this to other exploratory tasks.\nOne useful task is to evaluate the number of missings in a given variable using a histogram. We can do this using the ocean_imp_track dataset we created in the last exercise, which is loaded into this session."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-bad-imputations",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-bad-imputations",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.4 Evaluating bad imputations",
    "text": "35.4 Evaluating bad imputations\nIn order to evaluate imputations, it helps to know what something bad looks like. To explore this, let’s look at a typically bad imputation method: imputing using the mean value.\nIn this exercise we are going to explore how the mean imputation method works using a box plot, using the oceanbuoys dataset."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-imputations-the-scale",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-imputations-the-scale",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.5 Evaluating imputations: The scale",
    "text": "35.5 Evaluating imputations: The scale\nWhile the mean imputation might not look so bad when we compare it using a box plot, it is important to get a sense of the variation in the data. This is why it is important to explore how the scale and spread of imputed values changes compared to the data.\nOne way to evaluate the appropriateness of the scale of the imputations is to use a scatter plot to explore whether or not the values are appropriate."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-imputations-across-many-variables",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-imputations-across-many-variables",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.6 Evaluating imputations: Across many variables",
    "text": "35.6 Evaluating imputations: Across many variables\nSo far, we have covered ways to look at individual variables or pairs of variables and their imputed values. However, sometimes you want to look at imputations for many variables. To do this, you need to perform some data munging and re-arranging. This lesson covers how to perform this data wrangling, which can get a little bit hairy when considering its usage in nabular data. The function, shadow_long() gets the data into the right shape for these kinds of visualizations."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#using-simputation-to-impute-data",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#using-simputation-to-impute-data",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.7 Using simputation to impute data",
    "text": "35.7 Using simputation to impute data\nThere are many imputation packages in R. We are going to focus on using the simputation package, which provides a simple, powerful interface into performing imputations.\nBuilding a good imputation model is super important, but it is a complex topic - there is as much to building a good imputation model as there is for building a good statistical model. In this course, we are going to focus on how to evaluate imputations.\nFirst, we are going to look at using impute_lm() function, which imputes values according to a specified linear model.\nIn this exercise, we are going to apply the previous assessment techniques to data with impute_lm(), and then build upon this imputation method in subsequent lessons."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-6",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-6",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the oceanbuoys dataset:\n\nImpute humidity using wind_ew and wind_ns, and track missing values using add_label_shadow().\nPlot the imputed values for air_temp_c and humidity, putting them on the x and y-axis, respectively, and coloring by any_missing().\n\n\n\nE7.R\n\n# Impute humidity and air temperature using wind_ew and wind_ns, and track missing values\nocean_imp_lm_wind <- oceanbuoys %>% \n    bind_shadow() %>%\n    impute_lm(air_temp_c ~ wind_ew + wind_ns) %>% \n    impute_lm(humidity ~ wind_ew + wind_ns) %>%\n    add_label_shadow()\n    \n# Plot the imputed values for air_temp_c and humidity, colored by missingness\nggplot(ocean_imp_lm_wind, \n       aes(x = air_temp_c, y = humidity, color = any_missing)) +\n  geom_point()"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-and-comparing-imputations",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-and-comparing-imputations",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.8 Evaluating and comparing imputations",
    "text": "35.8 Evaluating and comparing imputations\nWhen you build up an imputation model, it’s a good idea to compare it to another method. In this lesson, we are going to compare the previously imputed dataset created using impute_lm() to the mean imputed dataset. Both of these datasets are included in this exercise as ocean_imp_lm_wind and ocean_imp_mean respectively."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-7",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-7",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nBind the models together using bind_rows(), placing the ocean_imp_mean model into mean, and ocean_imp_lm_wind into lm_wind.\nLook at the values of air_temp and humidity as a scatter plot, placing air_temp_c on the x-axis, humidity on the y-axis, color by any missings, and faceting by imputation model used (imp_model).\n\n\n\nE8.R\n\n# Bind the models together \nbound_models <- bind_rows(mean = ocean_imp_mean,\n                          lm_wind = ocean_imp_lm_wind,\n                          .id = \"imp_model\")\n\n# Inspect the values of air_temp and humidity as a scatter plot\nggplot(bound_models,\n       aes(x = air_temp_c,\n           y = humidity,\n           color = any_missing)) +\n  geom_point() + \n  facet_wrap(~imp_model)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-imputations-many-models-variables",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-imputations-many-models-variables",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.9 Evaluating imputations (many models & variables)",
    "text": "35.9 Evaluating imputations (many models & variables)\nWhen you build up an imputation model, it’s a good idea to compare it to another method.\nIn this lesson, we are going to get you to add a final imputation model that contains an extra useful piece of information that helps explain some of the variation in the data. You are then going to compare the values, as previously done in the last lesson."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-8",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-8",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUsing the oceanbuoys dataset:\n\nImpute data using impute_lm(), adding year to the model.\nBind the imputation methods together, placing ocean_imp_mean into mean, ocean_imp_lm_wind into lm_wind, and ocean_imp_lm_wind_year into lm_wind_year.\nLook at the values of air_temp_c (on the x-axis) and humidity (on the y-axis), coloring by any missings, and faceting by imputation model.\n\n\n\nE9.R\n\n# Build a model adding year to the outcome\nocean_imp_lm_wind_year <- bind_shadow(oceanbuoys) %>%\n  impute_lm(air_temp_c ~ wind_ew + wind_ns + year) %>%\n  impute_lm(humidity ~ wind_ew + wind_ns + year) %>%\n  add_label_shadow()\n\n# Bind the mean, lm_wind, and lm_wind_year models together\nbound_models <- bind_rows(mean = ocean_imp_mean,\n                          lm_wind = ocean_imp_lm_wind,\n                          lm_wind_year = ocean_imp_lm_wind_year,\n                          .id = \"imp_model\")\n\n# Explore air_temp and humidity, coloring by any missings, and faceting by imputation model\nggplot(bound_models, aes(x = air_temp_c, y = humidity, color = any_missing)) +\n  geom_point() + facet_wrap(~imp_model)"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#combining-and-comparing-many-imputation-models",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#combining-and-comparing-many-imputation-models",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.10 Combining and comparing many imputation models",
    "text": "35.10 Combining and comparing many imputation models\nTo evaluate the different imputation methods, we need to put them into a single dataframe. Next, you will compare three different approaches to handling missing data using the dataset, oceanbuoys.\nThe first method is using only the completed cases and is loaded as ocean_cc. The second method is imputing values using a linear model with predictions made using wind and is loaded as ocean_imp_lm_wind. You will create the third imputed dataset, ocean_imp_lm_all, using a linear model and impute the variables sea_temp_c, air_temp_c, and humidity using the variables wind_ew, wind_ns, year, latitude, longitude.\nYou will then bind all of the datasets together (ocean_cc, ocean_imp_lm_wind, and ocean_imp_lm_all), calling it bound_models."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-9",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-9",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate an imputed dataset named ocean_imp_lm_all using a linear model and impute the variables sea_temp_c, air_temp_c, and humidity using the variables wind_ew, wind_ns, year, latitude, longitude.\nBind all of the datasets together into the same object, calling it bound_models.\n\n\n\nE10.R\n\n# Create an imputed dataset using a linear models\nocean_imp_lm_all <- bind_shadow(oceanbuoys) %>%\n  add_label_shadow() %>%\n  impute_lm(sea_temp_c ~ wind_ew + wind_ns + year + latitude + longitude) %>%\n  impute_lm(air_temp_c ~ wind_ew + wind_ns + year + latitude + longitude) %>%\n  impute_lm(humidity ~ wind_ew + wind_ns + year + latitude + longitude)\n\n# Bind the datasets\nbound_models <- bind_rows(cc = ocean_cc,\n                          imp_lm_wind = ocean_imp_lm_wind,\n                          imp_lm_all = ocean_imp_lm_all,\n                          .id = \"imp_model\")\n\n# Look at the models\nbound_models"
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-the-different-parameters-in-the-model",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#evaluating-the-different-parameters-in-the-model",
    "title": "35  Connecting the dots (Imputation)",
    "section": "35.11 Evaluating the different parameters in the model",
    "text": "35.11 Evaluating the different parameters in the model\nWe are imputing our data for a reason - we want to analyze the data!\nIn this example, we are interested in predicting sea temperature, so we will build a linear model predicting sea temperature.\nWe will fit this model to each of the datasets we created and then explore the coefficients in the data.\nThe objects from the previous lesson (ocean_cc, ocean_imp_lm_wind, ocean_imp_lm_all, and bound_models) are loaded into the workspace."
  },
  {
    "objectID": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-10",
    "href": "Dealing_With_Missing_Data_in_R_C4.html#instructions-100-xp-10",
    "title": "35  Connecting the dots (Imputation)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate the model summary for each dataset with columns for residuals using residuals, predict, and tidy.\nExplore the coefficients in the model and put the model with the highest estimate for air_temp_c in the object best_model\n\n\n\nE11.R\n\n# Create the model summary for each dataset\nmodel_summary <- bound_models %>% \n  group_by(imp_model) %>%\n  nest() %>%\n  mutate(mod = map(data, ~lm(sea_temp_c ~ air_temp_c + humidity + year, data = .)),\n         res = map(mod, residuals),\n         pred = map(mod, predict),\n         tidy = map(mod, tidy))\n\n# Explore the coefficients in the model\nmodel_summary %>% \n    select(imp_model,tidy) %>%\n    unnest()\nbest_model <- \"imp_lm_all\""
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#r",
    "href": "Cluster_Analysis_in_R_C4.html#r",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "39.1 r",
    "text": "39.1 r"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#instructions-100-xp",
    "href": "Cluster_Analysis_in_R_C4.html#instructions-100-xp",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE1.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#section",
    "href": "Cluster_Analysis_in_R_C4.html#section",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "39.2 ",
    "text": "39.2"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#instructions-100-xp-1",
    "href": "Cluster_Analysis_in_R_C4.html#instructions-100-xp-1",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE2.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#section-1",
    "href": "Cluster_Analysis_in_R_C4.html#section-1",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "39.3 ",
    "text": "39.3"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#instructions-100-xp-2",
    "href": "Cluster_Analysis_in_R_C4.html#instructions-100-xp-2",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE3.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#section-2",
    "href": "Cluster_Analysis_in_R_C4.html#section-2",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "39.4 ",
    "text": "39.4"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#instructions-100-xp-3",
    "href": "Cluster_Analysis_in_R_C4.html#instructions-100-xp-3",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE4.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#section-3",
    "href": "Cluster_Analysis_in_R_C4.html#section-3",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "39.5 ",
    "text": "39.5"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#instructions-100-xp-4",
    "href": "Cluster_Analysis_in_R_C4.html#instructions-100-xp-4",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE5.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#section-4",
    "href": "Cluster_Analysis_in_R_C4.html#section-4",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "39.6 ",
    "text": "39.6"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C4.html#instructions-100-xp-5",
    "href": "Cluster_Analysis_in_R_C4.html#instructions-100-xp-5",
    "title": "39  Case Study: National Occupational Mean Wage",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE6.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#r",
    "href": "Cluster_Analysis_in_R_C1.html#r",
    "title": "36  Calculating Distance Between Observations",
    "section": "36.1 r",
    "text": "36.1 r"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#instructions-100-xp",
    "href": "Cluster_Analysis_in_R_C1.html#instructions-100-xp",
    "title": "36  Calculating Distance Between Observations",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE1.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#section",
    "href": "Cluster_Analysis_in_R_C1.html#section",
    "title": "36  Calculating Distance Between Observations",
    "section": "36.2 ",
    "text": "36.2"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#instructions-100-xp-1",
    "href": "Cluster_Analysis_in_R_C1.html#instructions-100-xp-1",
    "title": "36  Calculating Distance Between Observations",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE2.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#section-1",
    "href": "Cluster_Analysis_in_R_C1.html#section-1",
    "title": "36  Calculating Distance Between Observations",
    "section": "36.3 ",
    "text": "36.3"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#instructions-100-xp-2",
    "href": "Cluster_Analysis_in_R_C1.html#instructions-100-xp-2",
    "title": "36  Calculating Distance Between Observations",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE3.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#section-2",
    "href": "Cluster_Analysis_in_R_C1.html#section-2",
    "title": "36  Calculating Distance Between Observations",
    "section": "36.4 ",
    "text": "36.4"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#instructions-100-xp-3",
    "href": "Cluster_Analysis_in_R_C1.html#instructions-100-xp-3",
    "title": "36  Calculating Distance Between Observations",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE4.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#section-3",
    "href": "Cluster_Analysis_in_R_C1.html#section-3",
    "title": "36  Calculating Distance Between Observations",
    "section": "36.5 ",
    "text": "36.5"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#instructions-100-xp-4",
    "href": "Cluster_Analysis_in_R_C1.html#instructions-100-xp-4",
    "title": "36  Calculating Distance Between Observations",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE5.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#section-4",
    "href": "Cluster_Analysis_in_R_C1.html#section-4",
    "title": "36  Calculating Distance Between Observations",
    "section": "36.6 ",
    "text": "36.6"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C1.html#instructions-100-xp-5",
    "href": "Cluster_Analysis_in_R_C1.html#instructions-100-xp-5",
    "title": "36  Calculating Distance Between Observations",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE6.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#r",
    "href": "Cluster_Analysis_in_R_C2.html#r",
    "title": "37  Hierarchical Clustering",
    "section": "37.1 r",
    "text": "37.1 r"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#instructions-100-xp",
    "href": "Cluster_Analysis_in_R_C2.html#instructions-100-xp",
    "title": "37  Hierarchical Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE1.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#section",
    "href": "Cluster_Analysis_in_R_C2.html#section",
    "title": "37  Hierarchical Clustering",
    "section": "37.2 ",
    "text": "37.2"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#instructions-100-xp-1",
    "href": "Cluster_Analysis_in_R_C2.html#instructions-100-xp-1",
    "title": "37  Hierarchical Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE2.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#section-1",
    "href": "Cluster_Analysis_in_R_C2.html#section-1",
    "title": "37  Hierarchical Clustering",
    "section": "37.3 ",
    "text": "37.3"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#instructions-100-xp-2",
    "href": "Cluster_Analysis_in_R_C2.html#instructions-100-xp-2",
    "title": "37  Hierarchical Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE3.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#section-2",
    "href": "Cluster_Analysis_in_R_C2.html#section-2",
    "title": "37  Hierarchical Clustering",
    "section": "37.4 ",
    "text": "37.4"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#instructions-100-xp-3",
    "href": "Cluster_Analysis_in_R_C2.html#instructions-100-xp-3",
    "title": "37  Hierarchical Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE4.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#section-3",
    "href": "Cluster_Analysis_in_R_C2.html#section-3",
    "title": "37  Hierarchical Clustering",
    "section": "37.5 ",
    "text": "37.5"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#instructions-100-xp-4",
    "href": "Cluster_Analysis_in_R_C2.html#instructions-100-xp-4",
    "title": "37  Hierarchical Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE5.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#section-4",
    "href": "Cluster_Analysis_in_R_C2.html#section-4",
    "title": "37  Hierarchical Clustering",
    "section": "37.6 ",
    "text": "37.6"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C2.html#instructions-100-xp-5",
    "href": "Cluster_Analysis_in_R_C2.html#instructions-100-xp-5",
    "title": "37  Hierarchical Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE6.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#r",
    "href": "Cluster_Analysis_in_R_C3.html#r",
    "title": "38  K-means Clustering",
    "section": "38.1 r",
    "text": "38.1 r"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#instructions-100-xp",
    "href": "Cluster_Analysis_in_R_C3.html#instructions-100-xp",
    "title": "38  K-means Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE1.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#section",
    "href": "Cluster_Analysis_in_R_C3.html#section",
    "title": "38  K-means Clustering",
    "section": "38.2 ",
    "text": "38.2"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#instructions-100-xp-1",
    "href": "Cluster_Analysis_in_R_C3.html#instructions-100-xp-1",
    "title": "38  K-means Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE2.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#section-1",
    "href": "Cluster_Analysis_in_R_C3.html#section-1",
    "title": "38  K-means Clustering",
    "section": "38.3 ",
    "text": "38.3"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#instructions-100-xp-2",
    "href": "Cluster_Analysis_in_R_C3.html#instructions-100-xp-2",
    "title": "38  K-means Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE3.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#section-2",
    "href": "Cluster_Analysis_in_R_C3.html#section-2",
    "title": "38  K-means Clustering",
    "section": "38.4 ",
    "text": "38.4"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#instructions-100-xp-3",
    "href": "Cluster_Analysis_in_R_C3.html#instructions-100-xp-3",
    "title": "38  K-means Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE4.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#section-3",
    "href": "Cluster_Analysis_in_R_C3.html#section-3",
    "title": "38  K-means Clustering",
    "section": "38.5 ",
    "text": "38.5"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#instructions-100-xp-4",
    "href": "Cluster_Analysis_in_R_C3.html#instructions-100-xp-4",
    "title": "38  K-means Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE5.R"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#section-4",
    "href": "Cluster_Analysis_in_R_C3.html#section-4",
    "title": "38  K-means Clustering",
    "section": "38.6 ",
    "text": "38.6"
  },
  {
    "objectID": "Cluster_Analysis_in_R_C3.html#instructions-100-xp-5",
    "href": "Cluster_Analysis_in_R_C3.html#instructions-100-xp-5",
    "title": "38  K-means Clustering",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE6.R"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#null-defaults",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#null-defaults",
    "title": "13  All About Arguments",
    "section": "13.1 NULL defaults",
    "text": "13.1 NULL defaults\nThe cut() function used by cut_by_quantile() can automatically provide sensible labels for each category. The code to generate these labels is pretty complicated, so rather than appearing in the function signature directly, its labels argument defaults to NULL, and the calculation details are shown on the ?cut help page."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp",
    "title": "13  All About Arguments",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUpdate the definition of cut_by_quantile() so that the labels argument defaults to NULL. Remove the labels argument from the call to cut_by_quantile().\n\n\nE1.R\n\n# Set the default for labels to NULL\ncut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels=NULL, \ninterval_type) {\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the labels argument from the call\ncut_by_quantile(\n  n_visits,\n  interval_type = \"(lo, hi]\"\n)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#categorical-defaults",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#categorical-defaults",
    "title": "13  All About Arguments",
    "section": "13.2 Categorical defaults",
    "text": "13.2 Categorical defaults\nWhen cutting up a numeric vector, you need to worry about what happens if a value lands exactly on a boundary. You can either put this value into a category of the lower interval or the higher interval. That is, you can choose your intervals to include values at the top boundary but not the bottom (in mathematical terminology, “open on the left, closed on the right”, or (lo, hi]). Or you can choose the opposite (“closed on the left, open on the right”, or   [lo, hi)). cut_by_quantile() should allow these two choices.\nThe pattern for categorical defaults is:\n\nfunction(cat_arg = c(“choice1”, “choice2”)) { cat_arg <- match.arg(cat_arg)}\n\nFree hint: In the console, type head(rank) to see the start of rank()’s definition, and look at the ties.method argument."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-1",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-1",
    "title": "13  All About Arguments",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nUpdate the signature of cut_by_quantile() so that the interval_type argument can be “(lo, hi   ]” or “  [lo, hi)”. Note the space after each comma. Update the body of cut_by_quantile() to match the interval_type argument. Remove the interval_type argument from the call to cut_by_quantile().\n\n\nE2.R\n\n# Set the categories for interval_type to \"(lo, hi]\" and \"[lo, hi)\"\ncut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels = NULL, \ninterval_type=  c(\"(lo, hi]\", \"[lo, hi)\")) {\n  # Match the interval_type argument\n  interval_type <- match.arg(interval_type)\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n# Remove the interval_type argument from the call\ncut_by_quantile(n_visits)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#harmonic-mean",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#harmonic-mean",
    "title": "13  All About Arguments",
    "section": "13.3 Harmonic mean",
    "text": "13.3 Harmonic mean\nThe harmonic mean is the reciprocal of the arithmetic mean of the reciprocal of the data. That is\nThe harmonic mean is often used to average ratio data. You’ll be using it on the price/earnings ratio of stocks in the Standard and Poor’s 500 index, provided as std_and_poor500. Price/earnings ratio is a measure of how expensive a stock is.\nThe dplyr package is loaded."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-2",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-2",
    "title": "13  All About Arguments",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nLook at std_and_poor500 (you’ll need this later). Write a function, get_reciprocal, to get the reciprocal of an input x. Its only argument should be x, and it should return one over x.\nWrite a function, calc_harmonic_mean(), that calculates the harmonic mean of its only input, x.\nUsing std_and_poor500, group by sector, and summarize to calculate the harmonic mean of the price/earning ratios in the pe_ratio column.\n\n\n\nE3.R\n\n# Look at the Standard and Poor 500 data\nglimpse(std_and_poor500)\n\n# Write a function to calculate the reciprocal\nget_reciprocal <- function(x){\n  1/x \n}\n\n# From previous step\nget_reciprocal <- function(x) {\n  1 / x\n}\n\n# Write a function to calculate the harmonic mean\ncalc_harmonic_mean <- function(x) {\n  x %>%\n    get_reciprocal() %>%\n    mean() %>%\n    get_reciprocal()\n}\n\n# From previous steps\nget_reciprocal <- function(x) {\n  1 / x\n}\ncalc_harmonic_mean <- function(x) {\n  x %>%\n    get_reciprocal() %>%\n    mean() %>%\n    get_reciprocal()\n}\n\nstd_and_poor500 %>% \n  # Group by sector\n  group_by(sector) %>% \n  # Summarize, calculating harmonic mean of P/E ratio\n  summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio))"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#dealing-with-missing-values",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#dealing-with-missing-values",
    "title": "13  All About Arguments",
    "section": "13.4 Dealing with missing values",
    "text": "13.4 Dealing with missing values\nIn the last exercise, many sectors had an NA value for the harmonic mean. It would be useful for your function to be able to remove missing values before calculating.\nRather than writing your own code for this, you can outsource this functionality to mean().\nThe dplyr package is loaded."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-3",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-3",
    "title": "13  All About Arguments",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nModify the signature and body of calc_harmonic_mean() so it has an na.rm argument, defaulting to false, that gets passed to mean().\nUsing std_and_poor500, group by sector, and summarize to calculate the harmonic mean of the price/earning ratios in the pe_ratio column, removing missing values.\n\n\n\nE4.R\n\n# Add an na.rm arg with a default, and pass it to mean()\ncalc_harmonic_mean <- function(x,na.rm = FALSE) {\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm =na.rm) %>%\n    get_reciprocal()\n}\n\n# From previous step\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\nstd_and_poor500 %>% \n  # Group by sector\n  group_by(sector) %>% \n  # Summarize, calculating harmonic mean of P/E ratio\n  summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio,\n   na.rm = TRUE))"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#passing-arguments-with",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#passing-arguments-with",
    "title": "13  All About Arguments",
    "section": "13.5 Passing arguments with …",
    "text": "13.5 Passing arguments with …\nRather than explicitly giving calc_harmonic_mean() and na.rm argument, you can use … to simply “pass other arguments” to mean().\nThe dplyr package is loaded."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-4",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-4",
    "title": "13  All About Arguments",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nReplace the na.rm argument with … in the signature and body of calc_harmonic_mean().\nUsing std_and_poor500, group by sector, and summarize to calculate the harmonic mean of the price/earning ratios in the pe_ratio column, removing missing values.\n\n\n\nE5.R\n\n# Swap na.rm arg for ... in signature and body\ncalc_harmonic_mean <- function(x, ...) {\n  x %>%\n    get_reciprocal() %>%\n    mean(...) %>%\n    get_reciprocal()\n}\ncalc_harmonic_mean(x = c(1, NA, 3, NA, 5))\n\ncalc_harmonic_mean <- function(x, ...) {\n  x %>%\n    get_reciprocal() %>%\n    mean(...) %>%\n    get_reciprocal()\n}\n\nstd_and_poor500 %>% \n  # Group by sector\n  group_by(sector) %>% \n  # Summarize, calculating harmonic mean of P/E ratio\n  summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio,\n   na.rm = TRUE))"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#throwing-errors-with-bad-arguments",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#throwing-errors-with-bad-arguments",
    "title": "13  All About Arguments",
    "section": "13.6 Throwing errors with bad arguments",
    "text": "13.6 Throwing errors with bad arguments\nIf a user provides a bad input to a function, the best course of action is to throw an error letting them know. The two rules are\nThrow the error message as soon as you realize there is a problem (typically at the start of the function). Make the error message easily understandable. You can use the assert_*() functions from assertive to check inputs and throw errors when they fail."
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-5",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-5",
    "title": "13  All About Arguments",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd a line to the body of calc_harmonic_mean() to assert that x is numeric.\nLook at what happens when you pass a character argument to calc_harmonic_mean().\n\n\n\nE6.R\n\nalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  # Assert that x is numeric\n  assert_is_numeric(x)\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\n# See what happens when you pass it strings\ncalc_harmonic_mean(std_and_poor500$sector)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#custom-error-logic",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#custom-error-logic",
    "title": "13  All About Arguments",
    "section": "13.7 Custom error logic",
    "text": "13.7 Custom error logic\nSometimes the assert_*() functions in assertive don’t give the most informative error message. For example, the assertions that check if a number is in a numeric range will tell the user that a value is out of range, but the won’t say why that’s a problem. In that case, you can use the is_*() functions in conjunction with messages, warnings, or errors to define custom feedback.\nThe harmonic mean only makes sense when x has all positive values. (Try calculating the harmonic mean of one and minus one to see why.) Make sure your users know this!"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-6",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#instructions-100-xp-6",
    "title": "13  All About Arguments",
    "section": "13.8 Instructions 100 XP",
    "text": "13.8 Instructions 100 XP\n\nIf any values of x are non-positive (ignoring NAs) then throw an error.\nLook at what happens when you pass a character argument to calc_harmonic_mean().\n\n\n\nE7.R\n\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  assert_is_numeric(x)\n  # Check if any values of x are non-positive\n  if(any(is_non_positive(x), na.rm = TRUE)) {\n    # Throw an error\n    stop(\"x contains non-positive values, so the harmonic mean makes no sense.\")\n  }\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\n# See what happens when you pass it negative numbers\ncalc_harmonic_mean(std_and_poor500$pe_ratio - 20)"
  },
  {
    "objectID": "Introduction_to_Writing_Functions_in_R_C2.html#fixing-function-arguments",
    "href": "Introduction_to_Writing_Functions_in_R_C2.html#fixing-function-arguments",
    "title": "13  All About Arguments",
    "section": "13.9 Fixing function arguments",
    "text": "13.9 Fixing function arguments\nThe harmonic mean function is almost complete. However, you still need to provide some checks on the na.rm argument. This time, rather than throwing errors when the input is in an incorrect form, you are going to try to fix it.\nna.rm should be a logical vector with one element (that is, TRUE, or FALSE).\nThe assertive package is loaded for you.\nInstructions 100 XP\n\nUpdate calc_harmonic_mean() to fix the na.rm argument by using use_first() to select the * first na.rm element, and coerce_to() to change it to logical.\n\n\n\nE8.R\n\n# Update the function definition to fix the na.rm argument\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  assert_is_numeric(x)\n  if(any(is_non_positive(x), na.rm = TRUE)) {\n    stop(\"x contains non-positive values, so the harmonic mean makes no sense.\")\n  }\n  # Use the first value of na.rm, and coerce to logical\n  na.rm <- coerce_to(use_first(na.rm), target_class  = \"logical\")\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\n# See what happens when you pass it malformed na.rm\ncalc_harmonic_mean(std_and_poor500$pe_ratio, na.rm = 1:5)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#not-a-member",
    "href": "Cleaning_Data_in_R_C2.html#not-a-member",
    "title": "25  Categorical and Text Data",
    "section": "25.1 Not a member",
    "text": "25.1 Not a member\nNow that you’ve practiced identifying membership constraint problems, it’s time to fix these problems in a new dataset. Throughout this chapter, you’ll be working with a dataset called sfo_survey, containing survey responses from passengers taking flights from San Francisco International Airport (SFO). Participants were asked questions about the airport’s cleanliness, wait times, safety, and their overall satisfaction.\nThere were a few issues during data collection that resulted in some inconsistencies in the dataset. In this exercise, you’ll be working with the dest_size column, which categorizes the size of the destination airport that the passengers were flying to. A data frame called dest_sizes is available that contains all the possible destination sizes. Your mission is to find rows with invalid dest_sizes and remove them from the data frame.\ndplyr has been loaded and sfo_survey and dest_sizes are available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#instructions-100-xp",
    "href": "Cleaning_Data_in_R_C2.html#instructions-100-xp",
    "title": "25  Categorical and Text Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCount the number of occurrences of each dest_size in sfo_survey.\nUse the correct type of filtering join on the sfo_survey data frame and the dest_sizes data frame to get the rows of sfo_survey with invalid dest_size values.\nGet the id, airline, destination, and dest_size columns.\nUse the correct filtering join on sfo_survey and dest_sizes to get the rows of sfo_survey that have a valid dest_size.\nCount the number of times that each dest_size occurs to make sure there are no invalid values left behind."
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#question",
    "href": "Cleaning_Data_in_R_C2.html#question",
    "title": "25  Categorical and Text Data",
    "section": "Question",
    "text": "Question\nTake a closer look at your output. Which dest_size values appear to violate membership constraints?\nPossible Answers\n“huge”, “Small”, “Large”, and “Hub”.\n“huge”, ” Small “,”Large “, and” Hub”. Respuesta\n“Small”, “Medium”, “Large”, and “Hub”.\n\n\nE1.R\n\n# Count the number of occurrences of dest_size\nsfo_survey %>%\n  count(dest_size)\n  \n\n# Find bad dest_size rows\nsfo_survey %>% \n  # Join with dest_sizes data frame to get bad dest_size rows\n  anti_join(dest_sizes, by = \"dest_size\") %>%\n  # Select id, airline, destination, and dest_size cols\n  select(id, airline, destination, dest_size)\n  \n# Remove bad dest_size rows\nsfo_survey %>% \n  # Join with dest_sizes\n  semi_join(dest_sizes, by = \"dest_size\") %>%\n  # Count the number of each dest_size\n  count(dest_size)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#identifying-inconsistency",
    "href": "Cleaning_Data_in_R_C2.html#identifying-inconsistency",
    "title": "25  Categorical and Text Data",
    "section": "25.2 Identifying inconsistency",
    "text": "25.2 Identifying inconsistency\nIn the video exercise, you learned about different kinds of inconsistencies that can occur within categories, making it look like a variable has more categories than it should.\nIn this exercise, you’ll continue working with the sfo_survey dataset. You’ll examine the dest_size column again as well as the cleanliness column and determine what kind of issues, if any, these two categorical variables face.\ndplyr and is loaded and sfo_survey is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#instructions-100-xp-1",
    "href": "Cleaning_Data_in_R_C2.html#instructions-100-xp-1",
    "title": "25  Categorical and Text Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCount the number of occurrences of each category of the dest_size variable of sfo_survey.\nCount the number of occurrences of each category of the cleanliness variable of sfo_survey."
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#question-1",
    "href": "Cleaning_Data_in_R_C2.html#question-1",
    "title": "25  Categorical and Text Data",
    "section": "Question",
    "text": "Question\nSelect the statement that most accurately describes the categories in the dest_size variable of sfo_survey.\nPossible Answers\nThe categories in dest_size have no inconsistencies.\nThe categories in dest_size have inconsistent capitalization.\nThe categories in dest_size have inconsistent white space.\nThe categories in dest_size have inconsistent capitalization and white space. Respuesta"
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#question-2",
    "href": "Cleaning_Data_in_R_C2.html#question-2",
    "title": "25  Categorical and Text Data",
    "section": "Question",
    "text": "Question\nSelect the statement that most accurately describes the categories in the cleanliness variable of sfo_survey.\nPossible Answers\nThe categories in cleanliness have no inconsistencies.\nThe categories in cleanliness have inconsistent capitalization. Respuesta\nThe categories in cleanliness have inconsistent white space.\nThe categories in cleanliness have inconsistent capitalization and white space.\n\n\nE2.R\n\n# Count dest_size\nsfo_survey %>%\n  count(dest_size)\n\n\n# Count dest_size\nsfo_survey %>%\n  count(dest_size)\n\n# Count cleanliness\nsfo_survey %>%\n  count(cleanliness)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#correcting-inconsistency",
    "href": "Cleaning_Data_in_R_C2.html#correcting-inconsistency",
    "title": "25  Categorical and Text Data",
    "section": "25.3 Correcting inconsistency",
    "text": "25.3 Correcting inconsistency\nNow that you’ve identified that dest_size has whitespace inconsistencies and cleanliness has capitalization inconsistencies, you’ll use the new tools at your disposal to fix the inconsistent values in sfo_survey instead of removing the data points entirely, which could add bias to your dataset if more than 5% of the data points need to be dropped.\ndplyr and stringr are loaded and sfo_survey is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#instructions-100-xp-2",
    "href": "Cleaning_Data_in_R_C2.html#instructions-100-xp-2",
    "title": "25  Categorical and Text Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd a column to sfo_survey called dest_size_trimmed that contains the values in the dest_size column with all leading and trailing whitespace removed.\nAdd another column called cleanliness_lower that contains the values in the cleanliness column converted to all lowercase.\nCount the number of occurrences of each category in dest_size_trimmed.\nCount the number of occurrences of each category in cleanliness_lower.\n\n\n\nE3.R\n\n# Add new columns to sfo_survey\nsfo_survey <- sfo_survey %>%\n  # dest_size_trimmed: dest_size without whitespace\n  mutate(dest_size_trimmed = str_trim(dest_size),\n         # cleanliness_lower: cleanliness converted to lowercase\n         cleanliness_lower = str_to_lower(cleanliness))\n\n# Count values of dest_size_trimmed\nsfo_survey %>%\n  count(dest_size_trimmed)\n\n# Count values of cleanliness_lower\nsfo_survey %>%\n  count(cleanliness_lower)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#collapsing-categories",
    "href": "Cleaning_Data_in_R_C2.html#collapsing-categories",
    "title": "25  Categorical and Text Data",
    "section": "25.4 Collapsing categories",
    "text": "25.4 Collapsing categories\nOne of the tablets that participants filled out the sfo_survey on was not properly configured, allowing the response for dest_region to be free text instead of a dropdown menu. This resulted in some inconsistencies in the dest_region variable that you’ll need to correct in this exercise to ensure that the numbers you report to your boss are as accurate as possible.\ndplyr and forcats are loaded and sfo_survey is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#instructions-100-xp-3",
    "href": "Cleaning_Data_in_R_C2.html#instructions-100-xp-3",
    "title": "25  Categorical and Text Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCount the categories of dest_region\nCreate a vector called europe_categories containing the three values of dest_region that need to be collapsed.\nAdd a new column to sfo_survey called dest_region_collapsed that contains the values from the dest_region column, except the categories stored in europe_categories should be collapsed to Europe.\nCount the categories of dest_region_collapsed."
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#question-3",
    "href": "Cleaning_Data_in_R_C2.html#question-3",
    "title": "25  Categorical and Text Data",
    "section": "Question",
    "text": "Question\nFrom your output from step 1, which categories need to be collapsed?\nPossible Answers\n“EU” and “Europ” need to be collapsed to “Europe”.\n“EU”, “eur”, and “Europ” need to be collapsed to “Europe”. Respuesta\n“East US”, “Midwest US”, and “West US” need to be collapsed to “US”.\n“Asia” and “Central/South America” should be collapsed to “Asia and Central/South America”.\n\n\nE4.R\n\n# Count categories of dest_region\nsfo_survey %>%\n  count(dest_region)\n\n\n \n  \n# Count categories of dest_region\nsfo_survey %>%\n  count(dest_region)\n\n# Categories to map to Europe\neurope_categories <- c(\"EU\", \"eur\", \"Europ\")\n\n# Add a new col dest_region_collapsed\nsfo_survey %>%\n  # Map all categories in europe_categories to Europe\n  mutate(dest_region_collapsed = fct_collapse(dest_region, \n                                              Europe = europe_categories)) %>%\n  # Count categories of dest_region_collapsed\n  count(dest_region_collapsed)"
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#detecting-inconsistent-text-data",
    "href": "Cleaning_Data_in_R_C2.html#detecting-inconsistent-text-data",
    "title": "25  Categorical and Text Data",
    "section": "25.5 Detecting inconsistent text data",
    "text": "25.5 Detecting inconsistent text data\nYou’ve recently received some news that the customer support team wants to ask the SFO survey participants some follow-up questions. However, the auto-dialer that the call center uses isn’t able to parse all of the phone numbers since they’re all in different formats. After some investigation, you found that some phone numbers are written with hyphens (-) and some are written with parentheses ((,)). In this exercise, you’ll figure out which phone numbers have these issues so that you know which ones need fixing.\ndplyr and stringr are loaded, and sfo_survey is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#instructions-100-xp-4",
    "href": "Cleaning_Data_in_R_C2.html#instructions-100-xp-4",
    "title": "25  Categorical and Text Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFilter for rows with phone numbers that contain “-”s.\nFilter for rows with phone numbers that contain “(”, or “)”. Remember to use fixed() when searching for parentheses.\n\n\n\nE5.R\n\n# Filter for rows with \"-\" in the phone column\nsfo_survey %>%\n  filter(str_detect(phone, \"-\"))\n  \n  \n# Filter for rows with \"(\" or \")\" in the phone column\nsfo_survey %>%\n  filter(str_detect(phone, fixed(\"(\")) | str_detect(phone, fixed(\")\")))"
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#replacing-and-removing",
    "href": "Cleaning_Data_in_R_C2.html#replacing-and-removing",
    "title": "25  Categorical and Text Data",
    "section": "25.6 Replacing and removing",
    "text": "25.6 Replacing and removing\nIn the last exercise, you saw that the phone column of sfo_survey is plagued with unnecessary parentheses and hyphens. The customer support team has requested that all phone numbers be in the format “123 456 7890”. In this exercise, you’ll use your new stringr skills to fulfill this request.\ndplyr and stringr are loaded and sfo_survey is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#instructions-100-xp-5",
    "href": "Cleaning_Data_in_R_C2.html#instructions-100-xp-5",
    "title": "25  Categorical and Text Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRemove opening and closing parentheses from the phone column. Store this as a variable called phone_no_parens. Remember to use fixed()!\nAdd a new column to sfo_survey called phone_no_parens that contains the contents of phone_no_parens.\nCreate a new column of sfo_survey called phone_clean containing the values of phone_no_parens with all hyphens replaced with spaces.\n\n\n\nE6.R\n\n# Remove parentheses from phone column\nphone_no_parens <- sfo_survey$phone %>%\n  # Remove \"(\"s\n  str_replace_all(\"\\\\(\", \"\") %>%\n  # Remove \")\"s\n  str_replace_all(\"\\\\)\", \"\")\n  \n\n# Remove parentheses from phone column\nphone_no_parens <- sfo_survey$phone %>%\n  # Remove \"(\"s\n  str_remove_all(fixed(\"(\")) %>%\n  # Remove \")\"s\n  str_remove_all(fixed(\")\"))\n\n# Add phone_no_parens as column\nsfo_survey %>%\n  mutate(phone_no_parens = phone_no_parens)\n  \n  \n\n# Remove parentheses from phone column\nphone_no_parens <- sfo_survey$phone %>%\n  # Remove \"(\"s\n  str_remove_all(fixed(\"(\")) %>%\n  # Remove \")\"s\n  str_remove_all(fixed(\")\"))\n\n# Add phone_no_parens as column\nsfo_survey %>%\n  mutate(phone_no_parens = phone_no_parens,\n  # Replace all hyphens in phone_no_parens with spaces\n         phone_clean = str_replace_all(phone_no_parens,\"-\",\" \"))"
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#invalid-phone-numbers",
    "href": "Cleaning_Data_in_R_C2.html#invalid-phone-numbers",
    "title": "25  Categorical and Text Data",
    "section": "25.7 Invalid phone numbers",
    "text": "25.7 Invalid phone numbers\nThe customer support team is grateful for your work so far, but during their first day of calling participants, they ran into some phone numbers that were invalid. In this exercise, you’ll remove any rows with invalid phone numbers so that these faulty numbers don’t keep slowing the team down.\ndplyr and stringr are loaded and sfo_survey is available."
  },
  {
    "objectID": "Cleaning_Data_in_R_C2.html#instructions-100-xp-6",
    "href": "Cleaning_Data_in_R_C2.html#instructions-100-xp-6",
    "title": "25  Categorical and Text Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExamine the invalid phone numbers by filtering for numbers whose length is not equal to 12.\nRemove the rows with invalid numbers by filtering for numbers with a length of exactly 12.\n\n\n\nE7.R\n\n# Check out the invalid numbers\nsfo_survey %>%\n  filter(str_length(phone) != 12)\n\n# Remove rows with invalid numbers\nsfo_survey %>%\n  filter(str_length(phone) == 12)"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#r",
    "href": "Developing_R_Packages_C1.html#r",
    "title": "36  The R Package Structure",
    "section": "36.1 r",
    "text": "36.1 r"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#instructions-100-xp",
    "href": "Developing_R_Packages_C1.html#instructions-100-xp",
    "title": "36  The R Package Structure",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the create() function to create a new R package called datasummary.\nUse the dir() function to see what files and directories are created in your package.\n\n\n\nE1.R\n\n# Use the create function to set up your first package\ncreate(\"datasummary\")\n\n# Take a look at the files and folders in your package\ndir(\"datasummary\")"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#section",
    "href": "Developing_R_Packages_C1.html#section",
    "title": "36  The R Package Structure",
    "section": "36.2 ",
    "text": "36.2"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#instructions-100-xp-1",
    "href": "Developing_R_Packages_C1.html#instructions-100-xp-1",
    "title": "36  The R Package Structure",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nWrite a function, numeric_summary(), that takes two arguments, a vector x and the logical na.rm.\nInclude a check to your function so it fails with an error message if the vector x is not numeric.\nThe function should return a data.frame containing the minimum, median, standard deviation and maximum values.\nCheck that your function works using the Ozone column of the airquality data and removing missing values.\n\n\n\nE2.R\n\n# Create numeric_summary() function\nnumeric_summary <- function(x, na.rm) {\n\n    # Include an error if x is not numeric\n    if(!is.numeric(x)){\n        stop(\"Data must be numeric\")\n    }\n    \n    # Create data frame\n    data.frame( min = min(x, na.rm = na.rm),\n                median = median(x, na.rm = na.rm),\n                sd = sd(x, na.rm = na.rm),\n                max = max(x, na.rm = na.rm))\n}\n\n# Test numeric_summary() function\nnumeric_summary(airquality$Ozone, na.rm=TRUE)"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#section-1",
    "href": "Developing_R_Packages_C1.html#section-1",
    "title": "36  The R Package Structure",
    "section": "36.3 ",
    "text": "36.3"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#instructions-100-xp-2",
    "href": "Developing_R_Packages_C1.html#instructions-100-xp-2",
    "title": "36  The R Package Structure",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the dir() function again to see what is currently available in the R directory of datasummary.\nUsing the dump() function, write the numeric_summary() function to the R directory, saving the file as numeric_summary.R.\nPrint the contents of the R directory again.\n\n\n\nE3.R\n\n# What is in the R directory before adding a function?\ndir(\"datasummary/R\")\n\n# Use the dump() function to write the numeric_summary function\ndump(\"numeric_summary\", file = \"datasummary/R/numeric_summary.R\")\n\n# Verify that the file is in the correct directory\ndir(\"datasummary/R\")"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#section-2",
    "href": "Developing_R_Packages_C1.html#section-2",
    "title": "36  The R Package Structure",
    "section": "36.4 ",
    "text": "36.4"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#instructions-100-xp-3",
    "href": "Developing_R_Packages_C1.html#instructions-100-xp-3",
    "title": "36  The R Package Structure",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the dir() function, check what directories are currently in the package?\nThe weather dataset has been created for you. Add it to your package.\nAdd a vignette titled “Generating_Summaries_with_Data_Summary”.\n\n\n\nE4.R\n\n# What is in the package at the moment?\ndir(\"datasummary\")\n\n# Add the weather data\nuse_data(weather, pkg = \"datasummary\", overwrite = TRUE)\n\n# Add a vignette called \"Generating Summaries with Data Summary\"\nuse_vignette(\"Generating_Summaries_with_Data_Summary\", pkg = \"datasummary\")\n\n# What directories do you now have in your package now?\ndir(\"datasummary\")"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#section-3",
    "href": "Developing_R_Packages_C1.html#section-3",
    "title": "36  The R Package Structure",
    "section": "36.5 ",
    "text": "36.5"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#instructions-100-xp-4",
    "href": "Developing_R_Packages_C1.html#instructions-100-xp-4",
    "title": "36  The R Package Structure",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSend the new data_summary() function to a new file taking the same name as the function (You should use the dump() function here).\n\n\n\nE5.R\n\ndata_summary <- function(x, na.rm = TRUE){\n  \n  num_data <- select_if(x, .predicate = is.numeric) \n  \n  map_df(num_data, .f = numeric_summary, na.rm = TRUE, .id = \"ID\")\n  \n}\n\n# Write the function to the R directory\ndump(\"data_summary\", file = \"datasummary/R/data_summary.R\")"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#section-4",
    "href": "Developing_R_Packages_C1.html#section-4",
    "title": "36  The R Package Structure",
    "section": "36.6 ",
    "text": "36.6"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#instructions-100-xp-5",
    "href": "Developing_R_Packages_C1.html#instructions-100-xp-5",
    "title": "36  The R Package Structure",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE6.R"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#r",
    "href": "Developing_R_Packages_C2.html#r",
    "title": "37  Documenting Packages",
    "section": "37.1 r",
    "text": "37.1 r"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#instructions-100-xp",
    "href": "Developing_R_Packages_C2.html#instructions-100-xp",
    "title": "37  Documenting Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd the title “Numeric Summaries” to your header.\nAdd the following short description of the function: “Summarises numeric data and returns a data frame containing the minimum value, median, standard deviation, and maximum value.”\n\n\n\nE1.R\n\n# Add a title and description\n#' Numeric Summaries\n#'\n#' Summarises numeric data and returns a data frame containing the minimum value, median, standard deviation, and maximum value.\nnumeric_summary <- function(x, na.rm){\n\n    if(!is.numeric(x)){\n        stop(\"Data must be numeric\")\n    }\n    \n    data.frame( min = min(x, na.rm = na.rm),\n                median = median(x, na.rm = na.rm),\n                sd = sd(x, na.rm = na.rm),\n                max = max(x, na.rm = na.rm))\n}"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#section",
    "href": "Developing_R_Packages_C2.html#section",
    "title": "37  Documenting Packages",
    "section": "37.2 ",
    "text": "37.2"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#instructions-100-xp-1",
    "href": "Developing_R_Packages_C2.html#instructions-100-xp-1",
    "title": "37  Documenting Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd an appropriate tag to document the first argument of the numeric_summary() function.\nAdd the following details to this tag: “a numeric vector containing the values to summarize.”\n\n\n\nE2.R\n\n#' Numeric Summaries\n#'\n#' Summarises numeric data and returns a data frame containing the minimum value, median, standard deviation, and maximum value.\n#'\n# Add appropriate tag and details to document the first argument\n#' @param x a numeric vector containing the values to summarize.\nnumeric_summary <- function(x, na.rm){\n\n    if(!is.numeric(x)){\n        stop(\"data must be numeric\")\n    }\n    \n    data.frame( min = min(x, na.rm = na.rm),\n                median = median(x, na.rm = na.rm),\n                sd = sd(x, na.rm = na.rm),\n                max = max(x, na.rm = na.rm))\n}"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#section-1",
    "href": "Developing_R_Packages_C2.html#section-1",
    "title": "37  Documenting Packages",
    "section": "37.3 ",
    "text": "37.3"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#instructions-100-xp-2",
    "href": "Developing_R_Packages_C2.html#instructions-100-xp-2",
    "title": "37  Documenting Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUpdate the data_summary() function to import packages purrr and dplyr.\n\n\n\nE3.R\n\n#' Summary of Numeric Columns\n#'\n#' Generate specific summaries of numeric columns in a data frame\n#' \n#' @param x A data frame. Non-numeric columns will be removed\n#' @param na.rm A logical indicating whether missing values should be removed\n#' @import dplyr\n#' @import purrr\n#' @importFrom tidyr gather\ndata_summary <- function(x, na.rm = TRUE){\n  \n  num_data <- select_if(x, .predicate = is.numeric) \n  \n  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = \"ID\")\n  \n}"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#section-2",
    "href": "Developing_R_Packages_C2.html#section-2",
    "title": "37  Documenting Packages",
    "section": "37.4 ",
    "text": "37.4"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#instructions-100-xp-3",
    "href": "Developing_R_Packages_C2.html#instructions-100-xp-3",
    "title": "37  Documenting Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd an export tag to the data_summary() function.\n\n\n\nE4.R\n\n#' Summary of Numeric Columns\n#'\n#' Generate specific summaries of numeric columns in a data frame\n#' \n#' @param x A data frame. Non-numeric columns will be removed\n#' @param na.rm A logical indicating whether missing values should be removed\n#' @import dplyr\n#' @import purrr\n#' @importFrom tidyr gather\n#' @export\ndata_summary <- function(x, na.rm = TRUE){\n  \n  num_data <- select_if(x, .predicate = is.numeric) \n  \n  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = \"ID\")\n  \n}"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#section-3",
    "href": "Developing_R_Packages_C2.html#section-3",
    "title": "37  Documenting Packages",
    "section": "37.5 ",
    "text": "37.5"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#instructions-100-xp-4",
    "href": "Developing_R_Packages_C2.html#instructions-100-xp-4",
    "title": "37  Documenting Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd an examples tag to the data_summary() function header.\nAdd an example of running the function using the iris data.\nAdd a second example of running the function with the airquality data, not removing missing values.\n\n\n\nE5.R\n\n#' Data Summary for Numeric Columns\n#'\n#' Custom summaries of numeric data in a provided data frame\n#'\n#' @param x A data.frame containing at least one numeric column\n#' @param na.rm A logical indicating whether missing values should be removed\n#' @import dplyr\n#' @import purrr\n#' @importFrom tidyr gather\n#' @export\n#' @examples\n#' data_summary(iris)\n#' data_summary(airquality, na.rm = FALSE)\ndata_summary <- function(x, na.rm = TRUE){\n  \n  num_data <- select_if(x, .predicate = is.numeric) \n  \n  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = \"ID\")\n  \n}"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#section-4",
    "href": "Developing_R_Packages_C2.html#section-4",
    "title": "37  Documenting Packages",
    "section": "37.6 ",
    "text": "37.6"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#instructions-100-xp-5",
    "href": "Developing_R_Packages_C2.html#instructions-100-xp-5",
    "title": "37  Documenting Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nDocument the return value with the following description: “This function returns a data.frame including columns:”. Format the word data.frame as code.\nAdd 5 bullets for each of the 5 column names in the order they are returned (the function is loaded if you want to run it).\n\n\n\nE6.R\n\n#' Data Summary for Numeric Columns\n#'\n#' Custom summaries of numeric data in a provided data frame\n#'\n#' @param x A data.frame containing at least one numeric column\n#' @param na.rm A logical indicating whether missing values should be removed\n#' @import dplyr\n#' @import purrr\n#' @importFrom tidyr gather\n#' @export\n#' @examples\n#' data_summary(iris)\n#' data_summary(airquality, na.rm = FALSE)\n#'\n## Update the details for the return value\n#' @return This function returns a \\code{data.frame} including columns: \n#' \\itemize{\n#'  \\item ID\n#'  \\item min\n#'  \\item median\n#'  \\item sd\n#'  \\item max\n#' }\n#'\n#' @export\ndata_summary <- function(x, na.rm = TRUE){\n  \n  num_data <- select_if(x, .predicate = is.numeric) \n  \n  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = \"ID\")\n  \n}"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#r",
    "href": "Developing_R_Packages_C3.html#r",
    "title": "38  Checking and Building R Packages",
    "section": "38.1 r",
    "text": "38.1 r"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#instructions-100-xp",
    "href": "Developing_R_Packages_C3.html#instructions-100-xp",
    "title": "38  Checking and Building R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the check() function to check the datasummary package.\n\n\n\nE1.R\n\n# Check your package\ncheck(\"datasummary\")"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#section",
    "href": "Developing_R_Packages_C3.html#section",
    "title": "38  Checking and Building R Packages",
    "section": "38.2 ",
    "text": "38.2"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#instructions-100-xp-1",
    "href": "Developing_R_Packages_C3.html#instructions-100-xp-1",
    "title": "38  Checking and Building R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUpdate the roxygen header for the function “numeric_summary” to document the missing parameter (na.rm).\nInclude the following description for the parameter “a logical value indicating whether NA values should be stripped before the computation proceeds.”\n\n\n\nE2.R\n\n#' Numeric Summaries\n#' Summarises numeric data and returns a data frame containing the minimum value, median, standard deviation, and maximum value.\n#'\n#' @param x a numeric vector containing the values to summarize.\n#' @param na.rm \"a logical value indicating whether NA values should be stripped before the computation proceeds.\"\n  \nnumeric_summary <- function(x, na.rm){\n\n  if(!is.numeric(x)){\n    stop(\"data must be numeric\")\n  }\n\n  data.frame( min = min(x, na.rm = na.rm),\n              median = median(x, na.rm = na.rm),\n              sd = sd(x, na.rm = na.rm),\n              max = max(x, na.rm = na.rm))\n}"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#section-1",
    "href": "Developing_R_Packages_C3.html#section-1",
    "title": "38  Checking and Building R Packages",
    "section": "38.3 ",
    "text": "38.3"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#instructions-100-xp-2",
    "href": "Developing_R_Packages_C3.html#instructions-100-xp-2",
    "title": "38  Checking and Building R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nA new function, get_mean_temp(), is available in your workspace. Take a look at what it does by running get_mean_temp (with no brackets) in the console. Update the package-level documentation so this function can be successfully added to your package without causing check() to fail because of “undefined global variables”.\n\n\nE3.R\n\n#' datasummary: Custom Data Summaries\n#'\n#' Easily generate custom data frame summaries\n#'\n#' @docType package\n#' @name datasummary\n\"_PACKAGE\"\n\n# Update this function call\nutils::globalVariables(c(\"weather\", \"Temp\"))\n\nget_mean_temp"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#section-2",
    "href": "Developing_R_Packages_C3.html#section-2",
    "title": "38  Checking and Building R Packages",
    "section": "38.4 ",
    "text": "38.4"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#instructions-100-xp-3",
    "href": "Developing_R_Packages_C3.html#instructions-100-xp-3",
    "title": "38  Checking and Building R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE4.R\n\n# Add dplyr as an imported dependency to the DESCRIPTION file\nuse_package(\"dplyr\", pkg = \"datasummary\")\n\n# Add purrr as an imported dependency to the DESCRIPTION file\nuse_package(\"purrr\", pkg = \"datasummary\")\n\n# Add tidyr as an imported dependency to the DESCRIPTION file\nuse_package(\"tidyr\", pkg = \"datasummary\")"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#section-3",
    "href": "Developing_R_Packages_C3.html#section-3",
    "title": "38  Checking and Building R Packages",
    "section": "38.5 ",
    "text": "38.5"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#instructions-100-xp-4",
    "href": "Developing_R_Packages_C3.html#instructions-100-xp-4",
    "title": "38  Checking and Building R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nBuild the source version of the datasummary package.\nLook at the contents of the current directory using dir() to see the built version of your package.\n\n\n\nE5.R\n\n# Build the package\nbuild(\"datasummary\")\n\n# Examine the contents of the current directory\ndir()"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#section-4",
    "href": "Developing_R_Packages_C3.html#section-4",
    "title": "38  Checking and Building R Packages",
    "section": "38.6 ",
    "text": "38.6"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#instructions-100-xp-5",
    "href": "Developing_R_Packages_C3.html#instructions-100-xp-5",
    "title": "38  Checking and Building R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE6.R"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#r",
    "href": "Developing_R_Packages_C4.html#r",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.1 r",
    "text": "39.1 r"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#instructions-100-xp",
    "href": "Developing_R_Packages_C4.html#instructions-100-xp",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSet up the test framework for your datasummary package.\nLook at the contents of the package root directory.\nLook at the contents of the new folder which has been created.\n\n\n\nE1.R\n\n# Set up the test framework\nuse_testthat(\"datasummary\")\n\n# Look at the contents of the package root directory\ndir(\"datasummary\")\n\n# Look at the contents of the new folder which has been created \ndir(\"datasummary/tests\")"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#section",
    "href": "Developing_R_Packages_C4.html#section",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.2 ",
    "text": "39.2"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#instructions-100-xp-1",
    "href": "Developing_R_Packages_C4.html#instructions-100-xp-1",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCall data_summary() on iris and assign the result to iris_summary.\nAssign the number of rows in iris_summary to summary_rows.\nUse the function expect_equal() to test whether the result of calling data_summary() on the iris dataset returns 4 rows.\n\n\n\nE2.R\n\n# Create a summary of the iris dataset using your data_summary() function\niris_summary &lt;- data_summary(iris)\n\n# Count how many rows are returned\nsummary_rows &lt;- nrow(iris_summary) \n\n# Use expect_equal to test that calling data_summary() on iris returns 4 rows\nexpect_equal(summary_rows, 4)"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#section-1",
    "href": "Developing_R_Packages_C4.html#section-1",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.3 ",
    "text": "39.3"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#instructions-100-xp-2",
    "href": "Developing_R_Packages_C4.html#instructions-100-xp-2",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUpdate the first test below so that the test passes; do this by adjusting the tolerance parameter.\nWrite a test comparing expected_result and result which does not produce an error.\n\n\n\nE3.R\n\nresult &lt;- data_summary(weather)\n\n# Update this test so it passes\nexpect_equal(result$sd, c(2.1, 3.6), tolerance = 0.1)\n\nexpected_result &lt;- list(\n    ID = c(\"Day\", \"Temp\"),\n    min = c(1L, 14L),\n    median = c(4L, 19L),\n    sd = c(2.16024689946929, 3.65148371670111),\n    max = c(7L, 24L)\n)\n\n# Write a passing test that compares expected_result to result\nexpect_equivalent(result, expected_result)"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#section-2",
    "href": "Developing_R_Packages_C4.html#section-2",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.4 ",
    "text": "39.4"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#instructions-100-xp-3",
    "href": "Developing_R_Packages_C4.html#instructions-100-xp-3",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCall data_summary() on my_vector.\nPick an appropriate expect_*() function such that when you call data_summary() on my_vector again, the test passes.\n\n\n\nE4.R\n\n# Create a vector containing the numbers 1 through 10\nmy_vector &lt;- 1:10\n\n# Look at what happens when we apply this vector as an argument to data_summary()\ndata_summary(my_vector)\n\n# Test if running data_summary() on this vector returns an error\nexpect_error(data_summary(my_vector))"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#section-3",
    "href": "Developing_R_Packages_C4.html#section-3",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.5 ",
    "text": "39.5"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#instructions-100-xp-4",
    "href": "Developing_R_Packages_C4.html#instructions-100-xp-4",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCall data_summary() on airquality dataset with na.rm set to FALSE and see what happens.\nPick an appropriate expect_*() function such that when you call data_summary() on airquality with na.rm set to FALSE again, the test passes.\n\n\n\nE5.R\n\n# Run data_summary on the airquality dataset with na.rm set to FALSE\ndata_summary(airquality, na.rm = FALSE)\n\n# Use expect_warning to formally test this\nexpect_warning(data_summary(airquality, na.rm = FALSE))"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#section-4",
    "href": "Developing_R_Packages_C4.html#section-4",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.6 ",
    "text": "39.6"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#instructions-100-xp-5",
    "href": "Developing_R_Packages_C4.html#instructions-100-xp-5",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCall the non-exported function numeric_summary() on the Temp column of the weather dataset and assign it to result.\nPick an appropriate expect_*() function which tests expected and result are exactly equal.\n\n\n\nE6.R\n\n# Expected result\nexpected &lt;- data.frame(min = 14L, median = 19L, sd = 3.65148371670111, max = 24L)\n\n# Create variable result by calling numeric summary on the temp column of the weather dataset\nresult &lt;- datasummary:::numeric_summary(weather$Temp, na.rm = TRUE)\n\n# Test that the value returned matches the expected value\nexpect_equal(result, expected)"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#the-structure-of-an-r-package",
    "href": "Developing_R_Packages_C1.html#the-structure-of-an-r-package",
    "title": "36  The R Package Structure",
    "section": "36.1 The structure of an R package",
    "text": "36.1 The structure of an R package\nYou can use devtools to create the basic structure of an R package by using the create() function.\nThere are some optional arguments to this function but the main one that you will use is the path argument. You use this to specify where your package will be created and the name that your package will take.\nIf you want to create the package in your current working directory, as you often will, you just need to supply the name for the package. When naming your package remember to think about:\nIf the name is already taken by another package. Whether the name makes it clear what the package does. devtools is loaded in your workspace."
  },
  {
    "objectID": "Developing_R_Packages_C1.html#contents-of-an-r-package",
    "href": "Developing_R_Packages_C1.html#contents-of-an-r-package",
    "title": "36  The R Package Structure",
    "section": "36.2 Contents of an R package",
    "text": "36.2 Contents of an R package\nWhen you create an R package there are a number of files and folders that it can contain. This can include folders to store data, user guides (known as vignettes), tests (more on these later in the course) and demos among others. As a minimum there are only two directories that must be included along with two additional files – the DESCRIPTION and the NAMESPACE.\nOf the two compulsory directories one contains all of the user guides for your functions. As you will see later in the course this is created in a special way for you. The other is included by the create() function. Which of the following is the second compulsory directory?"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#answers-question-50-xp",
    "href": "Developing_R_Packages_C1.html#answers-question-50-xp",
    "title": "36  The R Package Structure",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\nPossible Answers\n\ndata\ntests\ninst\nR Respuesta"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#writing-a-simple-function",
    "href": "Developing_R_Packages_C1.html#writing-a-simple-function",
    "title": "36  The R Package Structure",
    "section": "36.3 Writing a simple function",
    "text": "36.3 Writing a simple function\nWhilst there are packages that contain only data, typically packages are created to collect together functions for performing a specific task. If you need a refresher on writing functions you might want to review the course Writing Functions in R.\nFor your package you are going to keep the functions simple. You are going to create a package that produces custom summary output for your data."
  },
  {
    "objectID": "Developing_R_Packages_C1.html#including-functions-in-a-package",
    "href": "Developing_R_Packages_C1.html#including-functions-in-a-package",
    "title": "36  The R Package Structure",
    "section": "36.4 Including functions in a package",
    "text": "36.4 Including functions in a package\nOnce you have written your function code you need to save it in the R directory of your package. Typically you can do that by saving an R script in the usual manner (i.e. “Save As”).\nIn the instance that you already have objects created, as you did in the last exercise, that you want to write to the R directory you can do this programmatically. You can use the function dump() to send a named R function to a particular file. The two arguments that you need to pass to this function are the name of the R object, as a character string, and the path to the file that you want to create, including the extension .R.\nThe package datasummary has already been created, along with the function numeric_summary() and is available in your workspace."
  },
  {
    "objectID": "Developing_R_Packages_C1.html#package-names",
    "href": "Developing_R_Packages_C1.html#package-names",
    "title": "36  The R Package Structure",
    "section": "36.5 Package names",
    "text": "36.5 Package names\nWhen you name your package you have to consider a number of things, from whether the name is already taken to if it makes it clear to users what the package might do. You also have to consider naming conventions. Regardless of whether you are putting your package on CRAN or not you should take the same care when you name your packages.\nSuppose you were to create a new package that takes some data and tidies it into a format that you can work with further. Which of the following names would be recommended?"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#answers-question-50-xp-1",
    "href": "Developing_R_Packages_C1.html#answers-question-50-xp-1",
    "title": "36  The R Package Structure",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\n\ntidyr\ntidy-data\ntidy_data\ntidydata Respuesta"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#writing-a-description-file",
    "href": "Developing_R_Packages_C1.html#writing-a-description-file",
    "title": "36  The R Package Structure",
    "section": "36.6 Writing a DESCRIPTION file",
    "text": "36.6 Writing a DESCRIPTION file\nThe DESCRIPTION file is used to provide information about the package. A template file is created for you by devtools that you can edit appropriately for your package. Which of the following items are NOT included in the DESCRIPTION file?"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#answers-question-50-xp-2",
    "href": "Developing_R_Packages_C1.html#answers-question-50-xp-2",
    "title": "36  The R Package Structure",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\n\nAuthor and maintainer details\nFunctions that the package contains Respuesta\nPackage dependencies\nLicense"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#detailing-authors-maintainers-and-contributors",
    "href": "Developing_R_Packages_C1.html#detailing-authors-maintainers-and-contributors",
    "title": "36  The R Package Structure",
    "section": "36.7 Detailing authors, maintainers and contributors",
    "text": "36.7 Detailing authors, maintainers and contributors\nOften a package is not the work of just one person but contributions from a number of people. This could be fellow authors, someone who has contributed elements of code to the package such as a bug fix, a thesis advisor or many, many more!\nWhich of the following roles would you include in the Authors@R field to describe the package maintainer?"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#answers-question-50-xp-3",
    "href": "Developing_R_Packages_C1.html#answers-question-50-xp-3",
    "title": "36  The R Package Structure",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\n\ncph\nctb\ncre Respuesta\naut"
  },
  {
    "objectID": "Developing_R_Packages_C1.html#the-use_-functions",
    "href": "Developing_R_Packages_C1.html#the-use_-functions",
    "title": "36  The R Package Structure",
    "section": "36.8 The use_* functions",
    "text": "36.8 The use_* functions\nBeyond the required structure you can include a number of additional directories containing elements such as vignettes (user guides), data and unit tests. The devtools package makes it really simple for you to add to the package structure by providing a series of use_* functions. For example, use_data() and use_vignette(). Note that when adding vignettes, it’s best not to include any spaces in the vignette name.\nWhen you are adding data you need to provide the name of the data object along with the argument pkg, giving the path to the package that you want to put your data in.\ndevtools is loaded in your workspace."
  },
  {
    "objectID": "Developing_R_Packages_C1.html#best-practice-for-structuring-code",
    "href": "Developing_R_Packages_C1.html#best-practice-for-structuring-code",
    "title": "36  The R Package Structure",
    "section": "36.9 Best practice for structuring code",
    "text": "36.9 Best practice for structuring code\nA typical R package contains a number of functions that you need to maintain. Whilst there are no strict rules around how you should structure code in a package you generally want to avoid having all of your code in a single script. As you can’t have sub-directories you also need to think carefully about how you name the file so that you can find your code again in the future.\nSuppose you were to write another function for your package that takes all numeric columns in your data and returns a data frame of all of their summary statistics. What would be the best way to structure this code?"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#a-simple-function-header",
    "href": "Developing_R_Packages_C2.html#a-simple-function-header",
    "title": "37  Documenting Packages",
    "section": "37.1 A simple function header",
    "text": "37.1 A simple function header\nThe roxygen headers are included in the same script as the function code. You use roxygen comments #’ to identify a line as part of the roxygen header. You can include regular R comments in the header if you wish, using the usual comment character, #.\nThe first three lines of the header have special meaning and you don’t need to use tags to identify them. The first three lines are:\ntitle description details Since you don’t mention any tags to identify the first three lines, it is necessary that you seperate each one of them by a new line. For example:\n\n#’ Title goes here #’ #’ Description goes here #’ #’ Details go here"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#documenting-function-arguments",
    "href": "Developing_R_Packages_C2.html#documenting-function-arguments",
    "title": "37  Documenting Packages",
    "section": "37.2 Documenting function arguments",
    "text": "37.2 Documenting function arguments\nOne of the most important aspects of a function to document are the arguments.\nWith roxygen you do this with the (param?) tag. You follow the tag with the argument name and then any details the user needs to know.\nIf there is a lot of information that you need to provide about an argument you might want to consider putting this in the details section instead.\nNormally you must document every argument of your function, but in this exercise you’ll just document the first argument and come back to the second one later!"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#importing-other-packages",
    "href": "Developing_R_Packages_C2.html#importing-other-packages",
    "title": "37  Documenting Packages",
    "section": "37.3 Importing other packages",
    "text": "37.3 Importing other packages\nIf you wish to use functions from other packages, you should import them in your roxygen header. You can use (import?) to import an entire package, or (importFrom?) to import a single function. To show you how (importFrom?) works, we have imported the gather() function from the tidyr package for you (although it’s not needed in this case)."
  },
  {
    "objectID": "Developing_R_Packages_C2.html#export-best-practice",
    "href": "Developing_R_Packages_C2.html#export-best-practice",
    "title": "37  Documenting Packages",
    "section": "37.4 Export best practice",
    "text": "37.4 Export best practice\nExporting a function makes it available for the end users of your package. Functions that are not exported are not directly available to your end users. It’s a really useful way to hide low level functions that you write to break up the main functionality.\nIt can often be tricky to decide which functions should be exported. Take a look at the descriptions below. Which one would you export?"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#answers-question-50-xp",
    "href": "Developing_R_Packages_C2.html#answers-question-50-xp",
    "title": "37  Documenting Packages",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\n\nUtility function to calculate the median absolute deviation.\nHigh level function for calculating all summaries of supplied data. Respuesta\nNot intended for end users."
  },
  {
    "objectID": "Developing_R_Packages_C2.html#exporting-functions",
    "href": "Developing_R_Packages_C2.html#exporting-functions",
    "title": "37  Documenting Packages",
    "section": "37.5 Exporting functions",
    "text": "37.5 Exporting functions\nAlthough the NAMESPACE file is used to identify functions that are exported you use roxygen tags to create this file. This makes it really easy to see, right next to the function, if your users will be able to work with it or not.\nWe mark a function for exporting using the tag (export?)."
  },
  {
    "objectID": "Developing_R_Packages_C2.html#adding-examples",
    "href": "Developing_R_Packages_C2.html#adding-examples",
    "title": "37  Documenting Packages",
    "section": "37.6 Adding examples",
    "text": "37.6 Adding examples\nExamples are the way that your end users will learn how to work with your code. You should provide at least one example for any function that is exported. If there are any tricky arguments to your function you might also want to consider examples for those.\nExamples should ideally be easily reproducible and should not cause an error. If you are going to publish your package to CRAN then examples should also run in a reasonable time, CRAN sets this to around 15 minutes but it’s worth double checking the recommendations at the time of submission.\nYou add examples using the (examples?) tag. Examples will typically span multiple lines. This is fine as roxygen will assume that everything is part of the examples until it finds another tag."
  },
  {
    "objectID": "Developing_R_Packages_C2.html#documenting-function-return-values",
    "href": "Developing_R_Packages_C2.html#documenting-function-return-values",
    "title": "37  Documenting Packages",
    "section": "37.7 Documenting function return values",
    "text": "37.7 Documenting function return values\nYou document the return value of a function using the tag (return?). This is where you can tell users what they can expect to get from the function, be that data, a graphic or any other output.\nWhen you write documentation you may want to format the text to look like code, link to other functions or, particularly for return objects, format as a bulleted list. You can do this using special formatting:\nFor code you use To link to other functions you use , although note the package name is only required if the function is not in your package To include an unordered list you use . Inside the brackets you mark new items with\nfollowed by the item text."
  },
  {
    "objectID": "Developing_R_Packages_C2.html#additional-documentation",
    "href": "Developing_R_Packages_C2.html#additional-documentation",
    "title": "37  Documenting Packages",
    "section": "37.8 Additional documentation",
    "text": "37.8 Additional documentation\nBeyond the functions arguments and return values there are many additional items that can be provided in function help files. From who wrote that function to other functions that you might be interested in using. We will also see later how we can identify dependencies using the roxygen header.\nOther tags that we can use include:\n(author?) to identify who wrote the function. (seealso?) to list other functions that may be of interest to users. (notes?) to add any other notes relating to the function (e.g. if its experimental, likely to change etc.)"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#instructions-100-xp-6",
    "href": "Developing_R_Packages_C2.html#instructions-100-xp-6",
    "title": "37  Documenting Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd in the author of the data_summary() function as “My Name myemail@example.com”.\nList the summary() function (from the base package) after adding the author.\n\n\n\nE7.R\n\n#' Summary of Numeric Columns\n#' Generate specific summaries of numeric columns in a data frame\n#'\n#' @param x A data frame. Non-numeric columns will be removed\n#' @param na.rm A logical indicating whether missing values should be removed\n#' @import dplyr\n#' @import purrr\n#' @importFrom tidyr gather\n#' @export\n#' @examples\n#' data_summary(iris)\n#' data_summary(airquality, na.rm = FALSE)\n#' \n#' @return This function returns a \\code{data.frame} including columns: \n#' \\itemize{\n#'  \\item ID\n#'  \\item min\n#'  \\item median\n#'  \\item sd\n#'  \\item max\n#' }\n#'\n## Add in the author of the `data_summary()` function\n#' @author My Name <myemail@example.com>\n## List the `summary()` function (from the `base` package)\n#' @seealso \\link[base]{summary}\ndata_summary <- function(x, na.rm = TRUE){\n  \n  num_data <- select_if(x, .predicate = is.numeric) \n  \n  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = \"ID\")\n  \n}"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#minimum-level-of-documentation",
    "href": "Developing_R_Packages_C2.html#minimum-level-of-documentation",
    "title": "37  Documenting Packages",
    "section": "37.9 Minimum level of documentation",
    "text": "37.9 Minimum level of documentation\nWhen producing documentation for a function there are certain things that you MUST provide. Which of the following is not required?"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#answers-question-50-xp-1",
    "href": "Developing_R_Packages_C2.html#answers-question-50-xp-1",
    "title": "37  Documenting Packages",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\n\nFunction title\nDocumentation of all function arguments\nDocumentation of the return value Respuesta"
  },
  {
    "objectID": "Developing_R_Packages_C2.html#adding-package-documentation",
    "href": "Developing_R_Packages_C2.html#adding-package-documentation",
    "title": "37  Documenting Packages",
    "section": "37.10 Adding package documentation",
    "text": "37.10 Adding package documentation\nIn addition to the individual functions you can also document the whole package.\nWhen you document a package you can use all of the same tags as for functions but the problem that you have is that there is no R code to document. Instead you put the keyword “_PACKAGE” underneath your package header.\nYou should also use the (docType?) and (name?) tags to indicate that this is package level documentation, and specify your package name.\nYou should then save the package documentation in the R directory in a file ending with .R with the same name as the package."
  },
  {
    "objectID": "Developing_R_Packages_C2.html#instructions-100-xp-7",
    "href": "Developing_R_Packages_C2.html#instructions-100-xp-7",
    "title": "37  Documenting Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nWrite a package header for your datasummary package with the title “Custom Data Summaries”.\nInclude the description “Easily generate custom data frame summaries”.\nAdd the relevant tags required for package level documentation.\nEnsure that the “_PACKAGE” keyword is added in the appropriate location.\n\n\n\nE8.R\n\n#' datasummary: Custom Data Summaries\n#'\n#' Easily generate custom data frame summaries\n#'\n#' @docType package\n#' @name datasummary\n\"_PACKAGE\""
  },
  {
    "objectID": "Developing_R_Packages_C2.html#documenting-data-objects",
    "href": "Developing_R_Packages_C2.html#documenting-data-objects",
    "title": "37  Documenting Packages",
    "section": "37.11 Documenting data objects",
    "text": "37.11 Documenting data objects\nIf you include a data set in your package you must also provide documentation for it. As with package level documentation, you should include a title and description.\nWith a data set, once again, there is no function object to document. You instead give the name of the dataset as a character string, for instance “airquality”. Additionally there are two tags that you need to use:\n\n(format?) to describe the format of the data. This is often used with the describe format.\n(source?) to identify where the data came from.\n\nYou should then save the package documentation in the R directory with a suitable name (in the above case, either airquality.R or data.R).\nThe weather dataset is available in your workspace. Run print(weather) to view it."
  },
  {
    "objectID": "Developing_R_Packages_C2.html#instructions-100-xp-8",
    "href": "Developing_R_Packages_C2.html#instructions-100-xp-8",
    "title": "37  Documenting Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUpdate the (format?) tag to include the correct number of rows and columns in weather.\nAdd descriptions of all columns in the weather dataset that you added to your package in the last chapter.\nAdd a (source?) tag to and describe the data as “Randomly generated data”.\n\n\n\nE9.R\n\n#' Random Weather Data\n#'\n#' A dataset containing randomly generated weather data.\n#'\n#' @format A data frame of 7 rows and 3 columns\n#' \\describe{\n#'  \\item{Day}{Numeric values giving day of the week, 1 = Monday, 7 = Sunday}\n#'  \\item{Temp}{Numeric values giving temperature in degrees Celsius}\n#'  \\item{Weather}{Character values describing the weather on that day}\n#' }\n#' @source Randomly generated data\n\"weather\""
  },
  {
    "objectID": "Developing_R_Packages_C2.html#creating-man-files",
    "href": "Developing_R_Packages_C2.html#creating-man-files",
    "title": "37  Documenting Packages",
    "section": "37.12 Creating man files",
    "text": "37.12 Creating man files\nOnce you’ve created your documentation using roxygen headers, you need to build your documentation. You can use the document() function from the devtools package to generate your documentation, supplying the path to the package as the first argument. The generated documentation will appear in the man directory. Once you’ve documented your package, you can view the help files just like you can with any of your existing packages."
  },
  {
    "objectID": "Developing_R_Packages_C2.html#instructions-100-xp-9",
    "href": "Developing_R_Packages_C2.html#instructions-100-xp-9",
    "title": "37  Documenting Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGenerate documentation for your package.\nLook at the contents of the man directory.\nView the documentation for the data_summary() function.\nView the documentation for the weather dataset.\n\n\n\nE10.R\n\n# Generate package documentation\ndocument(\"datasummary\")\n\n# Examine the contents of the man directory\ndir(\"datasummary/man\")\n\n# View the documentation for the data_summary function\nhelp(\"data_summary\")\n\n# View the documentation for the weather dataset\nhelp(\"weather\")"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#what-does-a-check-check",
    "href": "Developing_R_Packages_C3.html#what-does-a-check-check",
    "title": "38  Checking and Building R Packages",
    "section": "38.1 What does a “check” check?",
    "text": "38.1 What does a “check” check?\nIn the video, we talked about a number of things that running a check tests for, but which of the following does a check not test:"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#answers-question-50-xp",
    "href": "Developing_R_Packages_C3.html#answers-question-50-xp",
    "title": "38  Checking and Building R Packages",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\n\nAny vignettes can be built\nAny unit tests in the package pass\nCode runs as expected Respuesta\nYou have documented all your function arguments"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#running-a-check",
    "href": "Developing_R_Packages_C3.html#running-a-check",
    "title": "38  Checking and Building R Packages",
    "section": "38.2 Running a check",
    "text": "38.2 Running a check\nBefore a package can be made available on CRAN it is required to pass a series of checks that can be run from the command line using R CMD check. Even if you don’t intend to make a package available on CRAN, it is good practice to run this and ensure that your package passes all of the checks. Just like all other build features this is simplified in devtools with the function check().\nWhen you run check(), if the argument cran is set to TRUE, you run exactly the same checks which are run when you submit a package to CRAN. As the cran argument has a default value of TRUE, you do not need to do change it unless you do not wish to run all of the CRAN checks."
  },
  {
    "objectID": "Developing_R_Packages_C3.html#undocumented-parameters",
    "href": "Developing_R_Packages_C3.html#undocumented-parameters",
    "title": "38  Checking and Building R Packages",
    "section": "38.3 Undocumented parameters",
    "text": "38.3 Undocumented parameters\nIf you’ve forgotten to document any of the parameters, when you run the check() function, you’ll get a WARNING message that looks a bit like this:\n\nUndocumented arguments in documentation object ‘numeric_summary’ ‘na.rm’\n\nNote: you wouldn’t normally get this error for non-exported functions\nTo remove this warning, you’ll need to update the documentation for the parameter in the function’s .R file, and then run check() again. You might think you need to run the document() function again. However, there’s no need to do this, as check() automatically runs document() for you before completing its checks."
  },
  {
    "objectID": "Developing_R_Packages_C3.html#undefined-global-variables",
    "href": "Developing_R_Packages_C3.html#undefined-global-variables",
    "title": "38  Checking and Building R Packages",
    "section": "38.4 Undefined global variables",
    "text": "38.4 Undefined global variables\nThe way in which you define variables in tidyverse package functions can cause confusion for the R CMD check, which sees column names and the name of your dataset, and flags them as “undefined global variables”.\nTo get around this, you can manually specify the data and its columns as a vector to utils::globalVariables(), by including a line of code similar to the following in your package-level documentation:\n\nutils::globalVariables(c(“dataset_name”, “col_name_1”, “col_name_2”))\n\nThis defines dataset_name, col_name_1, and col_name_2 as global variables, and now you shouldn’t get the undefined global variables error."
  },
  {
    "objectID": "Developing_R_Packages_C3.html#depends-or-imports",
    "href": "Developing_R_Packages_C3.html#depends-or-imports",
    "title": "38  Checking and Building R Packages",
    "section": "38.5 Depends or imports?",
    "text": "38.5 Depends or imports?\nThe Depends and Imports fields in the DESCRIPTION file can cause a lot of confusion to those new to package building! Both of these fields contain package dependencies which are installed when you install the package. However, the difference between them is that the packages listed in depends are attached when the package is loaded, whereas the packages listed in imports are not.\nThis distinction is important because if two different packages have an identically named function, the version of the function from the package which has been loaded most recently will be used. Therefore, to make sure you are using the correct function, it is best practice to use imports to list your dependencies and then in your code explicitly name the package and function it’s from in the form package::function(), e.g. dplyr::select().\nIn the majority of cases, you should only list the version of R required in the Depends field and the package in the Imports field.\nWhich of the following should you add to the Imports field in your description file for the datasummary package?"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#answers-question-50-xp-1",
    "href": "Developing_R_Packages_C3.html#answers-question-50-xp-1",
    "title": "38  Checking and Building R Packages",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\n\nR (>= 3.4.3)\ndplyr Respuesta\nstats::filter\ndplyr::filter"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#adding-a-dependency",
    "href": "Developing_R_Packages_C3.html#adding-a-dependency",
    "title": "38  Checking and Building R Packages",
    "section": "38.6 Adding a dependency",
    "text": "38.6 Adding a dependency\nIn which field is it recommended that you list all of the packages that your package relies on, so they are also installed when your package is installed?"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#answers-question-50-xp-2",
    "href": "Developing_R_Packages_C3.html#answers-question-50-xp-2",
    "title": "38  Checking and Building R Packages",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\n\nDepends\nSuggests\nPackages\nImports Respuesta"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#building-an-r-package",
    "href": "Developing_R_Packages_C3.html#building-an-r-package",
    "title": "38  Checking and Building R Packages",
    "section": "38.7 Building an R package",
    "text": "38.7 Building an R package\nOnce you have created the correct structure, included all of your functions, created the package documentation and ensured that the checks pass you can then build your package so that it is in a shareable format.\nYou can build your package using the build() function from devtools. You can use this function to build either a source version of the package, or a Windows/Mac specific binary version. The source version of the package will have a file ending of “.tar.gz” and the binary will take the ending “.zip” (Windows) or “.tgz” (Mac). You can only build binaries for the current platform that you’re using. The build() function builds the source version of the package by default."
  },
  {
    "objectID": "Developing_R_Packages_C3.html#setting-a-package-up-for-using-travis",
    "href": "Developing_R_Packages_C3.html#setting-a-package-up-for-using-travis",
    "title": "38  Checking and Building R Packages",
    "section": "38.8 Setting a package up for using Travis",
    "text": "38.8 Setting a package up for using Travis\nYou can run use_github() and use_travis() to set up your package for use with GitHub and Travis CI. GitHub is a popular website used for storing code and version control, and Travis CI is used for continuous integration, but what actually is continuous integration?"
  },
  {
    "objectID": "Developing_R_Packages_C3.html#answers-question-50-xp-3",
    "href": "Developing_R_Packages_C3.html#answers-question-50-xp-3",
    "title": "38  Checking and Building R Packages",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\n\nAnother way of storing your code\nA way of maintaining multiple versions of your code\nA way of writing tests for your code\nA way of running checks every time you update your code Respuesta"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#setting-up-the-test-structure",
    "href": "Developing_R_Packages_C4.html#setting-up-the-test-structure",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.1 Setting up the test structure",
    "text": "39.1 Setting up the test structure\nYou can set up a test framework in a package using the function use_testthat().\nThis will create a tests directory that contains:\n\n\nA script testthat.R.\nA directory testthat.\n\n\nYou save your tests in the tests/testthat/ directory in files with filenames beginning with test-. So, for example, the simutils package has tests named:\n\n\ntest-na_counter.R\ntest-sample_from_data.R\n\n\nThere are no other strict rules governing the filenames, but you may find it easier to keep track of which functions you are testing if you name your tests after the functions like in the examples above. Alternatively, you can name your tests after areas of package functionality, for example, putting tests for multiple summary functions in test-summaries.R."
  },
  {
    "objectID": "Developing_R_Packages_C4.html#writing-an-individual-test",
    "href": "Developing_R_Packages_C4.html#writing-an-individual-test",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.2 Writing an individual test",
    "text": "39.2 Writing an individual test\nYou create individual tests within your test files using functions named with the pattern expect_* . To make your code easier to read, you may want to create the object to be tested (and/or the expected value, if there is one) before you call the expect_* function.\nHere is one of the tests from the simutils package.\n\nair_expected &lt;- c(Ozone = 37, Solar.R = 7, Wind = 0, Temp = 0, Month = 0, Day = 0)\n\n\nexpect_equal(na_counter(airquality), air_expected)\n\nThe expect_* functions differ in the number of parameters, but the first parameter is always the object being tested.\nWhen you run your tests, you might notice that there is no output. You will only see an output message if the test has failed."
  },
  {
    "objectID": "Developing_R_Packages_C4.html#testing-for-equality",
    "href": "Developing_R_Packages_C4.html#testing-for-equality",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.3 Testing for equality",
    "text": "39.3 Testing for equality\nYou can use expect_equal(), expect_equivalent() and expect_identical() in order to test whether the output of a function is as expected.\nThese three functions all have slightly different functionality:\nexpect_identical() checks that the values, attributes, and type of both objects are the same. expect_equal() checks that the values, and attributes of both objects are the same. You can adjust how strict expect_equal() is by adjusting the tolerance parameter. expect_equivalent() checks that the values, of both objects are the same."
  },
  {
    "objectID": "Developing_R_Packages_C4.html#testing-errors",
    "href": "Developing_R_Packages_C4.html#testing-errors",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.4 Testing errors",
    "text": "39.4 Testing errors\nYou can use expect_error() to test if running a function returns an error. If the function returns an error, the test will pass, otherwise, it will fail. You can optionally define the error message that should be returned to ensure that you are testing for the correct error."
  },
  {
    "objectID": "Developing_R_Packages_C4.html#testing-warnings",
    "href": "Developing_R_Packages_C4.html#testing-warnings",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.5 Testing warnings",
    "text": "39.5 Testing warnings\nYou can use expect_warning() to test if the output of a function also returns a warning. If the function returns a warning, the test will pass, otherwise, it will fail. You can optionally define the warning message that should be returned to ensure that you are testing for the correct warning.\nYour data_summary() function has been updated to issue a warning if na.rm is set to FALSE and if the data contains missing values."
  },
  {
    "objectID": "Developing_R_Packages_C4.html#testing-non-exported-functions",
    "href": "Developing_R_Packages_C4.html#testing-non-exported-functions",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.6 Testing non-exported functions",
    "text": "39.6 Testing non-exported functions\nAs only exported functions are loaded when tests are being run, you can test non-exported functions by referring to them using the package name, followed by three colons, and then the function name."
  },
  {
    "objectID": "Developing_R_Packages_C4.html#grouping-tests",
    "href": "Developing_R_Packages_C4.html#grouping-tests",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.7 Grouping tests",
    "text": "39.7 Grouping tests\nSo far, you’ve been using expect_*() functions to create individual tests. To run tests in packages you need to group these individual tests together. You do this using a function test_that(). You can use this to group together expectations that test specific functionality.\nYou can use context() to collect these groups together. You usually have one context per file. An advantage of doing this is that it makes it easier to work out where failing tests are located."
  },
  {
    "objectID": "Developing_R_Packages_C4.html#instructions-100-xp-6",
    "href": "Developing_R_Packages_C4.html#instructions-100-xp-6",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse context() to set the context to “Test data_summary()”.\nUse test_that() to group the tests below together, giving them the description “data_summary() handles errors correctly”.\n\n\n\nE7.R\n\n# Use context() and test_that() to group the tests below together\ncontext(\"Test data_summary()\")\n\ntest_that(\"data_summary() handles errors correctly\", {\n\n  # Create a vector\n  my_vector &lt;- 1:10\n\n  # Use expect_error()\n  expect_error(data_summary(my_vector))\n\n  # Use expect_warning()\n  expect_warning(data_summary(airquality, na.rm = FALSE))\n\n})"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#executing-unit-tests",
    "href": "Developing_R_Packages_C4.html#executing-unit-tests",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.8 Executing unit tests",
    "text": "39.8 Executing unit tests\nWith your tests scripts saved in the package structure you can always easily re-run your tests using the test() function in devtools. This function looks for all tests located in the tests/testhat or inst/tests directory with filenames beginning with test- and ending in .R, and executes each of them. As with the other devtools functions, you supply the path to the package as the first argument to the test() function."
  },
  {
    "objectID": "Developing_R_Packages_C4.html#instructions-100-xp-7",
    "href": "Developing_R_Packages_C4.html#instructions-100-xp-7",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nRun the tests on the datasummary package.\n\n\nE8.R\n\n# Run the tests on the datasummary package\ntest(\"datasummary\")"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#understanding-a-test-failure",
    "href": "Developing_R_Packages_C4.html#understanding-a-test-failure",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "39.9 Understanding a test failure",
    "text": "39.9 Understanding a test failure\nIn the video, we talked about a number of things you can do if you find a failing test, such as updating the test or changing the function. But, what’s the first thing you should do if you find a failing test?"
  },
  {
    "objectID": "Developing_R_Packages_C4.html#answers-question-50-xp",
    "href": "Developing_R_Packages_C4.html#answers-question-50-xp",
    "title": "39  Adding Unit Tests to R Packages",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP\n\nUpdate the test so it passes\nUpdate the R code so the test passes\nWork out the source of the test failure Respuesta\nBang your head against your desk"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C1.html#mean-and-median",
    "href": "Introduction_to_Statistics_in_R_C1.html#mean-and-median",
    "title": "40  Summary Statistics",
    "section": "40.1 Mean and median",
    "text": "40.1 Mean and median\nIn this chapter, you’ll be working with the 2018 Food Carbon Footprint Index from nu3. The food_consumption dataset contains information about the kilograms of food consumed per person per year in each country in each food category (consumption) as well as information about the carbon footprint of that food category (co2_emissions) measured in kilograms of carbon dioxide, or CO , per person per year in each country.\nIn this exercise, you’ll compute measures of center to compare food consumption in the US and Belgium using your dplyr skills.\ndplyr is loaded for you and food_consumption is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C1.html#instructions-100-xp",
    "href": "Introduction_to_Statistics_in_R_C1.html#instructions-100-xp",
    "title": "40  Summary Statistics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate two data frames: one that holds the rows of food_consumption for “Belgium” and the another that holds rows for “USA”. Call these belgium_consumption and usa_consumption. Calculate the mean and median of kilograms of food consumed per person per year for both countries.\nFilter food_consumption for rows with data about Belgium and the USA. Group the filtered data by country. Calculate the mean and median of the kilograms of food consumed per person per year in each country. Call these columns mean_consumption and median_consumption.\n\n\n\nE1.R\n\n# Filter for Belgium\nbelgium_consumption <- food_consumption %>%\n  filter(country == \"Belgium\")\n\n# Filter for USA\nusa_consumption <- food_consumption %>%\n  filter(country ==\"USA\")\n\n# Calculate mean and median consumption in Belgium\nmean(belgium_consumption$consumption)\nmedian(belgium_consumption$consumption)\n\n# Calculate mean and median consumption in USA\nmean(usa_consumption$consumption)\nmedian(usa_consumption$consumption)\n\n\nfood_consumption %>%\n  # Filter for Belgium and USA\n  filter(country %in% c(\"Belgium\", \"USA\")) %>%\n  # Group by country\n  group_by(country) %>%\n  # Get mean_consumption and median_consumption\n  summarise(mean_consumption = mean(consumption),\n      median_consumption = median(consumption))"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C1.html#mean-vs.-median",
    "href": "Introduction_to_Statistics_in_R_C1.html#mean-vs.-median",
    "title": "40  Summary Statistics",
    "section": "40.2 Mean vs. median",
    "text": "40.2 Mean vs. median\nIn the video, you learned that the mean is the sum of all the data points divided by the total number of data points, and the median is the middle value of the dataset where 50% of the data is less than the median, and 50% of the data is greater than the median. In this exercise, you’ll compare these two measures of center.\ndplyr and ggplot2 are loaded and food_consumption is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C1.html#instructions-100-xp-1",
    "href": "Introduction_to_Statistics_in_R_C1.html#instructions-100-xp-1",
    "title": "40  Summary Statistics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFilter food_consumption to get the rows where food_category is “rice”.\nCreate a histogram using ggplot2 of co2_emission for rice.\nFilter food_consumption to get the rows where food_category is “rice”.\nSummarize the data to get the mean and median of co2_emission, calling them mean_co2 and median_co2.\n\n\n\nE2.R\n\nfood_consumption %>%\n  # Filter for rice food category\n  filter(food_category == \"rice\") %>%\n  # Create histogram of co2_emission\n  ggplot(aes(x = co2_emission)) +\n    geom_histogram()\n    \nfood_consumption %>%\n  # Filter for rice food category\n  filter(food_category == \"rice\") %>%\n  # Create histogram of co2_emission\n  ggplot(aes(co2_emission)) +\n    geom_histogram()\n\nfood_consumption %>%\n  # Filter for rice food category\n  filter(food_category == \"rice\") %>% \n  # Get mean_co2 and median_co2\n  summarize(mean_co2 = mean(co2_emission),\n            median_co2 = median(co2_emission))"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C1.html#quartiles-quantiles-and-quintiles",
    "href": "Introduction_to_Statistics_in_R_C1.html#quartiles-quantiles-and-quintiles",
    "title": "40  Summary Statistics",
    "section": "40.3 Quartiles, quantiles, and quintiles",
    "text": "40.3 Quartiles, quantiles, and quintiles\nQuantiles are a great way of summarizing numerical data since they can be used to measure center and spread, as well as to get a sense of where a data point stands in relation to the rest of the dataset. For example, you might want to give a discount to the 10% most active users on a website.\nIn this exercise, you’ll calculate quartiles, quintiles, and deciles, which split up a dataset into 4, 5, and 10 pieces, respectively.\nThe dplyr package is loaded and food_consumption is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C1.html#instructions-100-xp-2",
    "href": "Introduction_to_Statistics_in_R_C1.html#instructions-100-xp-2",
    "title": "40  Summary Statistics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the quartiles of the co2_emission column of food_consumption.\nCalculate the six quantiles that split up the data into 5 pieces (quintiles) of the co2_emission column of food_consumption.\nCalculate the eleven quantiles of co2_emission that split up the data into ten pieces (deciles).\n\n\n\nE3.R\n\n# Calculate the quartiles of co2_emission\nquantile(food_consumption$co2_emission)\n\n# Calculate the quintiles of co2_emission\nquantile(food_consumption$co2_emission, probs =c(0, 0.2, 0.4, 0.6, 0.8, 1))\n\n# Calculate the deciles of co2_emission\nquantile(food_consumption$co2_emission, probs =c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1))"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C1.html#variance-and-standard-deviation",
    "href": "Introduction_to_Statistics_in_R_C1.html#variance-and-standard-deviation",
    "title": "40  Summary Statistics",
    "section": "40.4 Variance and standard deviation",
    "text": "40.4 Variance and standard deviation\nVariance and standard deviation are two of the most common ways to measure the spread of a variable, and you’ll practice calculating these in this exercise. Spread is important since it can help inform expectations. For example, if a salesperson sells a mean of 20 products a day, but has a standard deviation of 10 products, there will probably be days where they sell 40 products, but also days where they only sell one or two. Information like this is important, especially when making predictions.\nBoth dplyr and ggplot2 are loaded, and food_consumption is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C1.html#instructions-100-xp-3",
    "href": "Introduction_to_Statistics_in_R_C1.html#instructions-100-xp-3",
    "title": "40  Summary Statistics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the variance and standard deviation of co2_emission for each food_category by grouping by and summarizing variance as var_co2 and standard deviation as sd_co2.\nCreate a histogram of co2_emission for each food_category using facet_wrap().\n\n\n\nE4.R\n\n# Calculate variance and sd of co2_emission for each food_category\nfood_consumption %>% \n  group_by(food_category) %>% \n  summarize(var_co2 = var(co2_emission),\n           sd_co2 = sd(co2_emission))\n\n# Create subgraphs for each food_category: histogram of co2_emission\nggplot(food_consumption, aes(co2_emission)) +\n  # Create a histogram\n  geom_histogram() +\n  # Create a separate sub-graph for each food_category\n  facet_wrap(~ food_category)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C1.html#finding-outliers-using-iqr",
    "href": "Introduction_to_Statistics_in_R_C1.html#finding-outliers-using-iqr",
    "title": "40  Summary Statistics",
    "section": "40.5 Finding outliers using IQR",
    "text": "40.5 Finding outliers using IQR\nOutliers can have big effects on statistics like mean, as well as statistics that rely on the mean, such as variance and standard deviation. Interquartile range, or IQR, is another way of measuring spread that’s less influenced by outliers. IQR is also often used to find outliers. If a value is less than Q3 + 1.5 x IQR or greater than Q3 + 1.5 x IQR, it’s considered an outlier. In fact, this is how the lengths of the whiskers in a ggplot2 box plot are calculated. In this exercise, you’ll calculate IQR and use it to find some outliers. Both dplyr and ggplot2 are loaded and food_consumption is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C1.html#instructions-100-xp-4",
    "href": "Introduction_to_Statistics_in_R_C1.html#instructions-100-xp-4",
    "title": "40  Summary Statistics",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the total co2_emission per country by grouping by country and taking the sum of co2_emission. Call the sum total_emission and store the resulting data frame as emissions_by_country.\nCompute the first and third quartiles of total_emission and store these as q1 and q3.\nCalculate the interquartile range of total_emission and store it as iqr.\nCalculate the lower and upper cutoffs for outliers of total_emission, and store these as lower and upper.\nUse filter() to get countries with a total_emission greater than the upper cutoff or a total_emission less than the lower cutoff.\n\n\n\nE5.R\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country <- food_consumption %>%\n  group_by(country) %>%\n  summarize(total_emission = sum(co2_emission))\n\nemissions_by_country\n\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country <- food_consumption %>%\n  group_by(country) %>%\n  summarize(total_emission = sum(co2_emission))\n\n# Compute the first and third quartiles and IQR of total_emission\nq1 <- quantile(emissions_by_country$total_emission, probs = 0.25)\nq3 <- quantile(emissions_by_country$total_emission, probs = 0.75)\niqr <- q3 - q1\n\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country <- food_consumption %>%\n  group_by(country) %>%\n  summarize(total_emission = sum(co2_emission))\n\n# Compute the first and third quartiles and IQR of total_emission\nq1 <- quantile(emissions_by_country$total_emission, 0.25)\nq3 <- quantile(emissions_by_country$total_emission, 0.75)\niqr <- q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower <- quantile(emissions_by_country$total_emission, 0.25) - 1.5 * iqr\nupper <- quantile(emissions_by_country$total_emission, 0.75) + 1.5 * iqr\n\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country <- food_consumption %>%\n  group_by(country) %>%\n  summarize(total_emission = sum(co2_emission))\n\n# Compute the first and third quartiles and IQR of total_emission\nq1 <- quantile(emissions_by_country$total_emission, 0.25)\nq3 <- quantile(emissions_by_country$total_emission, 0.75)\niqr <- q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower <- q1 - 1.5 * iqr\nupper <- q3 + 1.5 * iqr\n\n# Filter emissions_by_country to find outliers\nemissions_by_country %>%\n  filter(total_emission < lower | total_emission > upper )"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#calculating-probabilities",
    "href": "Introduction_to_Statistics_in_R_C2.html#calculating-probabilities",
    "title": "41  Random Numbers and Probability",
    "section": "41.1 Calculating probabilities",
    "text": "41.1 Calculating probabilities\nYou’re in charge of the sales team, and it’s time for performance reviews, starting with Amir. As part of the review, you want to randomly select a few of the deals that he’s worked on over the past year so that you can look at them more deeply. Before you start selecting deals, you’ll first figure out what the chances are of selecting certain deals.\nRecall that the probability of an event can be calculated by\ndplyr is loaded and amir_deals is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp",
    "href": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp",
    "title": "41  Random Numbers and Probability",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCount the number of deals Amir worked on for each product type.\nCreate a new column called prob by dividing n by the total number of deals Amir worked on.\n\n\n\nE1.R\n\n# Count the deals for each product\namir_deals %>%\n  count(product)\n  \n\n# Calculate probability of picking a deal with each product\namir_deals %>%\n  count(product) %>%\n  mutate(prob = n/sum(n))"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#sampling-deals",
    "href": "Introduction_to_Statistics_in_R_C2.html#sampling-deals",
    "title": "41  Random Numbers and Probability",
    "section": "41.2 Sampling deals",
    "text": "41.2 Sampling deals\nIn the previous exercise, you counted the deals Amir worked on. Now it’s time to randomly pick five deals so that you can reach out to each customer and ask if they were satisfied with the service they received. You’ll try doing this both with and without replacement.\nAdditionally, you want to make sure this is done randomly and that it can be reproduced in case you get asked how you chose the deals, so you’ll need to set the random seed before sampling from the deals.\ndplyr is loaded and amir_deals is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-1",
    "href": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-1",
    "title": "41  Random Numbers and Probability",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSet the random seed to 31.\nTake a sample of 5 deals without replacement.\nTake a sample of 5 deals with replacement.\n\n\n\nE2.R\n\n\n# Set random seed to 31\nset.seed(31)\n\n# Sample 5 deals without replacement\namir_deals %>%\nsample_n(5)\n\n\n# Set random seed to 31\nset.seed(31)\n\n# Sample 5 deals with replacement\namir_deals %>%\n  sample_n(5, replace = TRUE)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#creating-a-probability-distribution",
    "href": "Introduction_to_Statistics_in_R_C2.html#creating-a-probability-distribution",
    "title": "41  Random Numbers and Probability",
    "section": "41.3 Creating a probability distribution",
    "text": "41.3 Creating a probability distribution\nA new restaurant opened a few months ago, and the restaurant’s management wants to optimize its seating space based on the size of the groups that come most often. On one night, there are 10 groups of people waiting to be seated at the restaurant, but instead of being called in the order they arrived, they will be called randomly. In this exercise, you’ll investigate the probability of groups of different sizes getting picked first. Data on each of the ten groups is contained in the restaurant_groups data frame.\nRemember that expected value can be calculated by multiplying each possible outcome with its corresponding probability and taking the sum. The restaurant_groups data is available and dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-2",
    "href": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-2",
    "title": "41  Random Numbers and Probability",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a histogram of the group_size column of restaurant_groups, setting the number of bins to 5.\nCount the number of each group_size in restaurant_groups, then add a column called probability that contains the probability of randomly selecting a group of each size. Store this in a new data frame called size_distribution.\nCalculate the expected value of the size_distribution, which represents the expected group size.\nCalculate the probability of randomly picking a group of 4 or more people by filtering and summarizing.\n\n\n\nE3.R\n\n# Create a histogram of group_size\nggplot(restaurant_groups, aes(group_size)) +\n  geom_histogram(bins = 5)\n  \n# Create probability distribution\nsize_distribution <- restaurant_groups %>%\n  # Count number of each group size\n  count(group_size) %>%\n  # Calculate probability\n  mutate(probability = n / sum(n))\n\nsize_distribution\n\n\n# Create probability distribution\nsize_distribution <- restaurant_groups %>%\n  count(group_size) %>%\n  mutate(probability = n / sum(n))\n\n# Calculate expected group size\nexpected_val <- sum(size_distribution$group_size *\n                    size_distribution$probability)\nexpected_val\n\n\n# Create probability distribution\nsize_distribution <- restaurant_groups %>%\n  count(group_size) %>%\n  mutate(probability = n / sum(n))\n\n# Calculate probability of picking group of 4 or more\nsize_distribution %>%\n  # Filter for groups of 4 or larger\n  filter(group_size >= 4) %>%\n  # Calculate prob_4_or_more by taking sum of probabilities\n   summarize(prob_4_or_more = sum(probability))"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#data-back-ups",
    "href": "Introduction_to_Statistics_in_R_C2.html#data-back-ups",
    "title": "41  Random Numbers and Probability",
    "section": "41.4 Data back-ups",
    "text": "41.4 Data back-ups\nThe sales software used at your company is set to automatically back itself up, but no one knows exactly what time the back-ups happen. It is known, however, that back-ups happen exactly every 30 minutes. Amir comes back from sales meetings at random times to update the data on the client he just met with. He wants to know how long he’ll have to wait for his newly-entered data to get backed up. Use your new knowledge of continuous uniform distributions to model this situation and answer Amir’s questions."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-3",
    "href": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-3",
    "title": "41  Random Numbers and Probability",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nTo model how long Amir will wait for a back-up using a continuous uniform distribution, save his lowest possible wait time as min and his longest possible wait time as max. Remember that back-ups happen every 30 minutes.\nCalculate the probability that Amir has to wait less than 5 minutes, and store in a variable called prob_less_than_5.\nCalculate the probability that Amir has to wait more than 5 minutes, and store in a variable called prob_greater_than_5.\nCalculate the probability that Amir has to wait between 10 and 20 minutes, and store in a variable called prob_between_10_and_20.\n\n\n\nE4.R\n\n# Min and max wait times for back-up that happens every 30 min\nmin <- 0\nmax <- 30\n\n# Min and max wait times for back-up that happens every 30 min\nmin <- 0\nmax <- 30\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 <- punif(5, min, max)\nprob_less_than_5\n\n# Min and max wait times for back-up that happens every 30 min\nmin <- 0\nmax <- 30\n\n# Calculate probability of waiting more than 5 mins\nprob_greater_than_5 <- punif(5, min, max, lower.tail = FALSE)\nprob_greater_than_5\n\n\n# Min and max wait times for back-up that happens every 30 min\nmin <- 0\nmax <- 30\n\n# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 <- punif(20, min, max) - punif(10, min, max)\nprob_between_10_and_20"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#simulating-wait-times",
    "href": "Introduction_to_Statistics_in_R_C2.html#simulating-wait-times",
    "title": "41  Random Numbers and Probability",
    "section": "41.5 Simulating wait times",
    "text": "41.5 Simulating wait times\nTo give Amir a better idea of how long he’ll have to wait, you’ll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Recall from the last exercise that his minimum wait time is 0 minutes and his maximum wait time is 30 minutes.\nA data frame called wait_times is available and dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-4",
    "href": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-4",
    "title": "41  Random Numbers and Probability",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSet the random seed to 334.\nGenerate 1000 wait times from the continuous uniform distribution that models Amir’s wait time. Add this as a new column called time in the wait_times data frame.\nCreate a histogram of the simulated wait times with 30 bins.\n\n\n\nE5.R\n\n# Set random seed to 334\nset.seed(334)\n\n# Set random seed to 334\nset.seed(334)\n\n# Generate 1000 wait times between 0 and 30 mins, save in time column\nwait_times %>%\n   mutate(time = runif(1000, min = 0, max = 30))\n   \n# Set random seed to 334\nset.seed(334)\n\n# Generate 1000 wait times between 0 and 30 mins, save in time column\nwait_times %>%\n  mutate(time = runif(1000, min = 0, max = 30)) %>%\n  # Create a histogram of simulated times\n  ggplot(aes(time)) +\n  geom_histogram(bins = 30)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#simulating-sales-deals",
    "href": "Introduction_to_Statistics_in_R_C2.html#simulating-sales-deals",
    "title": "41  Random Numbers and Probability",
    "section": "41.6 Simulating sales deals",
    "text": "41.6 Simulating sales deals\nAssume that Amir usually works on 3 deals per week, and overall, he wins 30% of deals he works on. Each deal has a binary outcome: it’s either lost, or won, so you can model his sales deals with a binomial distribution. In this exercise, you’ll help Amir simulate a year’s worth of his deals so he can better understand his performance."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-5",
    "href": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-5",
    "title": "41  Random Numbers and Probability",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSet the random seed to 10 and simulate a single deal.\nSimulate a typical week of Amir’s deals, or one week of 3 deals.\nSimulate a year’s worth of Amir’s deals, or 52 weeks of 3 deals each, and store in deals.\nCalculate the mean number of deals he won per week.\n\n\n\nE6.R\n\n\n# Set random seed to 10\nset.seed(10)\n\n# Simulate a single deal\nrbinom(1, 1, 0.3)\n\n# Set random seed to 10\nset.seed(10)\n\n# Simulate 1 week of 3 deals\nrbinom(1, 3, 0.3)\n\n# Set random seed to 10\nset.seed(10)\n\n# Simulate 52 weeks of 3 deals\ndeals <- rbinom(52, 3, 0.3)\n\n# Calculate mean deals won per week\nmean(deals)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#calculating-binomial-probabilities",
    "href": "Introduction_to_Statistics_in_R_C2.html#calculating-binomial-probabilities",
    "title": "41  Random Numbers and Probability",
    "section": "41.7 Calculating binomial probabilities",
    "text": "41.7 Calculating binomial probabilities\nJust as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you’ll calculate what the chances are of him closing different numbers of deals using the binomial distribution."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-6",
    "href": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-6",
    "title": "41  Random Numbers and Probability",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nWhat’s the probability that Amir closes all 3 deals in a week?\nWhat’s the probability that Amir closes 1 or fewer deals in a week?\nWhat’s the probability that Amir closes more than 1 deal?\n\n\n\nE7.R\n\n# Probability of closing 3 out of 3 deals\ndbinom(3, 3, 0.3)\n\n# Probability of closing <= 1 deal out of 3 deals\npbinom(1, 3, 0.3)\n\n# Probability of closing > 1 deal out of 3 deals\n\npbinom(1, 3, 0.3, lower.tail=FALSE)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#how-many-sales-will-be-won",
    "href": "Introduction_to_Statistics_in_R_C2.html#how-many-sales-will-be-won",
    "title": "41  Random Numbers and Probability",
    "section": "41.8 How many sales will be won?",
    "text": "41.8 How many sales will be won?\nNow Amir wants to know how many deals he can expect to close each week if his win rate changes. Luckily, you can use your binomial distribution knowledge to help him calculate the expected value in different situations. Recall from the video that the expected value of a binomial distribution can be calculated by"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-7",
    "href": "Introduction_to_Statistics_in_R_C2.html#instructions-100-xp-7",
    "title": "41  Random Numbers and Probability",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the expected number of sales out of the 3 he works on that Amir will win each week if he maintains his 30% win rate.\nCalculate the expected number of sales out of the 3 he works on that he’ll win if his win rate drops to 25%.\nCalculate the expected number of sales out of the 3 he works on that he’ll win if his win rate rises to 35%.\n\n\n\nE8.R\n\n\n# Expected number won with 30% win rate\nwon_30pct <- 3 * 0.3\nwon_30pct\n\n# Expected number won with 25% win rate\nwon_25pct <- 3 * 0.25\nwon_25pct\n\n# Expected number won with 35% win rate\nwon_35pct <- 3 * 0.35\nwon_35pct"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#distribution-of-amirs-sales",
    "href": "Introduction_to_Statistics_in_R_C3.html#distribution-of-amirs-sales",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "42.1 Distribution of Amir’s sales",
    "text": "42.1 Distribution of Amir’s sales\nSince each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals As part of Amir’s performance review, you want to be able to estimate the probability of him selling different amounts, but before you can do this, you’ll need to determine what kind of distribution the amount variable follows.\nBoth dplyr and ggplot2 are loaded and amir_deals is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp",
    "href": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a histogram with 10 bins to visualize the distribution of the amount.\n\n\n\nE1.R\n\n\n# Histogram of amount with 10 bins\nggplot(amir_deals, aes(amount)) +\n  geom_histogram(bins = 10)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#probabilities-from-the-normal-distribution",
    "href": "Introduction_to_Statistics_in_R_C3.html#probabilities-from-the-normal-distribution",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "42.2 Probabilities from the normal distribution",
    "text": "42.2 Probabilities from the normal distribution\nSince each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals and follow a normal distribution with a mean of 5000 dollars and a standard deviation of 2000 dollars. As part of his performance metrics, you want to calculate the probability of Amir closing a deal worth various amounts."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-1",
    "href": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-1",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nWhat’s the probability of Amir closing a deal worth less than $7500?\nWhat’s the probability of Amir closing a deal worth more than $1000?\nWhat’s the probability of Amir closing a deal worth between $3000 and $7000?\nWhat amount will 75% of Amir’s sales be more than?\n\n\n\nE2.R\n\n# Probability of deal < 7500\npnorm(7500, mean = 5000, sd = 2000)\n\n# Probability of deal > 1000\npnorm(1000, mean = 5000, sd = 2000, lower.tail =FALSE)\n\n# Probability of deal between 3000 and 7000\npnorm(7000, mean = 5000, sd = 2000) - pnorm(3000, mean = 5000, sd = 2000)\n\n# Calculate amount that 75% of deals will be more than\nqnorm(0.75, mean = 5000, sd = 2000, lower.tail=FALSE)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#simulating-sales-under-new-market-conditions",
    "href": "Introduction_to_Statistics_in_R_C3.html#simulating-sales-under-new-market-conditions",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "42.3 Simulating sales under new market conditions",
    "text": "42.3 Simulating sales under new market conditions\nThe company’s financial analyst is predicting that next quarter, the worth of each sale will increase by 20% and the volatility, or standard deviation, of each sale’s worth will increase by 30%. To see what Amir’s sales might look like next quarter under these new market conditions, you’ll simulate new sales amounts using the normal distribution and store these in the new_sales data frame, which has already been created for you.\nIn addition, dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-2",
    "href": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-2",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCurrently, Amir’s average sale amount is $5000. Calculate what his new average amount will be if it increases by 20% and store this in new_mean.\nAmir’s current standard deviation is $2000. Calculate what his new standard deviation will be if it increases by 30% and store this in new_sd.\nAdd a new column called amount to the data frame new_sales, which contains 36 simulated amounts from a normal distribution with a mean of new_mean and a standard deviation of new_sd.\nPlot the distribution of the new_sales amounts using a histogram with 10 bins.\n\n\n\nE3.R\n\n\n# Calculate new average amount\nnew_mean <- 5000 * 1.2\n\n# Calculate new standard deviation\nnew_sd <- 2000 * 1.3\n\n# Simulate 36 sales\nnew_sales <- new_sales %>% \n  mutate(amount = rnorm(36, mean = new_mean, sd = new_sd))\n\n# Create histogram with 10 bins\nggplot(new_sales, aes(amount)) +\n  geom_histogram(bins = 10)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#the-clt-in-action",
    "href": "Introduction_to_Statistics_in_R_C3.html#the-clt-in-action",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "42.4 The CLT in action",
    "text": "42.4 The CLT in action\nThe central limit theorem states that a sampling distribution of a sample statistic approaches the normal distribution as you take more samples, no matter the original distribution being sampled from.\nIn this exercise, you’ll focus on the sample mean and see the central limit theorem in action while examining the num_users column of amir_deals more closely, which contains the number of people who intend to use the product Amir is selling.\nBoth dplyr and ggplot2 are loaded and amir_deals is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-3",
    "href": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-3",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a histogram of the num_users column of amir_deals. Use 10 bins.\nSet the seed to 104. Take a sample of size 20 with replacement from the num_users column of amir_deals, and take the mean.\nRepeat this 100 times and store as sample_means. This will take 100 different samples and calculate the mean of each.\nA data frame called samples has been created for you with a column mean, which contains the values from sample_means. Create a histogram of the mean column with 10 bins.\n\n\n\nE4.R\n\n# Create a histogram of num_users\nggplot(amir_deals, aes(num_users)) +\n  geom_histogram(bins = 10  )\n  \n# Set seed to 104\nset.seed(104)\n\n# Sample 20 num_users with replacement from amir_deals\nsample(amir_deals$num_users, 20, replace = TRUE) %>%\n  # Take mean\n  mean()\n  \n# Set seed to 104\nset.seed(104)\n\n# Sample 20 num_users from amir_deals and take mean\nsample(amir_deals$num_users, size = 20, replace = TRUE) %>%\n  mean()\n\n# Repeat the above 100 times\nsample_means <- replicate(100, sample(amir_deals$num_users, size = 20, replace = TRUE) %>% mean())\n\n\n# Set seed to 104\nset.seed(104)\n\n# Sample 20 num_users from amir_deals and take mean\nsample(amir_deals$num_users, size = 20, replace = TRUE) %>%\n  mean()\n\n# Repeat the above 100 times\nsample_means <- replicate(100, sample(amir_deals$num_users, size = 20, replace = TRUE) %>% mean())\n\n# Create data frame for plotting\nsamples <- data.frame(mean = sample_means)\n\n# Histogram of sample means\nggplot(samples, aes(mean)) +\n  geom_histogram(bins = 10  )"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#the-mean-of-means",
    "href": "Introduction_to_Statistics_in_R_C3.html#the-mean-of-means",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "42.5 The mean of means",
    "text": "42.5 The mean of means\nYou want to know what the average number of users (num_users) is per deal, but you want to know this number for the entire company so that you can see if Amir’s deals have more or fewer users than the company’s average deal. The problem is that over the past year, the company has worked on more than ten thousand deals, so it’s not realistic to compile all the data. Instead, you’ll estimate the mean by taking several random samples of deals, since this is much easier than collecting data from everyone in the company.\nThe user data for all the company’s deals is available in all_deals."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-4",
    "href": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-4",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSet the random seed to 321.\nTake 30 samples of size 20 from all_deals$num_users and take the mean of each sample. Store the sample means in sample_means.\nTake the mean of sample_means.\nTake the mean of the num_users column of amir_deals.\n\n\n\nE5.R\n\n\n# Set seed to 321\nset.seed(321)\n\n# Take 30 samples of 20 values of num_users, take mean of each sample\nsample_means <- replicate(30, sample(all_deals$num_users, 20) %>% mean())\n\n# Calculate mean of sample_means\nmean(sample_means)\n\n# Calculate mean of num_users in amir_deals\nmean(amir_deals$num_users)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#tracking-lead-responses",
    "href": "Introduction_to_Statistics_in_R_C3.html#tracking-lead-responses",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "42.6 Tracking lead responses",
    "text": "42.6 Tracking lead responses\nYour company uses sales software to keep track of new sales leads. It organizes them into a queue so that anyone can follow up on one when they have a bit of free time. Since the number of lead responses is a countable outcome over a period of time, this scenario corresponds to a Poisson distribution. On average, Amir responds to 4 leads each day. In this exercise, you’ll calculate probabilities of Amir responding to different numbers of leads."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-5",
    "href": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-5",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nWhat’s the probability that Amir responds to 5 leads in a day, given that he responds to an average of 4?\nAmir’s coworker responds to an average of 5.5 leads per day. What is the probability that she answers 5 leads in a day?\nWhat’s the probability that Amir responds to 2 or fewer leads in a day?\nWhat’s the probability that Amir responds to more than 10 leads in a day?\n\n\n\nE6.R\n\n# Probability of 5 responses\ndpois(5, lambda = 4)\n\n\n# Probability of 5 responses from coworker\ndpois(5, lambda = 5.5)\n\n# Probability of 2 or fewer responses\nppois(2, lambda = 4)\n\n# Probability of > 10 responses\nppois(10, lambda = 4, lower.tail=FALSE)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#modeling-time-between-leads",
    "href": "Introduction_to_Statistics_in_R_C3.html#modeling-time-between-leads",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "42.7 Modeling time between leads",
    "text": "42.7 Modeling time between leads\nTo further evaluate Amir’s performance, you want to know how much time it takes him to respond to a lead after he opens it. On average, it takes 2.5 hours for him to respond. In this exercise, you’ll calculate probabilities of different amounts of time passing between Amir receiving a lead and sending a response."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-6",
    "href": "Introduction_to_Statistics_in_R_C3.html#instructions-100-xp-6",
    "title": "42  More Distributions and the Central Limit Theorem",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nWhat’s the probability it takes Amir less than an hour to respond to a lead?\nWhat’s the probability it takes Amir more than 4 hours to respond to a lead?\nWhat’s the probability it takes Amir 3-4 hours to respond to a lead?\n\n\n\nE7.R\n\n# Probability response takes < 1 hour\npexp(1, rate = 1/2.5)\n\n# Probability response takes > 4 hours\npexp(4, rate = 1/2.5, lower.tail=FALSE)\n\n# Probability response takes 3-4 hours\npexp(4, rate = 1/2.5) - pexp(3, rate = 1/2.5)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C4.html#relationships-between-variables",
    "href": "Introduction_to_Statistics_in_R_C4.html#relationships-between-variables",
    "title": "43  Correlation and Experimental Design",
    "section": "43.1 Relationships between variables",
    "text": "43.1 Relationships between variables\nIn this chapter, you’ll be working with a dataset world_happiness containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.\nIn this exercise, you’ll examine the relationship between a country’s life expectancy (life_exp) and happiness score (happiness_score) both visually and quantitatively. Both dplyr and ggplot2 are loaded and world_happiness is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C4.html#instructions-100-xp",
    "href": "Introduction_to_Statistics_in_R_C4.html#instructions-100-xp",
    "title": "43  Correlation and Experimental Design",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a scatterplot of happiness_score vs. life_exp using ggplot2.\nAdd a linear trendline to the scatterplot, setting se to FALSE.\nCalculate the correlation between life_exp and happiness_score.\n\n\n\nE1.R\n\n\n# Create a scatterplot of happiness_score vs. life_exp\nggplot(world_happiness, aes(life_exp, happiness_score)) +\n  geom_point()\n  \n# Add a linear trendline to scatterplot\nggplot(world_happiness, aes(life_exp, happiness_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n# Add a linear trendline to scatterplot\nggplot(world_happiness, aes(life_exp, happiness_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n# Correlation between life_exp and happiness_score\ncor(world_happiness$life_exp, world_happiness$happiness_score)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C4.html#what-cant-correlation-measure",
    "href": "Introduction_to_Statistics_in_R_C4.html#what-cant-correlation-measure",
    "title": "43  Correlation and Experimental Design",
    "section": "43.2 What can’t correlation measure?",
    "text": "43.2 What can’t correlation measure?\nWhile the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it’s far from perfect. In this exercise, you’ll explore one of the caveats of the correlation coefficient by examining the relationship between a country’s GDP per capita (gdp_per_cap) and happiness score.\nBoth dplyr and ggplot2 are loaded and world_happiness is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C4.html#instructions-100-xp-1",
    "href": "Introduction_to_Statistics_in_R_C4.html#instructions-100-xp-1",
    "title": "43  Correlation and Experimental Design",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a scatterplot showing the relationship between gdp_per_cap (on the x-axis) and life_exp (on the y-axis).\nCalculate the correlation between gdp_per_cap and life_exp.\n\n\n\nE2.R\n\n# Scatterplot of gdp_per_cap and life_exp\nggplot(world_happiness, aes(gdp_per_cap, life_exp)) +\n  geom_point()\n  \n# Scatterplot of gdp_per_cap and life_exp\nggplot(world_happiness, aes(gdp_per_cap, life_exp)) +\n  geom_point()\n\n# Correlation between gdp_per_cap and life_exp\ncor(world_happiness$gdp_per_cap, world_happiness$life_exp)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C4.html#transforming-variables",
    "href": "Introduction_to_Statistics_in_R_C4.html#transforming-variables",
    "title": "43  Correlation and Experimental Design",
    "section": "43.3 Transforming variables",
    "text": "43.3 Transforming variables\nWhen variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. In this exercise, you’ll perform a transformation yourself.\nBoth dplyr and ggplot2 are loaded and world_happiness is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C4.html#instructions-100-xp-2",
    "href": "Introduction_to_Statistics_in_R_C4.html#instructions-100-xp-2",
    "title": "43  Correlation and Experimental Design",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a scatterplot of happiness_score versus gdp_per_cap.\nCalculate the correlation between happiness_score and gdp_per_cap.\nAdd a new column to world_happiness called log_gdp_per_cap that contains the log of gdp_per_cap.\nCreate a scatterplot of happiness_score versus log_gdp_per_cap.\nCalculate the correlation between happiness_score and log_gdp_per_cap.\n\n\n\nE3.R\n\n# Scatterplot of happiness_score vs. gdp_per_cap\nggplot(world_happiness, aes(gdp_per_cap, happiness_score)) +\n  geom_point()\n\n# Calculate correlation\ncor(world_happiness$gdp_per_cap, world_happiness$happiness_score)\n\n\n# Create log_gdp_per_cap column\nworld_happiness <- world_happiness %>%\n  mutate(log_gdp_per_cap = log(gdp_per_cap))\n\n# Scatterplot of happiness_score vs. log_gdp_per_cap\nggplot(world_happiness, aes(log_gdp_per_cap, happiness_score)) +\n  geom_point()\n\n# Calculate correlation\ncor(world_happiness$log_gdp_per_cap, world_happiness$happiness_score)"
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C4.html#does-sugar-improve-happiness",
    "href": "Introduction_to_Statistics_in_R_C4.html#does-sugar-improve-happiness",
    "title": "43  Correlation and Experimental Design",
    "section": "43.4 Does sugar improve happiness?",
    "text": "43.4 Does sugar improve happiness?\nA new column has been added to world_happiness called grams_sugar_per_day, which contains the average amount of sugar eaten per person per day in each country. In this exercise, you’ll examine the effect of a country’s average sugar consumption on its happiness score.\nBoth dplyr and ggplot2 are loaded and world_happiness is available."
  },
  {
    "objectID": "Introduction_to_Statistics_in_R_C4.html#instructions-100-xp-3",
    "href": "Introduction_to_Statistics_in_R_C4.html#instructions-100-xp-3",
    "title": "43  Correlation and Experimental Design",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a scatterplot showing the relationship between grams_sugar_per_day (on the x-axis) and happiness_score (on the y-axis).\nCalculate the correlation between grams_sugar_per_day and happiness_score.\n\n\n\nE4.R\n\n# Scatterplot of grams_sugar_per_day and happiness_score\nggplot(world_happiness, aes(grams_sugar_per_day, happiness_score)) +\n  geom_point()\n\n# Correlation between grams_sugar_per_day and happiness_score\ncor(world_happiness$grams_sugar_per_day, world_happiness$happiness_score)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C1.html#visualizing-two-variables",
    "href": "Introduction_to_Regression_in_R_C1.html#visualizing-two-variables",
    "title": "44  Simple Linear Regression",
    "section": "44.1 Visualizing two variables",
    "text": "44.1 Visualizing two variables\nBefore you can run any statistical models, it’s usually a good idea to visualize your dataset. Here, we’ll look at the relationship between house price per area and the number of nearby convenience stores, using the Taiwan real estate dataset.\nOne challenge in this dataset is that the number of convenience stores contains integer data, causing points to overlap. To solve this, you will make the points transparent.\ntaiwan_real_estate is available, ggplot2 is loaded, and its black and white theme has been set."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C1.html#instructions-100-xp",
    "href": "Introduction_to_Regression_in_R_C1.html#instructions-100-xp",
    "title": "44  Simple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing taiwan_real_estate, draw a scatter plot of price_twd_msq (y-axis) versus n_convenience (x-axis).\nUpdate the plot to make the points 50% transparent by setting alpha to 0.5.\nUpdate the plot by adding a trend line, calculated using a linear regression. You can omit the confidence ribbon.\n\n\n\nE1.R\n\n# Draw a scatter plot of n_convenience vs. price_twd_msq\nggplot(taiwan_real_estate, aes(n_convenience,price_twd_msq))+\ngeom_point()\n\n\n# Make points 50% transparent\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +\n  geom_point( alpha = 0.5)\n\n# Add a linear trend line without a confidence ribbon\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C1.html#linear-regression-with-lm",
    "href": "Introduction_to_Regression_in_R_C1.html#linear-regression-with-lm",
    "title": "44  Simple Linear Regression",
    "section": "44.2 Linear regression with lm()",
    "text": "44.2 Linear regression with lm()\nWhile ggplot can display a linear regression trend line using geom_smooth(), it doesn’t give you access to the intercept and slope as variables, or allow you to work with the model results as variables. That means that sometimes you’ll need to run a linear regression yourself.\nTime to run your first model!\ntaiwan_real_estate is available. TWD is an abbreviation for Taiwan dollars."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C1.html#instructions-100-xp-1",
    "href": "Introduction_to_Regression_in_R_C1.html#instructions-100-xp-1",
    "title": "44  Simple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun a linear regression with price_twd_msq as the response variable, n_convenience as the explanatory variable, and taiwan_real_estate as the dataset.\n\n\n\nE2.R\n\n# Run a linear regression of price_twd_msq vs. n_convenience\n\nlm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)\n\nggplot(taiwan_real_estate, aes(price_twd_msq, n_convenience)) +\ngeom_smooth()"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C1.html#visualizing-numeric-vs.-categorical",
    "href": "Introduction_to_Regression_in_R_C1.html#visualizing-numeric-vs.-categorical",
    "title": "44  Simple Linear Regression",
    "section": "44.3 Visualizing numeric vs. categorical",
    "text": "44.3 Visualizing numeric vs. categorical\nIf the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn’t make sense. Instead, a good option is to draw a histogram for each category.\nThe Taiwan real estate dataset has a categorical variable in the form of the age of each house. The ages have been split into 3 groups: 0 to 15 years, 15 to 30 years, and 30 to 45 years.\ntaiwan_real_estate is available and ggplot2 is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C1.html#instructions-100-xp-2",
    "href": "Introduction_to_Regression_in_R_C1.html#instructions-100-xp-2",
    "title": "44  Simple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing taiwan_real_estate, plot a histogram of price_twd_msq with 10 bins.\nFacet the plot by house_age_years to give 3 panels.\n\n\n\nE3.R\n\n# Using taiwan_real_estate, plot price_twd_msq\nggplot(taiwan_real_estate, aes(price_twd_msq)) +\n    # Make it a histogram with 10 bins\n  geom_histogram(bins = 10) +\n  # Facet the plot so each house age group gets its own panel\n  facet_wrap(vars(house_age_years))"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C1.html#calculating-means-by-category",
    "href": "Introduction_to_Regression_in_R_C1.html#calculating-means-by-category",
    "title": "44  Simple Linear Regression",
    "section": "44.4 Calculating means by category",
    "text": "44.4 Calculating means by category\nA good way to explore categorical variables is to calculate summary statistics such as the mean for each category. Here, you’ll look at grouped means for the house prices in the Taiwan real estate dataset.\ntaiwan_real_estate is available and dplyr is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C1.html#instructions-100-xp-3",
    "href": "Introduction_to_Regression_in_R_C1.html#instructions-100-xp-3",
    "title": "44  Simple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGroup taiwan_real_estate by house_age_years.\nSummarize to calculate the mean price_twd_msq for each group, naming the column mean_by_group.\nAssign the result to summary_stats and look at the numbers.\n\n\n\nE4.R\n\nsummary_stats <- taiwan_real_estate %>% \n  # Group by house age\n  group_by(house_age_years) %>% \n  # Summarize to calculate the mean house price/area\n  summarize(mean_by_group = mean(price_twd_msq))\n\n# See the result\nsummary_stats"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C1.html#lm-with-a-categorical-explanatory-variable",
    "href": "Introduction_to_Regression_in_R_C1.html#lm-with-a-categorical-explanatory-variable",
    "title": "44  Simple Linear Regression",
    "section": "44.5 lm() with a categorical explanatory variable",
    "text": "44.5 lm() with a categorical explanatory variable\nLinear regressions also work with categorical explanatory variables. In this case, the code to run the model is the same, but the coefficients returned by the model are different. Here you’ll run a linear regression on the Taiwan real estate dataset.\ntaiwan_real_estate is available."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C1.html#instructions-100-xp-4",
    "href": "Introduction_to_Regression_in_R_C1.html#instructions-100-xp-4",
    "title": "44  Simple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun a linear regression with price_twd_msq as the response variable, house_age_years as the explanatory variable, and taiwan_real_estate as the dataset. Assign to mdl_price_vs_age.\nUpdate the model formula so that no intercept is included in the model. Assign to mdl_price_vs_age_no_intercept.\n\n\n\nE5.R\n\n# Run a linear regression of price_twd_msq vs. house_age_years\nmdl_price_vs_age <- lm(taiwan_real_estate$price_twd_msq ~ taiwan_real_estate$house_age_years )\n\n# See the result\nmdl_price_vs_age\n\n\n\n# Update the model formula to remove the intercept\nmdl_price_vs_age_no_intercept <- lm(\n  price_twd_msq ~ house_age_years + 0, \n  data = taiwan_real_estate\n)\n\n# See the result\nmdl_price_vs_age_no_intercept"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html",
    "href": "Introduction_to_Regression_in_R_C2.html",
    "title": "45  Predictions and model objects",
    "section": "",
    "text": "46 Create a tibble with n_convenience column from zero to ten\nexplanatory_data <- tibble( n_convenience = 0:10 )\npredict(mdl_price_vs_conv,explanatory_data )\nexplanatory_data <- tibble( n_convenience = 0:10 )\nprediction_data <- explanatory_data %>% mutate( price_twd_msq = predict(mdl_price_vs_conv, explanatory_data) )\nprediction_data"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#predicting-house-prices",
    "href": "Introduction_to_Regression_in_R_C2.html#predicting-house-prices",
    "title": "45  Predictions and model objects",
    "section": "45.1 Predicting house prices",
    "text": "45.1 Predicting house prices\nPerhaps the most useful feature of statistical models like linear regression is that you can make predictions. That is, you specify values for each of the explanatory variables, feed them to the model, and you get a prediction for the corresponding response variable. The code flow is as follows.\n\nexplanatory_data <- tibble( explanatory_var = some_values ) explanatory_data %>% mutate( response_var = predict(model, explanatory_data) )\n\nHere, you’ll make predictions for the house prices in the Taiwan real estate dataset.\ntaiwan_real_estate is available. The linear regression model of house price versus number of convenience stores is available as mdl_price_vs_conv (print it and read the call to see how it was made); and dplyr is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp",
    "href": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp",
    "title": "45  Predictions and model objects",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a tibble of explanatory data, where the number of convenience stores, n_convenience, takes the integer values from zero to ten.\nUse the model mdl_price_vs_conv to make predictions from explanatory_data.\nCreate a tibble of predictions named prediction_data.\n\nStart with explanatory_data.\nAdd an extra column, price_twd_msq, containing the predictions. ``` {.r filename=“E1.R”} # Create a tibble with n_convenience column from zero to ten explanatory_data <- tibble(n_convenience = 0:10)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#the-limits-of-prediction",
    "href": "Introduction_to_Regression_in_R_C2.html#the-limits-of-prediction",
    "title": "45  Predictions and model objects",
    "section": "50.1 The limits of prediction",
    "text": "50.1 The limits of prediction\nIn the last exercise you made predictions on some sensible, could-happen-in-real-life, situations. That is, the cases when the number of nearby convenience stores were between zero and ten. To test the limits of the model’s ability to predict, try some impossible situations.\nUse the console to try predicting house prices from mdl_price_vs_conv when there are -1 convenience stores. Do the same for 2.5 convenience stores. What happens in each case?\nmdl_price_vs_conv is available and dplyr is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-1",
    "href": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-1",
    "title": "45  Predictions and model objects",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate some impossible explanatory data. Define a tibble with one column, n_convenience, set to minus one, assigning to minus_one. Create another with n_convenience set to two point five, assigning to two_pt_five.\n\n\n\nE3.R\n\n# Define a tibble where n_convenience is -1\nminus_one <- tibble(n_convenience = -1)\n\n# Define a tibble where n_convenience is 2.5\ntwo_pt_five <- tibble(n_convenience = 2.5)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#extracting-model-elements",
    "href": "Introduction_to_Regression_in_R_C2.html#extracting-model-elements",
    "title": "45  Predictions and model objects",
    "section": "50.2 Extracting model elements",
    "text": "50.2 Extracting model elements\nThe variable returned by lm() that contains the model object has many elements. In order to perform further analysis on the model results, you need to extract the useful bits of it. The model coefficients, the fitted values, and the residuals are perhaps the most important bits of the linear model object.\nmdl_price_vs_conv is available."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-2",
    "href": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-2",
    "title": "45  Predictions and model objects",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPrint the coefficients of mdl_price_vs_conv.\nPrint the fitted values of mdl_price_vs_conv.\nPrint the residuals of mdl_price_vs_conv.\nPrint a summary of mdl_price_vs_conv.\n\n\n\nE4.R\n\n# Get the model coefficients of mdl_price_vs_conv\ncoef(mdl_price_vs_conv)\n\n\n# Get the fitted values of mdl_price_vs_conv\nfitted(mdl_price_vs_conv)\n\n\n# Get the residuals of mdl_price_vs_conv\nresiduals(mdl_price_vs_conv)\n\n\n# Print a summary of mdl_price_vs_conv\nsummary(mdl_price_vs_conv)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#manually-predicting-house-prices",
    "href": "Introduction_to_Regression_in_R_C2.html#manually-predicting-house-prices",
    "title": "45  Predictions and model objects",
    "section": "50.3 Manually predicting house prices",
    "text": "50.3 Manually predicting house prices\nYou can manually calculate the predictions from the model coefficients. When making predictions in real life, it is better to use predict(), but doing this manually is helpful to reassure yourself that predictions aren’t magic—they are simply arithmetic.\nIn fact, for a simple linear regression, the predicted value is just the intercept plus the slope times the explanatory variable.\nmdl_price_vs_conv and explanatory_data are available, and dplyr is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-3",
    "href": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-3",
    "title": "45  Predictions and model objects",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the coefficients of mdl_price_vs_conv, assigning to coeffs.\nGet the intercept, which is the first element of coeffs, assigning to intercept.\nGet the slope, which is the second element of coeffs, assigning to slope.\nManually predict price_twd_msq using the intercept, slope, and n_convenience.\n\n\n\nE5.R\n\n# Get the coefficients of mdl_price_vs_conv\ncoeffs <- coef(mdl_price_vs_conv)\n\n# Get the intercept\nintercept <- coeffs[1]\n\n# Get the slope\nslope <- coeffs[2]\n\nexplanatory_data %>% \n  mutate(\n    # Manually calculate the predictions\n    price_twd_msq = intercept + slope * n_convenience\n  )\n\n# Compare to the results from predict()\npredict(mdl_price_vs_conv, explanatory_data)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#using-broom",
    "href": "Introduction_to_Regression_in_R_C2.html#using-broom",
    "title": "45  Predictions and model objects",
    "section": "50.4 Using broom",
    "text": "50.4 Using broom\nMany programming tasks are easier if you keep all your data inside data frames. This is particularly true if you are a tidyverse fan, where dplyr and ggplot2 require you to use data frames. The broom package contains functions that decompose models into three data frames: one for the coefficient-level elements (the coefficients themselves, as well as p-values for each coefficient), the observation-level elements (like fitted values and residuals), and the model-level elements (mostly performance metrics).\nThe functions in broom are generic. That is, they work with many model types, not just linear regression model objects. They also work with logistic regression model objects (as you’ll see in Chapter 4), and many other types of model.\nmdl_price_vs_conv is available and broom is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-4",
    "href": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-4",
    "title": "45  Predictions and model objects",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nTidy the model to print the coefficient-level elements of mdl_price_vs_conv.\nAugment the model to print the observation-level elements of mdl_price_vs_conv.\nGlance at the model to print the model-level elements of mdl_price_vs_conv.\n\n\n\nE6.R\n\n# Get the coefficient-level elements of the model\ntidy(mdl_price_vs_conv)\n\n\n# Get the observation-level elements of the model\naugment(mdl_price_vs_conv)\n\n\n# Get the model-level elements of the model\nglance(mdl_price_vs_conv)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#plotting-consecutive-portfolio-returns",
    "href": "Introduction_to_Regression_in_R_C2.html#plotting-consecutive-portfolio-returns",
    "title": "45  Predictions and model objects",
    "section": "50.5 Plotting consecutive portfolio returns",
    "text": "50.5 Plotting consecutive portfolio returns\nRegression to the mean is also an important concept in investing. Here you’ll look at the annual returns from investing in companies in the Standard and Poor 500 index (S&P 500), in 2018 and 2019.\nThe sp500_yearly_returns dataset contains three columns:\nvariable meaning symbol Stock ticker symbol uniquely identifying the company. return_2018 A measure of investment performance in 2018. return_2019 A measure of investment performance in 2019.\nA positive number for the return means the investment increased in value; negative means it lost value.\nJust as with baseball home runs, a naive prediction might be that the investment performance stays the same from year to year, lying on the “y equals x” line.\nsp500_yearly_returns is available and ggplot2 is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-5",
    "href": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-5",
    "title": "45  Predictions and model objects",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing sp500_yearly_returns, draw a scatter plot of return_2019 vs. return_2018.\nAdd an “A-B line”, colored “green”, with size 1.\nAdd a smooth trend line made with the linear regression method, and no standard error ribbon.\nFix the coordinates so distances along the x and y axes appear the same.\n\n\n\nE7.R\n\n# Using sp500_yearly_returns, plot return_2019 vs. return_2018\nggplot(sp500_yearly_returns, aes(return_2018, return_2019)) +\n  # Make it a scatter plot\n  geom_point() +\n  # Add a line at y = x, colored green, size 1\n  geom_abline(color = \"green\", size = 1) +\n  # Add a linear regression trend line, no std. error ribbon\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Fix the coordinate ratio\n  coord_fixed()"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#modeling-consecutive-returns",
    "href": "Introduction_to_Regression_in_R_C2.html#modeling-consecutive-returns",
    "title": "45  Predictions and model objects",
    "section": "50.6 Modeling consecutive returns",
    "text": "50.6 Modeling consecutive returns\nLet’s quantify the relationship between returns in 2019 and 2018 by running a linear regression and making predictions. By looking at companies with extremely high or extremely low returns in 2018, we can see if their performance was similar in 2019.\nsp500_yearly_returns is available and dplyr is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-6",
    "href": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-6",
    "title": "45  Predictions and model objects",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun a linear regression on return_2019 versus return_2018 using sp500_yearly_returns. Assign to mdl_returns.\nCreate a data frame (or tibble) named explanatory_data. Give it one column with 2018 returns set to a vector containing -1, 0, and 1.\nUse mdl_returns to predict with explanatory_data.\n\n\n\nE8.R\n\n# Run a linear regression on return_2019 vs. return_2018 using sp500_yearly_returns\nmdl_returns <- lm(\n  return_2019 ~ return_2018, \n  data = sp500_yearly_returns\n)\n\n# See the result\nmdl_returns\n\n\n# Run a linear regression on return_2019 vs. return_2018 using sp500_yearly_returns\nmdl_returns <- lm(\n  return_2019 ~ return_2018, \n  data = sp500_yearly_returns\n)\n\n# Create a data frame with return_2018 at -1, 0, and 1 \nexplanatory_data <- tibble(return_2018 = c(-1,0,1))\n\n# Use mdl_returns to predict with explanatory_data\npredict(mdl_returns,explanatory_data)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#transforming-the-explanatory-variable",
    "href": "Introduction_to_Regression_in_R_C2.html#transforming-the-explanatory-variable",
    "title": "45  Predictions and model objects",
    "section": "50.7 Transforming the explanatory variable",
    "text": "50.7 Transforming the explanatory variable\nIf there is no straight line relationship between the response variable and the explanatory variable, it is sometimes possible to create one by transforming one or both of the variables. Here, you’ll look at transforming the explanatory variable.\nYou’ll take another look at the Taiwan real estate dataset, this time using the distance to the nearest MRT (metro) station as the explanatory variable. You’ll use code to make every commuter’s dream come true: shortening the distance to the metro station by taking the square root. Take that, geography!\ntaiwan_real_estate is available and ggplot2 and tibble are loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-7",
    "href": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-7",
    "title": "45  Predictions and model objects",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun the code provided, and look at the plot.\nEdit the plot so the x aesthetic is square root transformed.\nLook at the new plot. Notice how the numbers on the x-axis have changed. This is a different line to what was shown before. Do the points track the line more closely?\nRun a linear regression of price_twd_msq versus the square root of dist_to_mrt_m using taiwan_real_estate.\nCreate a data frame of prediction data named prediction_data. Start with explanatory_data, and add a column named after the response variable. Predict values using mdl_price_vs_dist and explanatory_data.\nEdit the plot to add a layer of points from prediction_data, colored “green”, size 5.\n\n\n\nE9.R\n\n# Run the code to see the plot\n# Edit so x-axis is square root of dist_to_mrt_m\nggplot(taiwan_real_estate, aes(sqrt(dist_to_mrt_m), price_twd_msq)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n  \n  \n# Run a linear regression of price_twd_msq vs. square root of dist_to_mrt_m using taiwan_real_estate\nmdl_price_vs_dist <- lm(\n  price_twd_msq ~ sqrt(dist_to_mrt_m), \n  data = taiwan_real_estate\n)\n\n# See the result\nmdl_price_vs_dist\n\n\n\n# Run a linear regression of price_twd_msq vs. square root of dist_to_mrt_m using taiwan_real_estate\nmdl_price_vs_dist <- lm(\n  price_twd_msq ~ sqrt(dist_to_mrt_m), \n  data = taiwan_real_estate\n)\n\nexplanatory_data <- tibble(\n  dist_to_mrt_m = seq(0, 80, 10) ^ 2\n)\n\n# Use mdl_price_vs_dist to predict explanatory_data\nprediction_data <- explanatory_data %>% \n  mutate(\n    price_twd_msq = predict(mdl_price_vs_dist, explanatory_data)\n  )\n\n# See the result\nprediction_data\n\n\n# From previous steps\nmdl_price_vs_dist <- lm(\n  price_twd_msq ~ sqrt(dist_to_mrt_m), \n  data = taiwan_real_estate\n)\nexplanatory_data <- tibble(\n  dist_to_mrt_m = seq(0, 80, 10) ^ 2\n)\nprediction_data <- explanatory_data %>% \n  mutate(\n    price_twd_msq = predict(mdl_price_vs_dist, explanatory_data)\n  )\n\nggplot(taiwan_real_estate, aes(sqrt(dist_to_mrt_m), price_twd_msq)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Add points from prediction_data, colored green, size 5\n  geom_point(data = prediction_data, color = \"green\", size = 5)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#transforming-the-response-variable-too",
    "href": "Introduction_to_Regression_in_R_C2.html#transforming-the-response-variable-too",
    "title": "45  Predictions and model objects",
    "section": "50.8 Transforming the response variable too",
    "text": "50.8 Transforming the response variable too\nThe response variable can be transformed too, but this means you need an extra step at the end to undo that transformation. That is, you “back transform” the predictions.\nIn the video, you saw the first step of the digital advertising workflow: spending money to buy ads, and counting how many people see them (the “impressions”). The next step is determining how many people click on the advert after seeing it.\nad_conversion is available and ggplot2 and tibble are loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-8",
    "href": "Introduction_to_Regression_in_R_C2.html#instructions-100-xp-8",
    "title": "45  Predictions and model objects",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun the code provided, and look at the plot.\nEdit the plot so the x and y aesthetics are transformed by raising them to the power 0.25.\nLook at the new plot. Do the points track the line more closely?\nRun a linear regression of n_clicks to the power 0.25 versus n_impressions to the power 0.25 using ad_conversion. Each variable in the formula needs to be specified “as is”, using I().\nComplete the code for the prediction data. Use mdl_click_vs_impression to predict n_clicks to the power 0.25 from explanatory_data.\nBack transform by raising n_clicks_025 to the power 4 to get n_clicks.\nEdit the plot to add a layer of points from prediction_data, colored “green”.\n\n\n\nE10.R\n\n# Run the code to see the plot\n# Edit to raise x, y aesthetics to power 0.25\nggplot(ad_conversion, aes(n_impressions ^ 0.25, n_clicks ^ 0.25)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n# Run a linear regression of n_clicks to the power 0.25 vs. n_impressions to the power 0.25 using ad_conversion\nmdl_click_vs_impression <- lm(\n  I(n_clicks ^ 0.25) ~ I(n_impressions ^ 0.25),\n  data = ad_conversion\n)\n\n\n\n# Run a linear regression of n_clicks to the power 0.25 vs. n_impressions to the power 0.25 using ad_conversion\nmdl_click_vs_impression <- lm(\n  I(n_clicks ^ 0.25) ~ I(n_impressions ^ 0.25),\n  data = ad_conversion\n)\n\nexplanatory_data <- tibble(\n  n_impressions = seq(0, 3e6, 5e5)\n)\n\nprediction_data <- explanatory_data %>% \n  mutate(\n    # Use mdl_click_vs_impression to predict n_clicks ^ 0.25\n    n_clicks_025 = predict(mdl_click_vs_impression, explanatory_data),\n    # Back transform to get n_clicks\n    n_clicks = n_clicks_025 ^ 4\n  )\n  \n\n\n# From previous steps\nmdl_click_vs_impression <- lm(\n  I(n_clicks ^ 0.25) ~ I(n_impressions ^ 0.25),\n  data = ad_conversion\n)\nexplanatory_data <- tibble(\n  n_impressions = seq(0, 3e6, 5e5)\n)\nprediction_data <- explanatory_data %>% \n  mutate(\n    n_clicks_025 = predict(mdl_click_vs_impression, explanatory_data),\n    n_clicks = n_clicks_025 ^ 4\n  )\n\nggplot(ad_conversion, aes(n_impressions ^ 0.25, n_clicks ^ 0.25)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Add points from prediction_data, colored green\n  geom_point(data = prediction_data, color = \"green\")"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C3.html#coefficient-of-determination",
    "href": "Introduction_to_Regression_in_R_C3.html#coefficient-of-determination",
    "title": "46  Assessing model fit",
    "section": "46.1 Coefficient of determination",
    "text": "46.1 Coefficient of determination\nThe coefficient of determination is a measure of how well the linear regression line fits the observed values. For simple linear regression, it is equal to the square of the correlation between the explanatory and response variables.\nHere, you’ll take another look at the second stage of the advertising pipeline: modeling the click response to impressions. Two models are available: mdl_click_vs_impression_orig models n_clicks versus n_impressions. mdl_click_vs_impression_trans is the transformed model you saw in Chapter 2. It models n_clicks ^ 0.25 versus n_impressions ^ 0.25.\nbroom is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C3.html#instructions-100-xp",
    "href": "Introduction_to_Regression_in_R_C3.html#instructions-100-xp",
    "title": "46  Assessing model fit",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPrint a summary of mdl_click_vs_impression_orig. Do the same for mdl_click_vs_impression_trans\nGet the coefficient of determination for mdl_click_vs_impression_orig by glancing at the model, then pulling the r.squared value.\nDo the same for mdl_click_vs_impression_trans.\n\n\n\nE1.R\n\n# Print a summary of mdl_click_vs_impression_orig\nsummary(mdl_click_vs_impression_orig)\n\n# Print a summary of mdl_click_vs_impression_trans\nsummary(mdl_click_vs_impression_trans)\n\n\n\n# Get coeff of determination for mdl_click_vs_impression_orig\nmdl_click_vs_impression_orig %>% \n  # Get the model-level details\n  glance() %>% \n  # Pull out r.squared\n  pull(r.squared)\n\n# Do the same for the transformed model\nmdl_click_vs_impression_trans %>% \n  glance() %>% \n  pull(r.squared)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C3.html#residual-standard-error",
    "href": "Introduction_to_Regression_in_R_C3.html#residual-standard-error",
    "title": "46  Assessing model fit",
    "section": "46.2 Residual standard error",
    "text": "46.2 Residual standard error\nResidual standard error (RSE) is a measure of the typical size of the residuals. Equivalently, it’s a measure of how badly wrong you can expect predictions to be. Smaller numbers are better, with zero being a perfect fit to the data.\nAgain, you’ll look at the models from the advertising pipeline, mdl_click_vs_impression_orig and mdl_click_vs_impression_trans. broom is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C3.html#instructions-100-xp-1",
    "href": "Introduction_to_Regression_in_R_C3.html#instructions-100-xp-1",
    "title": "46  Assessing model fit",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the residual standard error for mdl_click_vs_impression_orig by glancing at the model, then pulling the sigma value.\nDo the same for mdl_click_vs_impression_trans.\n\n\n\nE2.R\n\n# Get RSE for mdl_click_vs_impression_orig\nmdl_click_vs_impression_orig %>% \n  # Get the model-level details\n  glance() %>% \n  # Pull out sigma\n  pull(sigma)\n\n# Do the same for the transformed model\nmdl_click_vs_impression_trans %>% \n  glance() %>% \n  pull(sigma)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C3.html#drawing-diagnostic-plots",
    "href": "Introduction_to_Regression_in_R_C3.html#drawing-diagnostic-plots",
    "title": "46  Assessing model fit",
    "section": "46.3 Drawing diagnostic plots",
    "text": "46.3 Drawing diagnostic plots\nIt’s time for you to draw these diagnostic plots yourself. Let’s go back to the Taiwan real estate dataset and the model of house prices versus number of convenience stores.\nRecall that autoplot() lets you specify which diagnostic plots you are interested in.\n\n1 residuals vs. fitted values 2 Q-Q plot 3 scale-location\n\nmdl_price_vs_conv is available, and ggplot2 and ggfortify are loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C3.html#instructions-100-xp-2",
    "href": "Introduction_to_Regression_in_R_C3.html#instructions-100-xp-2",
    "title": "46  Assessing model fit",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPlot the three diagnostic plots (numbered 1 to 3) for mdl_price_vs_conv. Use a layout of three rows and one column.\n\n\n\nE3.R\n\n# Plot the three diagnostics for mdl_price_vs_conv\nautoplot(mdl_price_vs_conv, which = 1:3, nrow = 3, ncol = 1)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C3.html#extracting-leverage-and-influence",
    "href": "Introduction_to_Regression_in_R_C3.html#extracting-leverage-and-influence",
    "title": "46  Assessing model fit",
    "section": "46.4 Extracting leverage and influence",
    "text": "46.4 Extracting leverage and influence\nIn the last few exercises you explored which observations had the highest leverage and influence. Now you’ll extract those values from an augmented version of the model, and visualize them.\nmdl_price_vs_dist is available. dplyr, ggplot2 and ggfortify are loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C3.html#instructions-100-xp-3",
    "href": "Introduction_to_Regression_in_R_C3.html#instructions-100-xp-3",
    "title": "46  Assessing model fit",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAugment mdl_price_vs_dist, then arrange observations by descending influence (.hat), and get the head of the results.\nAugment mdl_price_vs_dist, then arrange observations by descending influence (.cooksd), and get the head of the results.\nPlot the three outlier diagnostic plots (numbered 4 to 6) for mdl_price_vs_dist. Use a layout of three rows and one column.\n\n\n\nE4.R\n\nmdl_price_vs_dist %>% \n  # Augment the model\n  augment() %>% \n  # Arrange rows by descending leverage\n  arrange(desc(.hat)) %>% \n  # Get the head of the dataset\n  head()\n \n\nmdl_price_vs_dist %>% \n  # Augment the model\n  augment() %>% \n  # Arrange rows by descending Cook's distance\n  arrange(desc(.cooksd)) %>% \n  # Get the head of the dataset\n  head()\n  \n\n# Plot the three outlier diagnostics for mdl_price_vs_dist\nautoplot(mdl_price_vs_dist, which = 4:6, nrow = 3, ncol = 1)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#exploring-the-explanatory-variables",
    "href": "Introduction_to_Regression_in_R_C4.html#exploring-the-explanatory-variables",
    "title": "47  Simple logistic regression",
    "section": "47.1 Exploring the explanatory variables",
    "text": "47.1 Exploring the explanatory variables\nWhen the response variable is logical, all the points lie on the y equals zero and y equals one lines, making it difficult to see what is happening. In the video, until you saw the trend line, it wasn’t clear how the explanatory variable was distributed on each line. This can be solved with a histogram of the explanatory variable, faceted on the response.\nYou will use these histograms to get to know the financial services churn dataset seen in the video.\nchurn is available and ggplot2 is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp",
    "href": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp",
    "title": "47  Simple logistic regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing churn, plot time_since_last_purchase as a histogram with binwidth 0.25 faceted in a grid with has_churned on each row.\nRedraw the plot with time_since_first_purchase. That is, using churn, plot time_since_first_purchase as a histogram with binwidth 0.25 faceted in a grid with has_churned on each row.\n\n\n\nE1.R\n\n# Using churn, plot time_since_last_purchase\nggplot(churn, aes(time_since_last_purchase)) +\n  # as a histogram with binwidth 0.25\n  geom_histogram(binwidth = 0.25) +\n  # faceted in a grid with has_churned on each row\n  facet_grid(rows = vars(has_churned))\n\n\n\n# Redraw the plot with time_since_first_purchase\nggplot(churn, aes(time_since_first_purchase)) +\n  geom_histogram(binwidth = 0.25) +\n  facet_grid(rows = vars(has_churned))"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#visualizing-linear-and-logistic-models",
    "href": "Introduction_to_Regression_in_R_C4.html#visualizing-linear-and-logistic-models",
    "title": "47  Simple logistic regression",
    "section": "47.2 Visualizing linear and logistic models",
    "text": "47.2 Visualizing linear and logistic models\nAs with linear regressions, ggplot2 will draw model predictions for a logistic regression without you having to worry about the modeling code yourself. To see how the predictions differ for linear and logistic regressions, try drawing both trend lines side by side. Spoiler: you should see a linear (straight line) trend from the linear model, and a logistic (S-shaped) trend from the logistic model.\nchurn is available and ggplot2 is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-1",
    "href": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-1",
    "title": "47  Simple logistic regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing churn plot has_churned vs. time_since_first_purchase as a scatter plot, adding a red linear regression trend line (without a standard error ribbon).\nUpdate the plot by adding a second trend line from logistic regression. (No standard error ribbon again).\n\n\n\nE2.R\n\n# Using churn plot has_churned vs. time_since_first_purchase\nggplot(churn, aes(time_since_first_purchase, has_churned)) +\n  # Make it a scatter plot\n  geom_point() +\n  # Add an lm trend line, no std error ribbon, colored red\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n\n\n\nggplot(churn, aes(time_since_first_purchase, has_churned)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  # Add a glm trend line, no std error ribbon, binomial family\n  geom_smooth(\n    method = \"glm\", \n    se = FALSE, \n    method.args = list(family = binomial)\n  )"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#logistic-regression-with-glm",
    "href": "Introduction_to_Regression_in_R_C4.html#logistic-regression-with-glm",
    "title": "47  Simple logistic regression",
    "section": "47.3 Logistic regression with glm()",
    "text": "47.3 Logistic regression with glm()\nLinear regression and logistic regression are special cases of a broader type of models called generalized linear models (“GLMs”). A linear regression makes the assumption that the residuals follow a Gaussian (normal) distribution. By contrast, a logistic regression assumes that residuals follow a binomial distribution.\nHere, you’ll model how the length of relationship with a customer affects churn."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-2",
    "href": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-2",
    "title": "47  Simple logistic regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFit a logistic regression of has_churned versus time_since_first_purchase using the churn dataset. Assign to mdl_churn_vs_relationship.\n\n\n\nE3.R\n\n# Fit a logistic regression of churn vs. length of relationship using the churn dataset\nmdl_churn_vs_relationship <- glm(\n  has_churned ~ time_since_first_purchase, \n  data = churn, \n  family = binomial\n)\n\n# See the result\nmdl_churn_vs_relationship"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#probabilities",
    "href": "Introduction_to_Regression_in_R_C4.html#probabilities",
    "title": "47  Simple logistic regression",
    "section": "47.4 Probabilities",
    "text": "47.4 Probabilities\nThere are four main ways of expressing the prediction from a logistic regression model—we’ll look at each of them over the next four exercises. Firstly, since the response variable is either “yes” or “no”, you can make a prediction of the probability of a “yes”. Here, you’ll calculate and visualize these probabilities.\nThree variables are available:\n\nmdl_churn_vs_relationship is the logistic regression model of has_churned versus time_since_first_purchase.\nexplanatory_data is a data frame of explanatory values.\nplt_churn_vs_relationship is a scatter plot of has_churned versus time_since_first_purchase with a smooth glm line. dplyr is loaded."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-3",
    "href": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-3",
    "title": "47  Simple logistic regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the model, mdl_churn_vs_relationship, and the explanatory data, explanatory_data, to predict the probability of churning. Assign the predictions to the has_churned column of a data frame, prediction_data. Remember to set the prediction type.\nUpdate the plt_churn_vs_relationship plot to add points from prediction_data, colored yellow with size of 2.\n\n\n\nE4.R\n\n# Make a data frame of predicted probabilities\nprediction_data <- explanatory_data %>% \n  mutate(   \n    has_churned = predict(\n      mdl_churn_vs_relationship, \n      explanatory_data, \n      type = \"response\"\n    )\n  )\n# See the result\nprediction_data\n\n\n\n\n# Make a data frame of predicted probabilities\nprediction_data <- explanatory_data %>% \n  mutate(   \n    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = \"response\")\n  )\n\n# Update the plot\nplt_churn_vs_relationship +\n  # Add points from prediction_data, colored yellow, size 2\n  geom_point(data = prediction_data, color = \"yellow\", size = 2)"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#most-likely-outcome",
    "href": "Introduction_to_Regression_in_R_C4.html#most-likely-outcome",
    "title": "47  Simple logistic regression",
    "section": "47.5 Most likely outcome",
    "text": "47.5 Most likely outcome\nWhen explaining your results to a non-technical audience, you may wish to side-step talking about probabilities and simply explain the most likely outcome. That is, rather than saying there is a 60% chance of a customer churning, you say that the most likely outcome is that the customer will churn. The tradeoff here is easier interpretation at the cost of nuance.\nmdl_churn_vs_relationship, explanatory_data, and plt_churn_vs_relationship are available and"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-4",
    "href": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-4",
    "title": "47  Simple logistic regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUpdate prediction_data to add a column of the most likely churn outcome, most_likely_outcome.\nUpdate plt_churn_vs_relationship, adding yellow points of size 2 with most_likely_outcome as the y aesthetic, using prediction_data.\n\n\n\nE5.R\n\n\n# Update the data frame\nprediction_data <- explanatory_data %>% \n  mutate(   \n    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = \"response\"),\n    # Add the most likely churn outcome\n    most_likely_outcome = round(has_churned)\n  )\n\n# See the result\nprediction_data\n\n\n\n# Update the data frame\nprediction_data <- explanatory_data %>% \n  mutate(   \n    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = \"response\"),\n    most_likely_outcome = round(has_churned)\n  )\n\n# Update the plot\nplt_churn_vs_relationship +\n  # Add most likely outcome points from prediction_data, colored yellow, size 2\n  geom_point(\n    aes(y = most_likely_outcome), \n    data = prediction_data, \n    color = \"yellow\",\n    size = 2\n  )"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#odds-ratio",
    "href": "Introduction_to_Regression_in_R_C4.html#odds-ratio",
    "title": "47  Simple logistic regression",
    "section": "47.6 Odds ratio",
    "text": "47.6 Odds ratio\nOdds ratios compare the probability of something happening with the probability of it not happening. This is sometimes easier to reason about than probabilities, particularly when you want to make decisions about choices. For example, if a customer has a 20% chance of churning, it maybe more intuitive to say “the chance of them not churning is four times higher than the chance of them churning”.\nmdl_churn_vs_relationship, explanatory_data, and plt_churn_vs_relationship are available and"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-5",
    "href": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-5",
    "title": "47  Simple logistic regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUpdate prediction_data to add a column, odds_ratio, of the odds ratios.\nUsing prediction_data, draw a line plot of odds_ratio versus time_since_first_purchase. Add a dotted horizontal line at odds_ratio equal to 1.\n\n\n\nE6.R\n\n# Update the data frame\nprediction_data <- explanatory_data %>% \n  mutate(   \n    has_churned = predict(\n      mdl_churn_vs_relationship, explanatory_data, \n      type = \"response\"\n    ),\n    # Add the odds ratio\n    odds_ratio = has_churned / (1 - has_churned)\n  )\n\n# See the result\nprediction_data\n\n\n\n# From previous step\nprediction_data <- explanatory_data %>% \n  mutate(   \n    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = \"response\"),\n    odds_ratio = has_churned / (1 - has_churned)\n  )\n\n# Using prediction_data, plot odds_ratio vs. time_since_first_purchase\nggplot(prediction_data, aes(time_since_first_purchase, odds_ratio)) +\n  # Make it a line plot\n  geom_line() +\n  # Add a dotted horizontal line at y = 1\n  geom_hline(yintercept = 1, linetype = \"dotted\")"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#log-odds-ratio",
    "href": "Introduction_to_Regression_in_R_C4.html#log-odds-ratio",
    "title": "47  Simple logistic regression",
    "section": "47.7 Log odds ratio",
    "text": "47.7 Log odds ratio\nOne downside to probabilities and odds ratios for logistic regression predictions is that the prediction lines for each are curved. This makes it harder to reason about what happens to the prediction when you make a change to the explanatory variable. The logarithm of the odds ratio (the “log odds ratio”) does have a linear relationship between predicted response and explanatory variable. That means that as the explanatory variable changes, you don’t see dramatic changes in the response metric - only linear changes.\nSince the actual values of log odds ratio are less intuitive than (linear) odds ratio, for visualization purposes it’s usually better to plot the odds ratio and apply a log transformation to the y-axis scale.\nmdl_churn_vs_relationship, explanatory_data, and plt_churn_vs_relationship are available and"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-6",
    "href": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-6",
    "title": "47  Simple logistic regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUpdate prediction_data to add the log odds ratio calculated two ways. Calculate it from the odds_ratio, then directly using predict().\nUpdate the plot to use a logarithmic y-scale.\n\n\n\nE7.R\n\n# Update the data frame\nprediction_data <- explanatory_data %>% \n  mutate(   \n    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = \"response\"),\n    odds_ratio = has_churned / (1 - has_churned),\n    # Add the log odds ratio from odds_ratio\n    log_odds_ratio = log(odds_ratio),\n    # Add the log odds ratio using predict()\n    log_odds_ratio2 = predict(mdl_churn_vs_relationship, explanatory_data)\n  )\n\n# See the result\nprediction_data\n\n\n\n# Update the data frame\nprediction_data <- explanatory_data %>% \n  mutate(   \n    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = \"response\"),\n    odds_ratio = has_churned / (1 - has_churned),\n    log_odds_ratio = log(odds_ratio)\n  )\n\n# Update the plot\nggplot(prediction_data, aes(time_since_first_purchase, odds_ratio)) +\n  geom_line() +\n  geom_hline(yintercept = 1, linetype = \"dotted\") +\n  # Use a logarithmic y-scale\n  scale_y_log10()"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#calculating-the-confusion-matrix",
    "href": "Introduction_to_Regression_in_R_C4.html#calculating-the-confusion-matrix",
    "title": "47  Simple logistic regression",
    "section": "47.8 Calculating the confusion matrix",
    "text": "47.8 Calculating the confusion matrix\nA confusion matrix (occasionally called a confusion table) is the basis of all performance metrics for models with a categorical response (such as a logistic regression). It contains the counts of each actual response-predicted response pair. In this case, where there are two possible responses (churn or not churn), there are four overall outcomes.\nThe customer churned and the model predicted that. The customer churned but the model didn’t predict that. The customer didn’t churn but the model predicted they did. The customer didn’t churn and the model predicted that. churn and mdl_churn_vs_relationship are available."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-7",
    "href": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-7",
    "title": "47  Simple logistic regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the actual responses from the has_churned column of the dataset. Assign to actual_response.\nGet the “most likely” predicted responses from the model. Assign to predicted_response.\nCreate a table of counts from the actual and predicted response vectors. Assign to outcomes.\n\n\n\nE8.R\n\n# Get the actual responses from the dataset\nactual_response <- churn$has_churned\n\n# Get the \"most likely\" predicted responses from the model\npredicted_response <- round(fitted(mdl_churn_vs_relationship))\n\n# Create a table of counts\noutcomes <- table(predicted_response, actual_response)\n\n# See the result\noutcomes"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#measuring-logistic-model-performance",
    "href": "Introduction_to_Regression_in_R_C4.html#measuring-logistic-model-performance",
    "title": "47  Simple logistic regression",
    "section": "47.9 Measuring logistic model performance",
    "text": "47.9 Measuring logistic model performance\nHaving the confusion matrix as a table object is OK, but a little hard to program with. By converting this to a yardstick confusion matrix object, you get methods for plotting and extracting performance metrics.\nThe confusion matrix, outcomes is available as a table object. ggplot2 and yardstick are loaded, and the yardstick.event_first option is set to FALSE."
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-8",
    "href": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-8",
    "title": "47  Simple logistic regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nConvert outcomes to a yardstick confusion matrix. Assign to confusion.\nAutomatically plot confusion.\nGet performance metrics from confusion, remembering that the positive response is in the second column.\n\nRun the tests on the datasummary package.\n\n\nE9.R\n\n# Convert outcomes to a yardstick confusion matrix\nconfusion <- conf_mat(outcomes)\n\n# Plot the confusion matrix\nautoplot(confusion)\n\n# Get performance metrics for the confusion matrix\nsummary(confusion, event_level = \"second\")"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#section",
    "href": "Introduction_to_Regression_in_R_C4.html#section",
    "title": "47  Simple logistic regression",
    "section": "47.10 ",
    "text": "47.10"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-9",
    "href": "Introduction_to_Regression_in_R_C4.html#instructions-100-xp-9",
    "title": "47  Simple logistic regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\nRun the tests on the datasummary package.\n\n\nE10.R"
  },
  {
    "objectID": "Introduction_to_Regression_in_R_C4.html#answers-question-50-xp",
    "href": "Introduction_to_Regression_in_R_C4.html#answers-question-50-xp",
    "title": "47  Simple logistic regression",
    "section": "Answers question 50 XP",
    "text": "Answers question 50 XP"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#fitting-a-parallel-slopes-linear-regression",
    "href": "Intermediate_Regression_in_R_C1.html#fitting-a-parallel-slopes-linear-regression",
    "title": "52  Parallel Slopes",
    "section": "52.1 Fitting a parallel slopes linear regression",
    "text": "52.1 Fitting a parallel slopes linear regression\nIn Introduction to Regression in R, you learned to fit linear regression models with a single explanatory variable. In many cases, using only one explanatory variable limits the accuracy of predictions. That means that to truly master linear regression, you need to be able to include multiple explanatory variables.\nThe case when there is one numeric explanatory variable and one categorical explanatory variable is sometimes called a “parallel slopes” linear regression due to the shape of the predictions—more on that in the next exercise.\nHere, you’ll revisit the Taiwan real estate dataset. Recall the meaning of each variable.\n\nVariable Meaning dist_to_mrt_station_m Distance to nearest MRT metro station, in meters. n_convenience No. of convenience stores in walking distance. house_age_years The age of the house, in years, in 3 groups. price_twd_msq House price per unit area, in New Taiwan dollars per meter squared.\n\ntaiwan_real_estate is available."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#instructions-100-xp",
    "href": "Intermediate_Regression_in_R_C1.html#instructions-100-xp",
    "title": "52  Parallel Slopes",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the taiwan_real_estate dataset, model the house price (in TWD per square meter) versus the number of nearby convenience stores.\nModel the house price (in TWD per square meter) versus the house age (in years). Don’t include an intercept term.\nModel the house price (in TWD per square meter) versus the number of nearby convenience stores plus the house age (in years). Don’t include an intercept term.\n\n\n\nE1.R\n\n# Fit a linear regr'n of price_twd_msq vs. n_convenience\nmdl_price_vs_conv &lt;- lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)\n\n# See the result\nmdl_price_vs_conv\n\n\n\n# Fit a linear regr'n of price_twd_msq vs. house_age_years, no intercept\nmdl_price_vs_age &lt;- lm(price_twd_msq ~ house_age_years + 0, data = taiwan_real_estate)\n\n# See the result\nmdl_price_vs_age\n\n\n\n# Fit a linear regr'n of price_twd_msq vs. n_convenience plus house_age_years, no intercept\nmdl_price_vs_both &lt;- lm(price_twd_msq ~ n_convenience + house_age_years + 0, data = taiwan_real_estate)\n\n# See the result\nmdl_price_vs_both"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#visualizing-each-explanatory-variable",
    "href": "Intermediate_Regression_in_R_C1.html#visualizing-each-explanatory-variable",
    "title": "52  Parallel Slopes",
    "section": "52.2 Visualizing each explanatory variable",
    "text": "52.2 Visualizing each explanatory variable\nBeing able to see the predictions made by a model makes it easier to understand. In the case where there is only one explanatory variable, ggplot lets you do this without any manual calculation or messing about.\nTo visualize the relationship between a numeric explanatory variable and the numeric response, you can draw a scatter plot with a linear trend line.\nTo visualize the relationship between a categorical explanatory variable and the numeric response, you can draw a box plot.\ntaiwan_real_estate is available and ggplot2 is loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-1",
    "href": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-1",
    "title": "52  Parallel Slopes",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the taiwan_real_estate dataset, plot the house price versus the number of nearby convenience stores.\nMake it a scatter plot.\nAdd a smooth linear regression trend line without a standard error ribbon.\nUsing the taiwan_real_estate dataset, plot the house price versus the house age.\nMake it a box plot.\n\n\n\nE2.R\n\n# Using taiwan_real_estate, plot price_twd_msq vs. n_convenience\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +\n  # Add a point layer\n  geom_point() +\n  # Add a smooth trend line using linear regr'n, no ribbon\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n# Using taiwan_real_estate, plot price_twd_msq vs. house_age_years\nggplot(taiwan_real_estate, aes(house_age_years, price_twd_msq)) +\n  # Add a box plot layer\n  geom_boxplot()"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#visualizing-parallel-slopes",
    "href": "Intermediate_Regression_in_R_C1.html#visualizing-parallel-slopes",
    "title": "52  Parallel Slopes",
    "section": "52.3 Visualizing parallel slopes",
    "text": "52.3 Visualizing parallel slopes\nThe two plots in the previous exercise gave very different predictions: one gave a predicted response that increased linearly with a numeric variable; the other gave a fixed response for each category. The only sensible way to reconcile these two conflicting predictions is to incorporate both explanatory variables in the model at once.\nWhen it comes to a linear regression model with a numeric and a categorical explanatory variable, ggplot2 doesn’t have an easy, “out of the box” way to show the predictions. Fortunately, the moderndive package includes an extra geom, geom_parallel_slopes() to make it simple.\ntaiwan_real_estate is available; ggplot2 and moderndive are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-2",
    "href": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-2",
    "title": "52  Parallel Slopes",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the taiwan_real_estate dataset, plot house prices versus the number of nearby convenience stores, colored by house age.\nMake it a scatter plot.\nAdd parallel slopes, without a standard error ribbon.\n\n\n\nE3.R\n\n# Using taiwan_real_estate, plot price_twd_msq vs. n_convenience colored by house_age_years\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) +\n  # Add a point layer\n  geom_point() +\n  # Add parallel slopes, no ribbon\n  geom_parallel_slopes(se = FALSE)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#predicting-with-a-parallel-slopes-model",
    "href": "Intermediate_Regression_in_R_C1.html#predicting-with-a-parallel-slopes-model",
    "title": "52  Parallel Slopes",
    "section": "52.4 Predicting with a parallel slopes model",
    "text": "52.4 Predicting with a parallel slopes model\nWhile ggplot can automatically show you model predictions, in order to get those values to program with, you’ll need to do the calculations yourself.\nJust as with the case of a single explanatory variable, the workflow has two steps: create a data frame of explanatory variables, then add a column of predictions. To make sure you’ve got the right answer, you can add your predictions to the ggplot with the geom_parallel_slopes() lines.\ntaiwan_real_estate and mdl_price_vs_both are available; dplyr, tidyr, and ggplot2 are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-3",
    "href": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-3",
    "title": "52  Parallel Slopes",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nn_convenience should take the numbers zero to ten.\nhouse_age_years should take the unique values of the house_age_years column of taiwan_real_estate.\nAdd a column to the explanatory_data named for the response variable, assigning to prediction_data.\nThe response column contain predictions made using mdl_price_vs_both and explanatory_data.\nUpdate the plot to add a point layer of predictions. Use the prediction_data, set the point size to 5, and the point shape to 15.\n\n\n\nE4.R\n\n# Make a grid of explanatory data\nexplanatory_data &lt;- expand_grid(\n  # Set n_convenience to zero to ten\n  n_convenience = 0:10,\n  # Set house_age_years to the unique values of that variable\n  house_age_years = unique(taiwan_real_estate$house_age_years)\n)\n\n# See the result\nexplanatory_data\n\n\n\n# From previous step\nexplanatory_data &lt;- expand_grid(\n  n_convenience = 0:10,\n  house_age_years = unique(taiwan_real_estate$house_age_years)\n)\n\n# Add predictions to the data frame\nprediction_data &lt;- explanatory_data\nprediction_data$price_twd_msq &lt;- predict(mdl_price_vs_both, explanatory_data)\n\n# See the result\nprediction_data\n\n\n\n\n# From previous steps\nexplanatory_data &lt;- expand_grid(\n  n_convenience = 0:10,\n  house_age_years = unique(taiwan_real_estate$house_age_years)\n)\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(\n    price_twd_msq = predict(mdl_price_vs_both, explanatory_data)\n  )\n\ntaiwan_real_estate %&gt;% \n  ggplot(aes(n_convenience, price_twd_msq, color = house_age_years)) +\n  geom_point() +\n  geom_parallel_slopes(se = FALSE) +\n  # Add points using prediction_data, with size 5 and shape 15\n  geom_point(data = prediction_data, size = 5, shape = 15)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#manually-calculating-predictions",
    "href": "Intermediate_Regression_in_R_C1.html#manually-calculating-predictions",
    "title": "52  Parallel Slopes",
    "section": "52.5 Manually calculating predictions",
    "text": "52.5 Manually calculating predictions\nAs with simple linear regression, you can manually calculate the predictions from the model coefficients. The only change for the parallel slopes case is that the intercept is different for each category of the categorical explanatory variable. That means you need to consider the case when each each category occurs separately.\ntaiwan_real_estate, mdl_price_vs_both, and explanatory_data are available; dplyr is loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-4",
    "href": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-4",
    "title": "52  Parallel Slopes",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the coefficients from mdl_price_vs_both, assigning to coeffs.\nAssign each of the elements of coeffs to the appropriate variable.\nAdd columns to explanatory_data.\n\nTo choose the intercept, in the case when house_age_years is “0 to 15”, choose intercept_0_15. In the case when house_age_years is “15 to 30”, choose intercept_15_30. Do likewise for “30 to 45”.\nManually calculate the predictions as the intercept plus the slope times n_convenience.\n\n\n\n\nE5.R\n\n# Get the coefficients from mdl_price_vs_both\ncoeffs &lt;- coefficients(mdl_price_vs_both)\n\n# Extract the slope coefficient\nslope &lt;- coeffs[1]\n\n# Extract the intercept coefficient for 0 to 15\nintercept_0_15 &lt;- coeffs[2]\n\n# Extract the intercept coefficient for 15 to 30\nintercept_15_30 &lt;- coeffs[3]\n\n# Extract the intercept coefficient for 30 to 45\nintercept_30_45 &lt;- coeffs[4]\n\n\n\n# From previous step\ncoeffs &lt;- coefficients(mdl_price_vs_both)\nslope &lt;- coeffs[1]\nintercept_0_15 &lt;- coeffs[2]\nintercept_15_30 &lt;- coeffs[3]\nintercept_30_45 &lt;- coeffs[4]\n\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(\n    # Consider the 3 cases to choose the intercept\n    intercept = case_when(\n      house_age_years == \"0 to 15\" ~ intercept_0_15,\n      house_age_years == \"15 to 30\" ~ intercept_15_30,\n      house_age_years == \"30 to 45\" ~ intercept_30_45 \n    ),\n    # Manually calculate the predictions\n    price_twd_msq = intercept + slope * n_convenience\n  )\n\n# See the results\nprediction_data"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#comparing-coefficients-of-determination",
    "href": "Intermediate_Regression_in_R_C1.html#comparing-coefficients-of-determination",
    "title": "52  Parallel Slopes",
    "section": "52.6 Comparing coefficients of determination",
    "text": "52.6 Comparing coefficients of determination\nRecall that the coefficient of determination is a measure of how well the linear regression line fits the observed values. An important motivation for including several explanatory variables in a linear regression is that you can improve the fit compared to considering only a single explanatory variable.\nHere you’ll compare the coefficient of determination for the three Taiwan house price models, to see which gives the best result.\nmdl_price_vs_conv, mdl_price_vs_age, and mdl_price_vs_both are available; dplyr and broom are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-5",
    "href": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-5",
    "title": "52  Parallel Slopes",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the unadjusted and adjusted coefficients of determination for mdl_price_vs_conv by glancing at the model, then selecting the r.squared and adj.r.squared values.\nDo the same for mdl_price_vs_age and mdl_price_vs_both.\n\n\n\nE6.R\n\nmdl_price_vs_conv %&gt;% \n  # Get the model-level coefficients\n  glance() %&gt;% \n  # Select the coeffs of determination\n  select(r.squared, adj.r.squared)\n\n# Get the coeffs of determination for mdl_price_vs_age\nmdl_price_vs_age %&gt;% \n  glance() %&gt;% \n  select(r.squared, adj.r.squared)\n\n# Get the coeffs of determination for mdl_price_vs_both\nmdl_price_vs_both %&gt;% \n  glance() %&gt;% \n  select(r.squared, adj.r.squared)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#comparing-residual-standard-error",
    "href": "Intermediate_Regression_in_R_C1.html#comparing-residual-standard-error",
    "title": "52  Parallel Slopes",
    "section": "52.7 Comparing residual standard error",
    "text": "52.7 Comparing residual standard error\nThe other common metric for assessing model fit is the residual standard error (RSE), which measures the typical size of the residuals.\nIn the last exercise you saw how including both explanatory variables into the model increased the coefficient of determination. How do you think using both explanatory variables will change the RSE?\nmdl_price_vs_conv, mdl_price_vs_age, and mdl_price_vs_both are available; dplyr and broom are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-6",
    "href": "Intermediate_Regression_in_R_C1.html#instructions-100-xp-6",
    "title": "52  Parallel Slopes",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the residual standard error for mdl_price_vs_conv by glancing at the model, then pulling the sigma value.\nDo the same for mdl_price_vs_age.\nDo the same for mdl_price_vs_both.\n\n\n\nE7.R\n\nmdl_price_vs_conv %&gt;% \n  # Get the model-level coefficients\n  glance() %&gt;% \n  # Pull out the RSE\n  pull(sigma)\n\n# Get the RSE for mdl_price_vs_age\nmdl_price_vs_age %&gt;% \n  glance() %&gt;% \n  pull(sigma)\n\n# Get the RSE for mdl_price_vs_both\nmdl_price_vs_both %&gt;% \n  glance() %&gt;% \n  pull(sigma)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#one-model-per-category",
    "href": "Intermediate_Regression_in_R_C2.html#one-model-per-category",
    "title": "53  Interactions",
    "section": "53.1 One model per category",
    "text": "53.1 One model per category\nThe model you ran on the whole dataset fits some parts of the data better than others. It’s worth taking a look at what happens when you run a linear model on different parts of the dataset separately, to see if each model agrees or disagrees with the others.\ntaiwan_real_estate is available; dplyr is loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#instructions-100-xp",
    "href": "Intermediate_Regression_in_R_C2.html#instructions-100-xp",
    "title": "53  Interactions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFilter taiwan_real_estate for rows where house_age_years is “0 to 15”, assigning to taiwan_0_to_15.\nRepeat this for the “15 to 30” and “30 to 45” house age categories.\nRun a linear regression of price_twd_msq versus n_convenience using the taiwan_0_to_15 dataset.\nRepeat this for taiwan_15_to_30 and taiwan_30_to_45.\n\n\n\nE1.R\n\n# Filter for rows where house age is 0 to 15 years\ntaiwan_0_to_15 &lt;- taiwan_real_estate %&gt;%\n  filter(house_age_years == \"0 to 15\")\n\n# Filter for rows where house age is 15 to 30 years\ntaiwan_15_to_30 &lt;- taiwan_real_estate %&gt;%\n  filter(house_age_years == \"15 to 30\")\n\n# Filter for rows where house age is 30 to 45 years\ntaiwan_30_to_45 &lt;- taiwan_real_estate %&gt;%\n  filter(house_age_years == \"30 to 45\")\n  \n  \n\n# From previous step\ntaiwan_0_to_15 &lt;- taiwan_real_estate %&gt;%\n  filter(house_age_years == \"0 to 15\")\ntaiwan_15_to_30 &lt;- taiwan_real_estate %&gt;%\n  filter(house_age_years == \"15 to 30\")\ntaiwan_30_to_45 &lt;- taiwan_real_estate %&gt;%\n  filter(house_age_years == \"30 to 45\")\n\n# Model price vs. no. convenience stores using 0 to 15 data\nmdl_0_to_15 &lt;- lm(price_twd_msq ~ n_convenience, data = taiwan_0_to_15)\n\n# Model price vs. no. convenience stores using 15 to 30 data\nmdl_15_to_30 &lt;- lm(price_twd_msq ~ n_convenience, data = taiwan_15_to_30)\n\n# Model price vs. no. convenience stores using 30 to 45 data\nmdl_30_to_45 &lt;- lm(price_twd_msq ~ n_convenience, data = taiwan_30_to_45)\n\n# See the results\nmdl_0_to_15\nmdl_15_to_30\nmdl_30_to_45"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#predicting-multiple-models",
    "href": "Intermediate_Regression_in_R_C2.html#predicting-multiple-models",
    "title": "53  Interactions",
    "section": "53.2 Predicting multiple models",
    "text": "53.2 Predicting multiple models\nIn order to see what each of the models for individual categories are doing, it’s helpful to make predictions from them. The flow is exactly the same as the flow for making predictions on the whole model, though remember that you only have a single explanatory variable in these models (so expand_grid() isn’t needed.)\nThe models mdl_0_to_15, mdl_15_to_30 and mdl_30_to_45 are available; dplyr is loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-1",
    "href": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-1",
    "title": "53  Interactions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a tibble of explanatory data, setting n_convenience to a vector from zero to ten, assigning to explanatory_data_0_to_15.\nAdd a column of predictions named price_twd_msq to explanatory_data, using mdl_0_to_15 and explanatory_data. Assign to prediction_data_0_to_15.\nRepeat this for the 15 to 30 year and 30 to 45 year house age categories.\n\n\n\nE2.R\n\n# Create a tibble of explanatory data, setting no. of conv stores to 0 to 10\nexplanatory_data &lt;- tibble(\n  n_convenience = 0:10\n)\n\n\n\n# From previous step\nexplanatory_data &lt;- tibble(\n  n_convenience = 0:10\n)\n\n# Add column of predictions using \"0 to 15\" model and explanatory data \nprediction_data_0_to_15 &lt;- explanatory_data %&gt;% \n  mutate(\n    mass_g = predict(mdl_0_to_15, explanatory_data),\n    )\n\n# Same again, with \"15 to 30\"\nprediction_data_15_to_30 &lt;- explanatory_data %&gt;% \n  mutate(\n    mass_g = predict(mdl_15_to_30, explanatory_data),\n    \n)\n\n# Same again, with \"30 to 45\"\nprediction_data_30_to_45 &lt;- explanatory_data %&gt;% \n  mutate(\n    mass_g = predict(mdl_30_to_45, explanatory_data),\n)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#visualizing-multiple-models",
    "href": "Intermediate_Regression_in_R_C2.html#visualizing-multiple-models",
    "title": "53  Interactions",
    "section": "53.3 Visualizing multiple models",
    "text": "53.3 Visualizing multiple models\nIn the last two exercises, you ran models for each category of house ages separately, then calculated predictions for each model. Now it’s time to visualize those predictions to see how they compare.\nWhen you use geom_smooth() in a ggplot with an aesthetic that splits the dataset into groups and draws a line for each group (like the color aesthetic), you get multiple trend lines. This is the same as running a model on each group separately, so we get a chance to test our predictions against ggplot’s.\ntaiwan_real_estate, prediction_data_0_to_15, prediction_data_15_to_30, and prediction_data_30_to_45 are available; ggplot2 is loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-2",
    "href": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-2",
    "title": "53  Interactions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing taiwan_real_estate, plot price_twd_msq versus n_convenience colored by house_age_years.\nAdd a point layer.\nAdd smooth trend lines for each color using the linear regression method and turning off the standard error ribbon.\nExtend the plot by adding the prediction points from prediction_data_0_to_15. Color them red, with size 3 and shape 15.\nAdd prediction points from prediction_data_15_to_30, colored green, size 3, and shape 15.\nAdd prediction points from prediction_data_30_to_45, colored blue, size 3, and shape 15.\n\n\n\nE3.R\n\n# Using taiwan_real_estate, plot price vs. no. of conv. stores colored by house age\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) +\n  # Make it a scatter plot\n  geom_point() +\n  # Add smooth linear regression trend lines, no ribbon\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n# Extend the plot to include prediction points\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Add points using prediction_data_0_to_15, colored red, size 3, shape 15\n   geom_point(data = prediction_data_0_to_15, size = 3, shape = 15, color = \"red\") +\n  # Add points using prediction_data_15_to_30, colored green, size 3, shape 15\n  geom_point(data = prediction_data_15_to_30, size = 3, shape = 15, color = \"green\") +\n  # Add points using prediction_data_30_to_45, colored blue, size 3, shape 15\n  geom_point(data = prediction_data_30_to_45, size = 3, shape = 15, color = \"blue\")"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#assessing-model-performance",
    "href": "Intermediate_Regression_in_R_C2.html#assessing-model-performance",
    "title": "53  Interactions",
    "section": "53.4 Assessing model performance",
    "text": "53.4 Assessing model performance\nTo test which approach is best—the whole dataset model or the models for each house age category—you need to calculate some metrics. Here’s, you’ll compare the coefficient of determination and the residual standard error for each model.\nFour models of price versus no. of convenience stores (mdl_all_ages, mdl_0_to_15, mdl_15_to_30, and mdl_30_to_45) are available; dplyr and broom are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-3",
    "href": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-3",
    "title": "53  Interactions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the coefficient of determination for mdl_all_ages, mdl_0_to_15, mdl_15_to_30, and mdl_30_to_45.\nGet the residual standard error for mdl_all_ages, mdl_0_to_15, mdl_15_to_30, and mdl_30_to_45.\n\n\n\nE4.R\n\n# Get the coeff. of determination for mdl_all_ages\nmdl_all_ages %&gt;% \n  glance() %&gt;% \n  pull(r.squared)\n\n# Get the coeff. of determination for mdl_0_to_15\nmdl_0_to_15 %&gt;% \n  glance() %&gt;% \n  pull(r.squared)\n\n# Get the coeff. of determination for mdl_15_to_30\nmdl_15_to_30 %&gt;% \n  glance() %&gt;% \n  pull(r.squared)\n\n# Get the coeff. of determination for mdl_30_to_45\nmdl_30_to_45 %&gt;% \n  glance() %&gt;% \n  pull(r.squared)\n\n\n\n\n# Get the RSE for mdl_all\nmdl_all_ages %&gt;% \n  glance() %&gt;% \n  pull(sigma)\n\n# Get the RSE for mdl_0_to_15\nmdl_0_to_15 %&gt;% \n  glance() %&gt;% \n  pull(sigma)\n\n# Get the RSE for mdl_15_to_30\nmdl_15_to_30 %&gt;% \n  glance() %&gt;% \n  pull(sigma)\n\n# Get the RSE for mdl_30_to_45\nmdl_30_to_45 %&gt;% \n  glance() %&gt;% \n  pull(sigma)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#specifying-an-interaction",
    "href": "Intermediate_Regression_in_R_C2.html#specifying-an-interaction",
    "title": "53  Interactions",
    "section": "53.5 Specifying an interaction",
    "text": "53.5 Specifying an interaction\nSo far you used a single parallel slopes model, which gave an OK fit for the whole dataset, then three separate models for each house age category, which gave a better fit for each individual category, but was clunky because you had three separate models to work with and explain. Ideally, you’d have a single model that had all the predictive power of the individual models.\nDefining this single model is achieved through adding interactions between explanatory variables. R’s formula syntax is flexible, and gives you a couple of options, depending on whether you prefer concise code that is quick to type and to read, or explicit code that describes what you are doing in detail.\ntaiwan_real_estate is available."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-4",
    "href": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-4",
    "title": "53  Interactions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFit a linear regression of price_twd_msq versus n_convenience and house_age_years and their interaction, using the “times” syntax to implicitly generate the interaction.\nFit a linear regression of price_twd_msq versus n_convenience and house_age_years and their interaction, using the “colon” syntax to explicitly generate the interaction.\n\n\n\nE5.R\n\n# Model price vs both with an interaction using \"times\" syntax\nlm(price_twd_msq ~ n_convenience * house_age_years, data = taiwan_real_estate)\n\n\n\n\n# Model price vs both with an interaction using \"colon\" syntax\nlm(\n  price_twd_msq ~ n_convenience + house_age_years + n_convenience:house_age_years, \n  data = taiwan_real_estate\n)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#interactions-with-understandable-coeffs",
    "href": "Intermediate_Regression_in_R_C2.html#interactions-with-understandable-coeffs",
    "title": "53  Interactions",
    "section": "53.6 Interactions with understandable coeffs",
    "text": "53.6 Interactions with understandable coeffs\nThe previous model with the interaction term returned coefficients that were a little tricky to interpret. In order clarify what the model is predicting, you can reformulate the model in a way that returns understandable coefficients. For further clarity, you can compare the results to the models on the separate house age categories (mdl_0_to_15, mdl_15_to_30, and mdl_30_to_45).\ntaiwan_real_estate, mdl_0_to_15, mdl_15_to_30, and mdl_30_to_45 are available."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-5",
    "href": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-5",
    "title": "53  Interactions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFit a linear regression of price_twd_msq versus house_age_years plus an interaction between n_convenience and house_age_years, and no global intercept, using the taiwan_real_estate dataset.\nFor comparison, get the coefficients for the three models for each category: mdl_0_to_15, mdl_15_to_30, and mdl_30_to_45.\n\n\n\nE6.R\n\n# Model price vs. house age plus an interaction, no intercept\nmdl_readable_inter &lt;- lm(\n  price_twd_msq ~ house_age_years + n_convenience:house_age_years + 0, \n  data = taiwan_real_estate\n)\n\n# See the result\nmdl_readable_inter\n\n# Get coefficients for mdl_0_to_15\ncoefficients(mdl_0_to_15)\n\n# Get coefficients for mdl_15_to_30\ncoefficients(mdl_15_to_30)\n\n# Get coefficients for mdl_30_to_45\ncoefficients(mdl_30_to_45)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#predicting-with-interactions",
    "href": "Intermediate_Regression_in_R_C2.html#predicting-with-interactions",
    "title": "53  Interactions",
    "section": "53.7 Predicting with interactions",
    "text": "53.7 Predicting with interactions\nAs with every other regression model you’ve created, the fun part is making predictions. Fortunately, the code flow for this case is the same as the one without interactions—R can handle calculating the interactions without any extra prompting from you. The only thing you need to remember is the trick for getting combinations of explanatory variables.\nmdl_price_vs_both_inter is available; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-6",
    "href": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-6",
    "title": "53  Interactions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nMake a grid of explanatory data, formed from combinations of the following variables.\n\nn_convenience should take the numbers zero to ten.\nhouse_age_years should take the unique values of the house_age_years column of taiwan_real_estate.\n\nAdd a column to the explanatory_data, assigning to prediction_data.\nThe column should be named after the response variable, and contain predictions made using mdl_price_vs_both_inter and explanatory_data.\nUsing taiwan_real_estate, plot price_twd_msq versus n_convenience, colored by house_age_years.\nAdd a point layer.\nAdd smooth trend lines using linear regression, no standard error ribbon.\nAdd another point layer using prediction_data, with size 5 and shape 15.\n\n\n\nE7.R\n\n# Make a grid of explanatory data\nexplanatory_data &lt;- expand_grid(\n  # Set n_convenience to zero to ten\n  n_convenience = 0:10,\n  # Set house_age_years to the unique values of that variable\n  house_age_years = unique(taiwan_real_estate$house_age_years)\n)\n\n# See the result\nexplanatory_data\n\n\n\n\n# From previous step\nexplanatory_data &lt;- expand_grid(\n  n_convenience = 0:10,\n  house_age_years = unique(taiwan_real_estate$house_age_years)\n)\n\n# Add predictions to the data frame\nprediction_data &lt;- explanatory_data %&gt;%\n  mutate(price_twd_msq = predict(mdl_price_vs_both_inter, explanatory_data))\n\n# See the result\nprediction_data\n\n\n\n\n# From previous step\nexplanatory_data &lt;- expand_grid(\n  n_convenience = 0:10,\n  house_age_years = unique(taiwan_real_estate$house_age_years)\n)\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(\n    price_twd_msq = predict(mdl_price_vs_both_inter, explanatory_data)\n  )\n\n# Using taiwan_real_estate, plot price vs. no. of convenience stores, colored by house age\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) +\n  # Make it a scatter plot\n  geom_point() +\n  # Add linear regression trend lines, no ribbon\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # Add points from prediction_data, size 5, shape 15\n  geom_point(data = prediction_data, size = 5, shape = 15)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#manually-calculating-predictions-with-interactions",
    "href": "Intermediate_Regression_in_R_C2.html#manually-calculating-predictions-with-interactions",
    "title": "53  Interactions",
    "section": "53.8 Manually calculating predictions with interactions",
    "text": "53.8 Manually calculating predictions with interactions\nIn order to understand how predict() works, it’s time to calculate the predictions manually again. For this model, there are three separate lines to calculate for, and in each one, the prediction is an intercept plus a slope times the numeric explanatory value. The tricky part is getting the right intercept and the right slope for each case.\nmdl_price_vs_both_inter and explanatory_data are available; dplyr and tidyr are available."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-7",
    "href": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-7",
    "title": "53  Interactions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the coefficients from mdl_price_vs_both_inter, assigning to coeffs.\nGet the three intercept coefficients from coeffs, assigning to intercept_0_15, intercept_15_30, and intercept_30_45.\nGet the three slope coefficients from coeffs, assigning to slope_0_15, slope_15_30, and slope_30_45.\n\n\n\nE8.R\n\n\n Get the intercept for 0 to 15 year age group\nintercept_0_15 &lt;- coeffs[1]\n\n# Get the intercept for 15 to 30 year age group\nintercept_15_30 &lt;- coeffs[2]\n\n# Get the intercept for 30 to 45 year age group\nintercept_30_45 &lt;- coeffs[3]\n\n# Get the slope for 0 to 15 year age group\nslope_0_15 &lt;- coeffs[4]\n\n# Get the slope for 15 to 30 year age group\nslope_15_30 &lt;- coeffs[5]\n\n# Get the slope for 30 to 45 year age group\nslope_30_45 &lt;- coeffs[6]"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#modeling-ebay-auctions",
    "href": "Intermediate_Regression_in_R_C2.html#modeling-ebay-auctions",
    "title": "53  Interactions",
    "section": "53.9 Modeling eBay auctions",
    "text": "53.9 Modeling eBay auctions\nSometimes modeling a whole dataset suggests trends that disagree with models on separate parts of that dataset. This is known as Simpson’s paradox. In the most extreme case, you may see a positive slope on the whole dataset, and negative slopes on every subset of that dataset (or the other way around).\nOver the next few exercises, you’ll look at eBay auctions of Palm Pilot M515 PDA models.\nvariable meaning price Final sale price, USD openbid The opening bid, USD auction_type How long did the auction last?\nauctions is available; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-8",
    "href": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-8",
    "title": "53  Interactions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nLook at the structure of the auctions dataset and familiarize yourself with its columns.\nFit a linear regression model of price versus openbid, using the auctions dataset. Look at the coefficients.\nUsing auctions, plot price versus openbid as a scatter plot with linear regression trend lines (no ribbon). Look at the trend line.\n\n\n\nE9.R\n\n# Take a glimpse at the dataset\nglimpse(auctions)\n\n# Model price vs. opening bid using auctions\nmdl_price_vs_openbid &lt;- lm(price ~ openbid, data = auctions)\n\n# See the result\nmdl_price_vs_openbid\n\n\n\n\n# Using auctions, plot price vs. opening bid as a scatter plot with linear regression trend lines\nggplot(auctions, aes(openbid, price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#modeling-each-auction-type",
    "href": "Intermediate_Regression_in_R_C2.html#modeling-each-auction-type",
    "title": "53  Interactions",
    "section": "53.10 Modeling each auction type",
    "text": "53.10 Modeling each auction type\nYou just saw that the opening bid price appeared not to affect the final sale price of Palm Pilots in the eBay auctions. Now let’s look at what happens when you model the three auction types (3 day, 5 day, and 7 day) separately.\nauctions is available; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-9",
    "href": "Intermediate_Regression_in_R_C2.html#instructions-100-xp-9",
    "title": "53  Interactions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFit a linear regression model of price versus openbid and auction_type, with an interaction, using the auctions dataset. Look at the coefficients.\nUsing auctions, plot price versus openbid, colored by auction_type, as a scatter plot with linear regression trend lines (no ribbon). Look at the trend lines.\n\n\n\nE10.R\n\n# Fit linear regression of price vs. opening bid and auction type, with an interaction.\nmdl_price_vs_both &lt;- lm(\n  price ~ auction_type + openbid:auction_type + 0, # or price ~ auction_type * openbid\n  data = auctions\n)\n\n# See the result\nmdl_price_vs_both\n\n\n\n\n# Using auctions, plot price vs. opening bid colored by auction type as a scatter plot with linear regr'n trend lines\nggplot(auctions, aes(openbid, price, color = auction_type)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#d-visualizations",
    "href": "Intermediate_Regression_in_R_C3.html#d-visualizations",
    "title": "54  Multiple Linear Regression",
    "section": "54.1 3D visualizations",
    "text": "54.1 3D visualizations\nSince computer screens and paper are both two-dimensional objects, most plots are best suited to visualizing two variables at once. For the case of three continuous variables, you can draw a 3D scatter plot, but perspective problems usually make it difficult to interpret. There are some “flat” alternatives that provide easier interpretation, though they require a little thinking about to make.\ntaiwan_real_estate is available; magrittr, plot3D and ggplot2 are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#instructions-100-xp",
    "href": "Intermediate_Regression_in_R_C3.html#instructions-100-xp",
    "title": "54  Multiple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nWith the taiwan_real_estate dataset, draw a 3D scatter plot of the number of nearby convenience stores on the x-axis, the square-root of the distance to the nearest MRT stop on the y-axis, and the house price on the z-axis.\nWith the taiwan_real_estate dataset, draw a scatter plot of the square-root of the distance to the nearest MRT stop versus the number of nearby convenience stores, colored by house price.\nUse the continuous viridis color scale, using the “plasma” option.\n\n\n\nE1.R\n\n# With taiwan_real_estate, draw a 3D scatter plot of no. of conv. stores, sqrt dist to MRT, and price\nscatter3D\n\nscatter3D(taiwan_real_estate$n_convenience, sqrt(taiwan_real_estate$dist_to_mrt_m), taiwan_real_estate$price_twd_msq)\n\n\n\n\n# Using taiwan_real_estate, plot sqrt dist to MRT vs. no. of conv stores, colored by price\nggplot(taiwan_real_estate,\naes(n_convenience, sqrt(dist_to_mrt_m), color = price_twd_msq)) + \n  # Make it a scatter plot\n  geom_point() +\n  # Use the continuous viridis plasma color scale\n  scale_color_viridis_c (option=\"plasma\")"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#modeling-2-numeric-explanatory-variables",
    "href": "Intermediate_Regression_in_R_C3.html#modeling-2-numeric-explanatory-variables",
    "title": "54  Multiple Linear Regression",
    "section": "54.2 Modeling 2 numeric explanatory variables",
    "text": "54.2 Modeling 2 numeric explanatory variables\nYou already saw how to make a model and predictions with a numeric and a categorical explanatory variable. The code for modeling and predicting with two numeric explanatory variables in the same, other than a slight difference in how to specify the explanatory variables to make predictions against.\nHere you’ll model and predict the house prices against the number of nearby convenience stores and the square-root of the distance to the nearest MRT station.\ntaiwan_real_estate is available; dplyr, tidyr and ggplot2 are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-1",
    "href": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-1",
    "title": "54  Multiple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFit a linear regression of house price versus the number of convenience stores and the square-root of the distance to the nearest MRT stations, without an interaction, using the taiwan_real_estate dataset.\nCreate expanded grid of explanatory variables with number of convenience stores from 0 to 10 and the distance to the nearest MRT station as a sequence from 0 to 80 in steps of 10, all squared (0, 100, 400, …, 6400). Assign to explanatory_data.\nAdd a column of predictions to explanatory_data using mdl_price_vs_conv_dist and explanatory_data. Assign to prediction_data.\nExtend the plot to add a layer of points using the prediction data, colored yellow, with size 3.\n\n\n\nE2.R\n\n# Fit a linear regression of price vs. no. of conv. stores and sqrt dist. to nearest MRT, no interaction\nmdl_price_vs_conv_dist &lt;- lm(price_twd_msq ~ n_convenience + sqrt(dist_to_mrt_m), data = taiwan_real_estate)\n\n# See the result\nmdl_price_vs_conv_dist\n\n\n\n\n# From previous step \nmdl_price_vs_conv_dist &lt;- lm(price_twd_msq ~ n_convenience + sqrt(dist_to_mrt_m), data = taiwan_real_estate)\n\n# Create expanded grid of explanatory variables with no. of conv. stores and  dist. to nearest MRT\nexplanatory_data &lt;- expand_grid(\n  n_convenience = 0:10,\n  dist_to_mrt_m = seq(0, 80, 10) ^ 2\n)\n\n# Add predictions using mdl_price_vs_conv_dist and explanatory_data\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(\n    price_twd_msq = predict(mdl_price_vs_conv_dist, explanatory_data)\n  )\n\n# See the result\nprediction_data\n\n\n\n\n# From previous steps\nmdl_price_vs_conv_dist &lt;- lm(price_twd_msq ~ n_convenience + sqrt(dist_to_mrt_m), data = taiwan_real_estate)\nexplanatory_data &lt;- expand_grid(n_convenience = 0:10, dist_to_mrt_m = seq(0, 80, 10) ^ 2)\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(price_twd_msq = predict(mdl_price_vs_conv_dist, explanatory_data))\n\n# Add predictions to plot\nggplot(\n  taiwan_real_estate, \n  aes(n_convenience, sqrt(dist_to_mrt_m), color = price_twd_msq)\n) + \n  geom_point() +\n  scale_color_viridis_c(option = \"plasma\")+\n  # Add prediction points colored yellow, size 3\n  geom_point(data = prediction_data, color = \"yellow\", size = 3)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#including-an-interaction",
    "href": "Intermediate_Regression_in_R_C3.html#including-an-interaction",
    "title": "54  Multiple Linear Regression",
    "section": "54.3 Including an interaction",
    "text": "54.3 Including an interaction\nJust as in the case with one numeric and one categorical explanatory variable, it is possible that numeric explanatory variables can interact. With this model structure, you’ll get a third slope coefficient: one for each explanatory variable and one for the interaction.\nHere you’ll run and predict the same model as in the previous exercise, but this time including an interaction between the explanatory variables.\ntaiwan_real_estate is available; dplyr, tidyr and ggplot2 are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-2",
    "href": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-2",
    "title": "54  Multiple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFit a linear regression of house price versus the number of convenience stores and the square-root of the distance to the nearest MRT stations, with an interaction, using the taiwan_real_estate dataset.\nCreate expanded grid of explanatory variables with number of convenience stores from 0 to 10 and the distance to the nearest MRT station as a sequence from 0 to 80 in steps of 10, all squared (0, 100, 400, …, 6400). Assign to explanatory_data.\nAdd a column of predictions to explanatory_data using mdl_price_vs_conv_dist and explanatory_data. Assign to prediction_data.\nExtend the plot to add a layer of points using the prediction data, colored yellow, with size 3.\n\n\n\nE3.R\n\n# Fit a linear regression of price vs. no. of conv. stores and sqrt dist. to nearest MRT, with interaction\nmdl_price_vs_conv_dist &lt;- lm(\n  price_twd_msq ~ n_convenience * sqrt(dist_to_mrt_m), data = taiwan_real_estate\n)\n\n# See the result\nmdl_price_vs_conv_dist\n\n\n\n# From previous step \nmdl_price_vs_conv_dist &lt;- lm(price_twd_msq ~ n_convenience * sqrt(dist_to_mrt_m), data = taiwan_real_estate)\n\n# Create expanded grid of explanatory variables with no. of conv. stores and  dist. to nearest MRT\nexplanatory_data &lt;- expand_grid(\n  n_convenience = 0:10,\n  dist_to_mrt_m = seq(0, 80, 10) ^ 2\n)\n\n# Add predictions using mdl_price_vs_conv_dist and explanatory_data\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(\n    price_twd_msq = predict(mdl_price_vs_conv_dist, explanatory_data)\n  )\n\n# See the result\nprediction_data\n\n\n\n\n# From previous steps\nmdl_price_vs_conv_dist &lt;- lm(price_twd_msq ~ n_convenience * sqrt(dist_to_mrt_m), data = taiwan_real_estate)\nexplanatory_data &lt;- expand_grid(n_convenience = 0:10, dist_to_mrt_m = seq(0, 80, 10) ^ 2)\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(price_twd_msq = predict(mdl_price_vs_conv_dist, explanatory_data))\n\n# Add predictions to plot\nggplot(\n  taiwan_real_estate, \n  aes(n_convenience, sqrt(dist_to_mrt_m), color = price_twd_msq)\n) + \n  geom_point() +\n  scale_color_viridis_c(option = \"plasma\") +\n  # Add prediction points colored yellow, size 3\n  geom_point(data = prediction_data, color = \"yellow\", size = 3)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#visualizing-many-variables",
    "href": "Intermediate_Regression_in_R_C3.html#visualizing-many-variables",
    "title": "54  Multiple Linear Regression",
    "section": "54.4 Visualizing many variables",
    "text": "54.4 Visualizing many variables\nAs you begin to consider more variables, plotting them all at the same time becomes increasingly difficult. In addition to using x and y scales for two numeric variables, you can use color for a third numeric variable, and you can use faceting for categorical variables. And that’s about your limit before the plots become to difficult to interpret. There are some specialist plot types like correlation heatmaps and parallel coordinates plots that will handle more variables, but they give you much less information about each variable, and they aren’t great for visualizing model predictions.\nHere you’ll push the limits of the scatter plot by showing the house price, the distance to the MRT station, the number of nearby convenience stores, and the house age, all together in one plot.\ntaiwan_real_estate is available; ggplot2 is loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-3",
    "href": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-3",
    "title": "54  Multiple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the taiwan_real_estate dataset, draw a scatter plot of n_convenience versus the square root of dist_to_mrt_m, colored by price_twd_msq.\nUse the continuous viridis plasma color scale.\nFacet the plot, wrapping by house_age_years.\n\n\n\nE4.R\n\n# Using taiwan_real_estate, no. of conv. stores vs. sqrt of dist. to MRT, colored by plot house price\nggplot(\n  taiwan_real_estate, \n  aes(sqrt(dist_to_mrt_m), n_convenience, color = price_twd_msq)\n) +\n  # Make it a scatter plot\n  geom_point() +\n  # Use the continuous viridis plasma color scale\n  scale_color_viridis_c(option = \"plasma\") +\n  # Facet, wrapped by house age\n  facet_wrap(vars(house_age_years))"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#different-levels-of-interaction",
    "href": "Intermediate_Regression_in_R_C3.html#different-levels-of-interaction",
    "title": "54  Multiple Linear Regression",
    "section": "54.5 Different levels of interaction",
    "text": "54.5 Different levels of interaction\nOnce you have three explanatory variables, the number of options for specifying interactions increases. You can specify no interactions. You can specify 2-way interactions, which gives you model coefficients for each pair of variables. The third option is to specify all the interactions, which means the three 2-way interactions and and interaction between all three explanatory variables.\nAs the number of explanatory variables increases further, the number of interaction possibilities rapidly increases.\ntaiwan_real_estate is available."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-4",
    "href": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-4",
    "title": "54  Multiple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFit a linear regression of house price versus n_convenience, the square-root of dist_to_mrt_m, and house_age_years. Don’t include a global intercept, and don’t include any interactions.\nFit a linear regression of house price versus the square-root of dist_to_mrt_m, n_convenience, and house_age_years. Don’t include a global intercept, but do include 2-way and 3-way interactions between the explanatory variables.\nFit a linear regression of house price versus the square-root of dist_to_mrt_m, n_convenience, and house_age_years. Don’t include a global intercept, but do include 2-way (not 3-way) interactions between the explanatory variables.\n\n\n\nE5.R\n\n# Model price vs. no. of conv. stores, sqrt dist. to MRT station & house age, no global intercept, no interactions\nmdl_price_vs_all_no_inter &lt;- lm(\n  price_twd_msq ~ n_convenience + sqrt(dist_to_mrt_m) + house_age_years + 0, \n  data = taiwan_real_estate\n)\n\n# See the result\nmdl_price_vs_all_no_inter\n\n\n\n\n\n# Model price vs. sqrt dist. to MRT station, no. of conv. stores & house age, no global intercept, 3-way interactions\nmdl_price_vs_all_3_way_inter &lt;- lm(\n  price_twd_msq ~ sqrt(dist_to_mrt_m) * n_convenience * house_age_years + 0, \n  data = taiwan_real_estate\n)\n\n# See the result\nmdl_price_vs_all_3_way_inter\n\n\n\n# Model price vs. sqrt dist. to MRT station, no. of conv. stores & house age, no global intercept, 2-way interactions\nmdl_price_vs_all_2_way_inter &lt;- lm(\n  price_twd_msq ~ (sqrt(dist_to_mrt_m) + n_convenience + house_age_years) ^ 2 + 0, \n  data = taiwan_real_estate\n)\n\n# See the result\nmdl_price_vs_all_2_way_inter"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#predicting-again",
    "href": "Intermediate_Regression_in_R_C3.html#predicting-again",
    "title": "54  Multiple Linear Regression",
    "section": "54.6 Predicting again",
    "text": "54.6 Predicting again\nYou’ve followed the prediction workflow several times now with different combinations of explanatory variables. Time to try it once more on the model with three explanatory variables. Here, you’ll use the model with 3-way interactions, though the code is the same when using any of the three models from the previous exercise.\ntaiwan_real_estate and mdl_price_vs_all_3_way_inter are available; dplyr, tidyr and ggplot2 are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-5",
    "href": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-5",
    "title": "54  Multiple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nMake a grid of explanatory data, formed from combinations of the following variables.\n\ndist_to_mrt_m should take a sequence from zero to eighty in steps of ten, all squared (0, 100, 400, …, 6400).\nn_convenience should take the numbers zero to ten.\nhouse_age_years should take the unique values of the house_age_years column of taiwan_real_estate.\n\nAdd a column to the explanatory_data, assigning to prediction_data.\nThe column should be named after the response variable, and contain predictions made using mdl_price_vs_all_3_way_inter and explanatory_data.\nExtend the plot to include predictions as points from prediction_data, with size 3 and shape 15.\nLook at the plot. What do the prediction points tell you?\n\n\n\nE6.R\n\n# Make a grid of explanatory data\nexplanatory_data &lt;- expand_grid(\n  # Set dist_to_mrt_m a seq from 0 to 80 by 10s, squared\n  dist_to_mrt_m = seq(0, 80, 10) ^ 2,\n  # Set n_convenience to 0 to 10\n  n_convenience = 0:10,\n  # Set house_age_years to the unique values of that variable\n  house_age_years = unique(taiwan_real_estate$house_age_years)\n)\n\n# See the result\nexplanatory_data\n\n\n\n# From previous step\nexplanatory_data &lt;- expand_grid(\n  dist_to_mrt_m = seq(0, 80, 10) ^ 2,\n  n_convenience = 0:10,\n  house_age_years = unique(taiwan_real_estate$house_age_years)\n)\n\n# Add predictions to the data frame\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(\n    price_twd_msq = predict(mdl_price_vs_all_3_way_inter, explanatory_data)\n  )\n\n# See the result\nprediction_data\n\n\n\n\n# From previous step\nexplanatory_data &lt;- expand_grid(\n  dist_to_mrt_m = seq(0, 80, 10) ^ 2,\n  n_convenience = 0:10,\n  house_age_years = unique(taiwan_real_estate$house_age_years)\n)\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(price_twd_msq = predict(mdl_price_vs_all_3_way_inter, explanatory_data))\n\n# Extend the plot\nggplot(\n  taiwan_real_estate, \n  aes(sqrt(dist_to_mrt_m), n_convenience, color = price_twd_msq)\n) +\n  geom_point() +\n  scale_color_viridis_c(option = \"plasma\") +\n  facet_wrap(vars(house_age_years)) +\n  # Add points from prediction data, size 3, shape 15\n    geom_point(data = prediction_data, color = \"yellow\", size = 3, shape = 15)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#linear-regression-algorithm",
    "href": "Intermediate_Regression_in_R_C3.html#linear-regression-algorithm",
    "title": "54  Multiple Linear Regression",
    "section": "54.7 Linear regression algorithm",
    "text": "54.7 Linear regression algorithm\nTo truly understand linear regression, it is helpful to know how the algorithm works. The code for lm() is hundreds of lines because it has to work with any formula and any dataset. However, in the case of simple linear regression for a single dataset, you can implement a linear regression algorithm in just a few lines of code.\nThe workflow is\nWrite a script to calculate the sum of squares. Turn this into a function. Use R’s general purpose optimization function find the coefficients that minimize this. The explanatory values (the n_convenience column of taiwan_real_estate) are available as x_actual. The response values (the price_twd_msq column of taiwan_real_estate) are available as y_actual."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-6",
    "href": "Intermediate_Regression_in_R_C3.html#instructions-100-xp-6",
    "title": "54  Multiple Linear Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSet the intercept to ten.\nSet the slope to one.\nCalculate the predicted y-values as the intercept plus the slope times the actual x-values.\nCalculate the differences between actual and predicted y-values.\nCalculate the sum of squares. Get the sum of the differences in y-values, squaring each value.\nComplete the function body.\n\nGet the intercept from the first element of coeffs.\nGet the slope from the second element of coeffs.\nCalculate the predicted y-values as the intercept plus the slope times the actual x-values.\nCalculate the differences between actual and predicted y-values.\nCalculate the sum of squares. Get the sum of the differences in y-values, squaring each value.\n\nOptimize the sum of squares metric.\nCall an optimization function. Initially guess that the intercept is zero and the slope is zero by passing a named vector of parameters. *Use calc_sum_of_squares as the optimization function.\n\n\n\nE7.R\n\n# Set the intercept to 10\nintercept &lt;- 10\n\n# Set the slope to 1\nslope &lt;- 1\n\n# Calculate the predicted y values\ny_pred &lt;- intercept + slope * x_actual\n\n# Calculate the differences between actual and predicted\ny_diff &lt;- y_actual - y_pred\n\n# Calculate the sum of squares\nsum(y_diff ^ 2)\n\n\n\n\n# Set the intercept to 10\nintercept &lt;- 10\n\n# Set the slope to 1\nslope &lt;- 1\n\n# Calculate the predicted y values\ny_pred &lt;- intercept + slope * x_actual\n\n# Calculate the differences between actual and predicted\ny_diff &lt;- y_actual - y_pred\n\n# Calculate the sum of squares\nsum(y_diff ^ 2)\n\n\n\n\n\n# From previous step\ncalc_sum_of_squares &lt;- function(coeffs) {\n  intercept &lt;- coeffs[1]\n  slope &lt;- coeffs[2]\n  y_pred &lt;- intercept + slope * x_actual\n  y_diff &lt;- y_actual - y_pred\n  sum(y_diff ^ 2)\n}\n\n# Optimize the metric\noptim(\n  # Initially guess 0 intercept and 0 slope\n  par = c(intercept = 0, slope = 0), \n  # Use calc_sum_of_squares as the optimization fn\n  fn = calc_sum_of_squares\n)\n\n# Compare the coefficients to those calculated by lm()\nlm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#visualizing-multiple-explanatory-variables",
    "href": "Intermediate_Regression_in_R_C4.html#visualizing-multiple-explanatory-variables",
    "title": "55  Multiple Logistic Regression",
    "section": "55.1 Visualizing multiple explanatory variables",
    "text": "55.1 Visualizing multiple explanatory variables\nLogistic regression also supports multiple explanatory variables. Plotting has similar issues as the linear regression case: it quickly becomes difficult to include more numeric variables in the plot. Here we’ll look at the case of two numeric explanatory variables, and the solution is basically the same as before: use color to denote the response.\nHere there are only two possible values of response (zero and one), and later when we add predicted responses, the values all lie between zero and one. Once you include predicted responses, the most important thing to determine from the plot is whether the predictions are close to zero, or close to one. That means that a 2-color gradient split at 0.5 is really useful: responses above 0.5 are one color, and responses below 0.5 are another color.\nThe bank churn dataset is available as churn; ggplot2 is loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#instructions-100-xp",
    "href": "Intermediate_Regression_in_R_C4.html#instructions-100-xp",
    "title": "55  Multiple Logistic Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the churn dataset, plot the recency of purchase, time_since_last_purchase, versus the length of customer relationship, time_since_first_purchase, colored by whether or not the customer churned, has_churned.\nAdd a point layer, with transparency set to 0.5.\nUse a 2-color gradient, with midpoint 0.5.\nUse the black and white theme.\n\n\n\nE1.R\n\n# Using churn, plot recency vs. length of relationship colored by churn status\nggplot(\n  churn, \n  aes(time_since_first_purchase, time_since_last_purchase, color = has_churned)\n) +\n  # Make it a scatter plot, with transparency 0.5\n  geom_point(alpha = 0.5) +\n  # Use a 2-color gradient split at 0.5\n  scale_color_gradient2(midpoint = 0.5) +\n  # Use the black and white theme\n  theme_bw()"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#logistic-regression-with-2-explanatory-variables",
    "href": "Intermediate_Regression_in_R_C4.html#logistic-regression-with-2-explanatory-variables",
    "title": "55  Multiple Logistic Regression",
    "section": "55.2 Logistic regression with 2 explanatory variables",
    "text": "55.2 Logistic regression with 2 explanatory variables\nTo include multiple explanatory variables in logistic regression models, the syntax is the same as for linear regressions. The only change is the same as in the simple case: you run a generalized linear model with a binomial error family.\nHere you’ll fit a model of churn status with both of the explanatory variables from the dataset: the length of customer relationship and the recency of purchase.\nchurn is available."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-1",
    "href": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-1",
    "title": "55  Multiple Logistic Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFit a logistic regression of churn status, has_churned versus length of customer relationship, time_since_first_purchase and recency of purchase, time_since_last_purchase, and an interaction between the explanatory variables.\n\n\n\nE2.R\n\n# Fit a logistic regression of churn status vs. length of relationship, recency, and an interaction\nmdl_churn_vs_both_inter &lt;- glm(\n  has_churned ~ time_since_first_purchase * time_since_last_purchase, \n  data = churn, \n  family = binomial\n)\n\n# See the result\nmdl_churn_vs_both_inter"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#logistic-regression-prediction",
    "href": "Intermediate_Regression_in_R_C4.html#logistic-regression-prediction",
    "title": "55  Multiple Logistic Regression",
    "section": "55.3 Logistic regression prediction",
    "text": "55.3 Logistic regression prediction\nAs with linear regression, the joy of logistic regression is that you can make predictions. Let’s step through the prediction flow one more time!\nchurn and mdl_churn_vs_both_inter are available; dplyr, tidyr and ggplot2 are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-2",
    "href": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-2",
    "title": "55  Multiple Logistic Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a grid of explanatory variables.\n\nSet time_since_first_purchase to a sequence from minus two to four in steps of 0.1.\nSet time_since_last_purchase to a sequence from minus one to six in steps of 0.1.\n\nAdd a column to explanatory_data named has_churned containing predictions using mdl_churn_vs_both_inter and explanatory_data with type “response”.\nExtend the plot by adding points from prediction_data with size 3 and shape 15.\n\n\n\nE3.R\n\n# Make a grid of explanatory data\nexplanatory_data &lt;- expand_grid(\n  # Set len. relationship to seq from -2 to 4 in steps of 0.1\n  time_since_first_purchase = seq(-2, 4, 0.1),\n  # Set recency to seq from -1 to 6 in steps of 0.1\n  time_since_last_purchase = seq(-1, 6, 0.1)\n)\n\n# See the result\nexplanatory_data\n\n\n\n\n# From previous steps\nexplanatory_data &lt;- expand_grid(\n  time_since_first_purchase = seq(-2, 4, 0.1),\n  time_since_last_purchase = seq(-1, 6, 0.1)\n)\n\n# Add a column of predictions using mdl_churn_vs_both_inter and explanatory_data with type response\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(has_churned = predict(mdl_churn_vs_both_inter, explanatory_data, type = \"response\"))\n\n# See the result\nprediction_data\n\n\n\n\n# From previous steps\nexplanatory_data &lt;- expand_grid(\n  time_since_first_purchase = seq(-2, 4, 0.1),\n  time_since_last_purchase = seq(-1, 6, 0.1)\n)\nprediction_data &lt;- explanatory_data %&gt;% \n  mutate(\n    has_churned = predict(mdl_churn_vs_both_inter, explanatory_data, type = \"response\")\n  )\n\n# Extend the plot\nggplot(\n  churn, \n  aes(time_since_first_purchase, time_since_last_purchase, color = has_churned)\n) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient2(midpoint = 0.5) +\n  theme_bw() +\n  # Add points from prediction_data with size 3 and shape 15\n  geom_point(data = prediction_data, size = 3, shape = 15)"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#confusion-matrix",
    "href": "Intermediate_Regression_in_R_C4.html#confusion-matrix",
    "title": "55  Multiple Logistic Regression",
    "section": "55.4 Confusion matrix",
    "text": "55.4 Confusion matrix\nWhen the response variable has just two outcomes, like the case of churn, the measures of success for the model are “how many cases where the customer churned did the model correctly predict?” and “how many cases where the customer didn’t churn did the model correctly predict?”. These can be found by generating a confusion matrix and calculating summary metrics on it. A mosaic plot is the natural way to visualize the results.\nchurn and mdl_churn_vs_both_inter are available; dplyr and yardstick are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-3",
    "href": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-3",
    "title": "55  Multiple Logistic Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the actual responses from the churn dataset.\nGet the predicted responses from the rounded, fitted values of mdl_churn_vs_both_inter.\nCreate a table of the actual and predicted response values.\nConvert the table to a conf_mat confusion matrix object.\n“Automatically” plot the confusion matrix, confusion.\nGet summary metrics from the confusion matrix. Remember that the churn event is in the second row/column of the matrix.\n\n\n\nE4.R\n\n# Get the actual responses from churn\nactual_response &lt;- churn$has_churned\n\n# Get the predicted responses from the model\npredicted_response &lt;- round(fitted(mdl_churn_vs_both_inter))\n\n# Get a table of these values\noutcomes &lt;- table(predicted_response, actual_response)\n\n# Convert the table to a conf_mat object\nconfusion &lt;- conf_mat(outcomes)\n\n# See the result\nconfusion\n\n\n\n\n# From previous step\nactual_response &lt;- churn$has_churned\npredicted_response &lt;- round(fitted(mdl_churn_vs_both_inter))\noutcomes &lt;- table(predicted_response, actual_response)\nconfusion &lt;- conf_mat(outcomes)\n\n# \"Automatically\" plot the confusion matrix\nautoplot(confusion)\n\n# Get summary metrics\nsummary(confusion, event_level = \"second\")"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#cumulative-distribution-function",
    "href": "Intermediate_Regression_in_R_C4.html#cumulative-distribution-function",
    "title": "55  Multiple Logistic Regression",
    "section": "55.5 Cumulative distribution function",
    "text": "55.5 Cumulative distribution function\nUnderstanding the logistic distribution is key to understanding logistic regression. Like the normal (Gaussian) distribution, it is a probability distribution of a single continuous variable. Here you’ll visualize the cumulative distribution function (CDF) for the logistic distribution. That is, if you have a logistically distributed variable, x, and a possible value, xval, that x could take, then the CDF gives the probability that x is less than xval.\nThe logistic distribution’s CDF is calculated with the logistic function (hence the name). The plot of this has an S-shape, known as a sigmoid curve. An important property of this function is that it takes an input that can be any number from minus infinity to infinity, and returns a value between zero and one.\nggplot2 is loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-4",
    "href": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-4",
    "title": "55  Multiple Logistic Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a tibble containing three columns.\n\nx values as a sequence from minus ten to ten in steps of 0.1.\nlogistic_x made from x transformed with the logistic distribution CDF.\nlogistic_x_man made from x transformed with a logistic function calculated from the equation\n\n\n\ncd(x) = 1/(1+exp(-x)).\n\n* Check that both logistic transformations (logistic_x and logistic_x_man) have the same values with all.equal().\n\nUsing the logistic_distn_cdf dataset, plot logistic_x versus x as a line plot.\n\n\n\nE5.R\n\nlogistic_distn_cdf &lt;- tibble(\n  # Make a seq from -10 to 10 in steps of 0.1\n  x = seq(-10, 10, 0.1),\n  # Transform x with built-in logistic CDF\n  logistic_x = plogis(x),\n  # Transform x with manual logistic\n  logistic_x_man = 1 / (1 + exp(-x))\n) \n\n# Check that each logistic function gives the same results\nall.equal(\n  logistic_distn_cdf$logistic_x, \n  logistic_distn_cdf$logistic_x_man\n)\n\n\n\n# From previous step\nlogistic_distn_cdf &lt;- tibble(\n  x = seq(-10, 10, 0.1),\n  logistic_x = plogis(x),\n  logistic_x_man = 1 / (1 + exp(-x))\n)\n\n# Using logistic_distn_cdf, plot logistic_x vs. x\nggplot(logistic_distn_cdf, aes(x, logistic_x)) +\n  # Make it a line plot\n  geom_line()"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#inverse-cumulative-distribution-function",
    "href": "Intermediate_Regression_in_R_C4.html#inverse-cumulative-distribution-function",
    "title": "55  Multiple Logistic Regression",
    "section": "55.6 Inverse cumulative distribution function",
    "text": "55.6 Inverse cumulative distribution function\nThe logistic function (logistic distribution CDF) has another important property: each x input value is transformed to a unique value. That means that the transformation can be reversed. The logit function is the name for the inverse logistic function, which is also the logistic distribution inverse cumulative distribution function. (All three terms mean exactly the same thing.)\nThe logit function takes values between zero and one, and returns values between minus infinity and infinity.\ndplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-5",
    "href": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-5",
    "title": "55  Multiple Logistic Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a tibble containing three columns.\n\nx values as a sequence from minus 0.001 to 0.999 in steps of 0.001.\nlogit_p made from p transformed with the logistic distribution inverse CDF.\nlogit_p_man made from p transformed with the equation\n\n\n\nlog(p/(1-p)).\n\n* Check that both logit transformations (logit_p and logit_p_man) have the same values with all.equal().\n\nUsing the logistic_distn_inv_cdf dataset, plot logit_p versus p as a line plot.\n\n\n\nE6.R\n\n\nlogistic_distn_inv_cdf &lt;- tibble(\n  # Make a seq from 0.001 to 0.999 in steps of 0.001\n  p = seq(0.001, 0.999, 0.001),\n  # Transform with built-in logistic inverse CDF\n  logit_p = qlogis(p),\n  # Transform with manual logit\n  logit_p_man = log(p / (1 - p))\n) \n\n# Check that each logistic function gives the same results\nall.equal(\n  logistic_distn_inv_cdf$logit_p, \n  logistic_distn_inv_cdf$logit_p_man\n)\n\n\n\n\n# From previous step\nlogistic_distn_inv_cdf &lt;- tibble(\n  p = seq(0.001, 0.999, 0.001),\n  logit_p = qlogis(p),\n  logit_p_man = log(p / (1 - p))\n)\n\n# Using logistic_distn_inv_cdf, plot logit_p vs. p\nggplot(logistic_distn_inv_cdf, aes(p, logit_p)) +\n  # Make it a line plot\n  geom_line()"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#binomial-family-argument",
    "href": "Intermediate_Regression_in_R_C4.html#binomial-family-argument",
    "title": "55  Multiple Logistic Regression",
    "section": "55.7 binomial family argument",
    "text": "55.7 binomial family argument\nThe big difference between running a linear regression with lm() and running a logistic regression with glm() is that you have to set glm()’s family argument to binomial. binomial() is a function that returns a list of other functions that tell glm() how to perform calculations in the regression. The two most interesting functions are linkinv and linkfun, which are used for transforming variables from the whole number line (minus infinity to infinity) to probabilities (zero to one) and back again.\nA vector of values, x, and a vector of probabilities, p, are available."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-6",
    "href": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-6",
    "title": "55  Multiple Logistic Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExamine the structure of the binomial() function. Notice that it contains two elements that are functions, binomial()\\[linkinv, and binomial()\\]linkfun.\nCall binomial()$linkinv() on x, assigning to linkinv_x.\nCheck that linkinv_x and plogis() of x give the same results with all.equal().\nCall binomial()$linkfun() on p, assigning to linkfun_p.\nCheck that linkfun_p and qlogis() of p give the same results.\n\n\n\nE7.R\n\n# Look at the structure of binomial() function\nstr(binomial())\n\n# Call the link inverse on x\nlinkinv_x &lt;- binomial()$linkinv(x)\n\n# Check linkinv_x and plogis() of x give same results \nall.equal(linkinv_x, plogis(x))\n\n# Call the link fun on p\nlinkfun_p &lt;- binomial()$linkfun(p)\n\n# Check linkfun_p and qlogis() of p give same results \nall.equal(linkfun_p, qlogis(p))"
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#logistic-regression-algorithm",
    "href": "Intermediate_Regression_in_R_C4.html#logistic-regression-algorithm",
    "title": "55  Multiple Logistic Regression",
    "section": "55.8 Logistic regression algorithm",
    "text": "55.8 Logistic regression algorithm\nLet’s dig into the internals and implement a logistic regression algorithm. Since R’s glm() function is very complex, you’ll stick to implementing simple logistic regression for a single dataset.\nRather than using sum of squares as the metric, we want to use likelihood. However, log-likelihood is more computationally stable, so we’ll use that instead. Actually, there is one more change: since we want to maximize log-likelihood, but optim() defaults to finding minimum values, it is easier to calculate the negative log-likelihood.\nThe log-likelihood value for each observation is\nThe metric to calculate is minus the sum of these log-likelihood contributions.\nThe explanatory values (the time_since_last_purchase column of churn) are available as x_actual. The response values (the has_churned column of churn) are available as y_actual."
  },
  {
    "objectID": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-7",
    "href": "Intermediate_Regression_in_R_C4.html#instructions-100-xp-7",
    "title": "55  Multiple Logistic Regression",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSet the intercept to one.\nSet the slope to 0.5.\nCalculate the predicted y-values as the intercept plus the slope times the actual x-values, all transformed with the logistic distribution CDF.\nCalculate the log-likelihood for each term as the log of the predicted y-values times the actual y-values, plus the log of one minus the predicted y-values times one minus the actual y-values.\nCalculate minus the sum of the log-likelihoods for each term.\nRun the tests on the datasummary package.\nComplete the function body.\n\nGet the intercept from the first element of coeffs.\nGet the slope from the second element of coeffs.\nCalculate the predicted y-values as the intercept plus the slope times the actual x-values, transformed with the logistic distribution CDF.\nCalculate the log-likelihood for each term as the log of the predicted y-values times the actual y-values, plus the log of one minus the predicted y-values times one minus the actual y-values.\nCalculate minus the sum of the log-likelihoods for each term.\n\nOptimize the sum of squares metric.\n\nCall an optimization function.\nInitially guess that the intercept is zero and the slope is one.\nUse calc_neg_log_likelihood as the optimization function.\n\n\n\n\nE8.R\n\n# Set the intercept to 1\nintercept &lt;- 1\n\n# Set the slope to 0.5\nslope &lt;- 0.5\n\n# Calculate the predicted y values\ny_pred &lt;- plogis(intercept + slope * x_actual)\n\n# Calculate the log-likelihood for each term\nlog_likelihoods &lt;- log(y_pred) * y_actual + log(1 - y_pred) * (1 - y_actual)\n\n# Calculate minus the sum of the log-likelihoods for each term\n-sum(log_likelihoods)\n\n\n\n\ncalc_neg_log_likelihood &lt;- function(coeffs) {\n  # Get the intercept coeff\n  intercept &lt;- coeffs[1]\n\n  # Get the slope coeff\n  slope &lt;- coeffs[2]\n\n  # Calculate the predicted y values\n  y_pred &lt;- plogis(intercept + slope * x_actual)\n\n  # Calculate the log-likelihood for each term\n  log_likelihoods &lt;- log(y_pred) * y_actual + log(1 - y_pred) * (1 - y_actual)\n\n  # Calculate minus the sum of the log-likelihoods for each term\n  -sum(log_likelihoods)\n}\n\n\n\n\n# From previous step\ncalc_neg_log_likelihood &lt;- function(coeffs) {\n  intercept &lt;- coeffs[1]\n  slope &lt;- coeffs[2]\n  y_pred &lt;- plogis(intercept + slope * x_actual)\n  log_likelihoods &lt;- log(y_pred) * y_actual + log(1 - y_pred) * (1 - y_actual)\n  -sum(log_likelihoods)\n}\n\n# Optimize the metric\noptim(\n  # Initially guess 0 intercept and 1 slope\n  par = c(intercept = 0, slope = 1),\n  # Use calc_neg_log_likelihood as the optimization fn \n  fn = calc_neg_log_likelihood\n)\n\n# Compare the coefficients to those calculated by glm()\nglm(has_churned ~ time_since_last_purchase, data = churn, family = binomial)"
  },
  {
    "objectID": "Sampling_in_R_C1.html#section",
    "href": "Sampling_in_R_C1.html#section",
    "title": "52  Introduction to Sampling",
    "section": "52.1 ",
    "text": "52.1"
  },
  {
    "objectID": "Sampling_in_R_C1.html#instructions-100-xp",
    "href": "Sampling_in_R_C1.html#instructions-100-xp",
    "title": "52  Introduction to Sampling",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse View() to view the spotify_population dataset. Explore it in the viewer until you are clear on what it contains.\nUse dplyr to sample 1000 rows from spotify_population, assigning to spotify_sample.\nUsing the spotify_population dataset, calculate the mean duration in minutes. Call the calculated column mean_dur.\nUsing the spotify_sample dataset, perform the same calculation in another column called mean_dur.\nLook at the two values. How different are they?\n\n\n\nE1.R\n\n# View the whole population dataset\nspotify_population\n\n# Sample 1000 rows from spotify_population\nspotify_sample &lt;- slice_sample(spotify_population, n = 1000)\n\n# See the result\nspotify_sample\n\n\n\n# From previous step\nspotify_sample &lt;- spotify_population %&gt;% \n  slice_sample(n = 1000)\n\n# Calculate the mean duration in mins from spotify_population\nmean_dur_pop &lt;- spotify_population %&gt;% \n  summarize(mean_dur = mean(duration_minutes))\n\n# Calculate the mean duration in mins from spotify_sample\nmean_dur_samp &lt;- spotify_sample %&gt;% \n  summarize(mean_dur = mean(duration_minutes))\n\n# See the results\nmean_dur_pop\nmean_dur_samp"
  },
  {
    "objectID": "Sampling_in_R_C1.html#section-1",
    "href": "Sampling_in_R_C1.html#section-1",
    "title": "52  Introduction to Sampling",
    "section": "52.2 ",
    "text": "52.2"
  },
  {
    "objectID": "Sampling_in_R_C1.html#instructions-100-xp-1",
    "href": "Sampling_in_R_C1.html#instructions-100-xp-1",
    "title": "52  Introduction to Sampling",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the loudness column of spotify_population, assigning to loudness_pop.\nUsing base-R, sample loudness_pop to get 100 random values, assigning to loudness_samp.\nCalculate the standard deviation of loudness_pop.\nCalculate the standard deviation of loudness_samp.\nLook at the two values. How different are they?\n\n\n\nE2.R\n\n# Get the loudness column of spotify_population\nloudness_pop &lt;- spotify_population$loudness\n\n# Sample 100 values of loudness_pop\nloudness_samp &lt;- sample(loudness_pop, size = 100)\n\n# See the results\nloudness_samp\n\n\n\n# From previous step\nloudness_pop &lt;- spotify_population$loudness\nloudness_samp &lt;- sample(loudness_pop, size = 100)\n\n# Calculate the standard deviation of loudness_pop\nsd_loudness_pop &lt;- sd(loudness_pop)\n\n# Calculate the standard deviation of loudness_samp\nsd_loudness_samp &lt;- sd(loudness_samp)\n\n# See the results\nsd_loudness_pop\nsd_loudness_samp"
  },
  {
    "objectID": "Sampling_in_R_C1.html#section-2",
    "href": "Sampling_in_R_C1.html#section-2",
    "title": "52  Introduction to Sampling",
    "section": "52.3 ",
    "text": "52.3"
  },
  {
    "objectID": "Sampling_in_R_C1.html#instructions-100-xp-2",
    "href": "Sampling_in_R_C1.html#instructions-100-xp-2",
    "title": "52  Introduction to Sampling",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing spotify_population, draw a histogram of acousticness with binwidth of 0.01.\nUpdate the histogram code to use the spotify_mysterious_sample dataset.\nSet the x-axis limits from zero to one (for easier comparison with the previous plot).\n\n\n\nE3.R\n\n# Visualize the distribution of acousticness as a histogram with a binwidth of 0.01\nggplot(data = spotify_population, aes(x = acousticness)) +\ngeom_histogram(binwidth = 0.01)\n\n\n\n# Update the histogram to use spotify_mysterious_sample with x-axis limits from 0 to 1\nggplot(spotify_mysterious_sample, aes(acousticness)) +\n  geom_histogram(binwidth = 0.01)+\n  xlim(0:1)"
  },
  {
    "objectID": "Sampling_in_R_C1.html#section-3",
    "href": "Sampling_in_R_C1.html#section-3",
    "title": "52  Introduction to Sampling",
    "section": "52.4 ",
    "text": "52.4"
  },
  {
    "objectID": "Sampling_in_R_C1.html#instructions-100-xp-3",
    "href": "Sampling_in_R_C1.html#instructions-100-xp-3",
    "title": "52  Introduction to Sampling",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing spotify_population, draw a histogram of duration_minutes with binwidth of 0.5.\nUpdate the histogram code to use the spotify_mysterious_sample2 dataset.\nSet the x-axis limits from zero to fifteen (for easier comparison with the previous plot).\n\n\n\nE4.R\n\n# Visualize the distribution of duration_minutes as a histogram with a binwidth of 0.5\nggplot(spotify_population, aes(duration_minutes)) +\n  geom_histogram(binwidth = 0.5)\n  \n  \n\n# Update the histogram to use spotify_mysterious_sample2 with x-axis limits from 0 to 15\nggplot(spotify_mysterious_sample2, aes(duration_minutes)) +\n  geom_histogram(binwidth = 0.01) +\n  xlim(0, 15)"
  },
  {
    "objectID": "Sampling_in_R_C1.html#a",
    "href": "Sampling_in_R_C1.html#a",
    "title": "52  Introduction to Sampling",
    "section": "52.5 A",
    "text": "52.5 A"
  },
  {
    "objectID": "Sampling_in_R_C1.html#instructions-100-xp-4",
    "href": "Sampling_in_R_C1.html#instructions-100-xp-4",
    "title": "52  Introduction to Sampling",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nComplete the data frame of random numbers. -Generate n_numbers from a uniform distribution ranging from -3 to 3. -Generate n_numbers from a normal distribution with mean 5 and standard deviation 2.\nUsing randoms, plot a histogram of the uniform column, using binwidth 0.25.\nUsing randoms, plot a histogram of the normal column, using binwidth 0.5.\n\n\n\nE5.R\n\n# Generate random numbers from ...\nrandoms &lt;- data.frame(\n  # a uniform distribution from -3 to 3\n  uniform = runif(n_numbers, min = -3, max = 3),\n  # a normal distribution with mean 5 and sd 2\n  normal = rnorm(n_numbers, mean = 5, sd = 2)\n)\n\n\n# From previous step\nrandoms &lt;- data.frame(\n  uniform = runif(n_numbers, min = -3, max = 3),\n  normal = rnorm(n_numbers, mean = 5, sd = 2)\n)\n\n# Plot a histogram of uniform values, binwidth 0.25\nggplot(randoms, aes(uniform)) +\n  geom_histogram(binwidth = 0.25)\n  \n\n\n# From previous step\nrandoms &lt;- data.frame(\n  uniform = runif(n_numbers, min = -3, max = 3),\n  normal = rnorm(n_numbers, mean = 5, sd = 2)\n)\n\n# Plot a histogram of normal values, binwidth 0.5\nggplot(randoms, aes(normal)) +\n  geom_histogram(binwidth = 0.5)"
  },
  {
    "objectID": "Sampling_in_R_C1.html#section-4",
    "href": "Sampling_in_R_C1.html#section-4",
    "title": "52  Introduction to Sampling",
    "section": "52.6 ",
    "text": "52.6"
  },
  {
    "objectID": "Sampling_in_R_C1.html#instructions-100-xp-5",
    "href": "Sampling_in_R_C1.html#instructions-100-xp-5",
    "title": "52  Introduction to Sampling",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE6.R"
  },
  {
    "objectID": "Sampling_in_R_C1.html#section-5",
    "href": "Sampling_in_R_C1.html#section-5",
    "title": "52  Introduction to Sampling",
    "section": "52.7 ",
    "text": "52.7"
  },
  {
    "objectID": "Sampling_in_R_C1.html#instructions-100-xp-6",
    "href": "Sampling_in_R_C1.html#instructions-100-xp-6",
    "title": "52  Introduction to Sampling",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE7.R"
  },
  {
    "objectID": "Sampling_in_R_C2.html#section",
    "href": "Sampling_in_R_C2.html#section",
    "title": "53  Sampling Methods",
    "section": "53.1 ",
    "text": "53.1"
  },
  {
    "objectID": "Sampling_in_R_C2.html#instructions-100-xp",
    "href": "Sampling_in_R_C2.html#instructions-100-xp",
    "title": "53  Sampling Methods",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nView the attrition_pop dataset. Explore it in the viewer until you are clear on what it contains.\nSet the random seed to a value of your choosing.\nAdd a row ID column to the dataset, then use simple random sampling to get 200 rows.\nView the sample dataset, attrition_samp. What do you notice about the row IDs?\n\n\n\nE1.R\n\n# View the attrition_pop dataset\nView(attrition_pop)\n\n# Set the seed\nset.seed(18900217)\n\nattrition_samp &lt;- attrition_pop %&gt;% \n  # Add a row ID column\n  rowid_to_column() %&gt;% \n  # Get 200 rows using simple random sampling\n  slice_sample(n = 200)\n\n# View the attrition_samp dataset\nView(attrition_samp)"
  },
  {
    "objectID": "Sampling_in_R_C2.html#section-1",
    "href": "Sampling_in_R_C2.html#section-1",
    "title": "53  Sampling Methods",
    "section": "53.2 ",
    "text": "53.2"
  },
  {
    "objectID": "Sampling_in_R_C2.html#instructions-100-xp-1",
    "href": "Sampling_in_R_C2.html#instructions-100-xp-1",
    "title": "53  Sampling Methods",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSet the sample size to 200.\nGet the population size from attrition_pop.\nCalculate the interval between rows to be sampled.\nGet the row indexes for the sample as a numeric sequence of interval, 2 * interval, up to sample_size * interval.\nSystematically sample attrition_pop, assigning to attrition_sys_samp.\n\nAdd a row ID column to attrition_pop.\nGet the rows of the population corresponding to row_indexes.\n\n\n\n\nE2.R\n\n# Set the sample size to 200\nsample_size &lt;- 200\n\n# Get the population size from attrition_pop\npop_size &lt;- nrow(attrition_pop)\n\n# Calculate the interval\ninterval &lt;- pop_size %/% sample_size\n\n\n# From previous step\nsample_size &lt;- 200\npop_size &lt;- nrow(attrition_pop)\ninterval &lt;- pop_size %/% sample_size\n\n# Get row indexes for the sample\nrow_indexes &lt;- seq_len(sample_size) * interval\n\nattrition_sys_samp &lt;- attrition_pop %&gt;% \n  # Add a row ID column\n  rowid_to_column() %&gt;% \n  # Get 200 rows using systematic sampling\n  slice(row_indexes)\n\n# See the result\nView(attrition_sys_samp)"
  },
  {
    "objectID": "Sampling_in_R_C2.html#section-2",
    "href": "Sampling_in_R_C2.html#section-2",
    "title": "53  Sampling Methods",
    "section": "53.3 ",
    "text": "53.3"
  },
  {
    "objectID": "Sampling_in_R_C2.html#instructions-100-xp-2",
    "href": "Sampling_in_R_C2.html#instructions-100-xp-2",
    "title": "53  Sampling Methods",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd a row ID column to attrition_pop.\nUsing the attrition_pop_id dataset, plot YearsAtCompany versus rowid as a scatter plot, with a smooth trend line.\nShuffle the rows of attrition_pop.\nAdd a row ID column to attrition_pop.\nRepeat the plot of YearsAtCompany versus rowid with points and a smooth trend line, this time using attrition_shuffled.\n\n\n\nE3.R\n\n# Add a row ID column to attrition_pop\nattrition_pop_id &lt;- attrition_pop %&gt;% \n  rowid_to_column()\n\n# Using attrition_pop_id, plot YearsAtCompany vs. rowid\nggplot(attrition_pop_id, aes(rowid, YearsAtCompany)) +\n  # Make it a scatter plot\n  geom_point() +\n  # Add a smooth trend line\n  geom_smooth()\n  \n\n# Shuffle the rows of attrition_pop then add row IDs\nattrition_shuffled &lt;- attrition_pop %&gt;% \n  slice_sample(prop = 1) %&gt;% \n  rowid_to_column()\n\n# Using attrition_shuffled, plot YearsAtCompany vs. rowid\n# Add points and a smooth trend line\nggplot(attrition_shuffled, aes(rowid, YearsAtCompany)) +\n  geom_point() +\n  geom_smooth()"
  },
  {
    "objectID": "Sampling_in_R_C2.html#section-3",
    "href": "Sampling_in_R_C2.html#section-3",
    "title": "53  Sampling Methods",
    "section": "53.4 ",
    "text": "53.4"
  },
  {
    "objectID": "Sampling_in_R_C2.html#instructions-100-xp-3",
    "href": "Sampling_in_R_C2.html#instructions-100-xp-3",
    "title": "53  Sampling Methods",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the counts of employees by Education level from attrition_pop, sorted by descending count\nAdd a percent column of percentages (100 times the count divided by the total count).\nUse proportional stratified sampling on attrition_pop to get 40% of each Education group. That is, group by Education and perform a simple random sample of proportion 0.4 on each group.\nUngroup the stratified sample.\nAs you did with attrition_pop, get the counts of employees by Education level from attrition_strat, sorted by descending count, then add a percent column of percentages.\n\n\n\nE4.R\n\neducation_counts_pop &lt;- attrition_pop %&gt;% \n  # Count the employees by Education level, sorting by n\n  count(Education, sort = TRUE) %&gt;% \n  # Add a percent column\n  mutate(percent = 100 * n / sum(n))\n\n# See the results\neducation_counts_pop\n\n\n\n# From previous step\nattrition_pop %&gt;% \n  count(Education, sort = TRUE) %&gt;% \n  mutate(percent = 100 * n / sum(n))\n\n# Use proportional stratified sampling to get 40% of each Education group\nattrition_strat &lt;- attrition_pop %&gt;% \n  group_by(Education) %&gt;% \n  slice_sample(prop = 0.4) %&gt;% \n  ungroup()\n\n# See the result\nattrition_strat\n\n\n\n# From previous steps\nattrition_pop %&gt;% \n  count(Education, sort = TRUE) %&gt;% \n  mutate(percent = 100 * n / sum(n))\nattrition_strat &lt;- attrition_pop %&gt;% \n  group_by(Education) %&gt;% \n  slice_sample(prop = 0.4) %&gt;% \n  ungroup()\n\n# Get the counts and percents from attrition_strat\neducation_counts_strat &lt;- attrition_strat %&gt;% \n  count(Education, sort = TRUE) %&gt;% \n  mutate(percent = 100 * n / sum(n))\n\n# See the results\neducation_counts_strat"
  },
  {
    "objectID": "Sampling_in_R_C2.html#a",
    "href": "Sampling_in_R_C2.html#a",
    "title": "53  Sampling Methods",
    "section": "53.5 A",
    "text": "53.5 A"
  },
  {
    "objectID": "Sampling_in_R_C2.html#instructions-100-xp-4",
    "href": "Sampling_in_R_C2.html#instructions-100-xp-4",
    "title": "53  Sampling Methods",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse equal counts stratified sampling on attrition_pop to get 30 employees from each Education group. That is, group by Education and perform a simple random sample of size 30 on each group.\nUngroup the stratified sample.\nGet the counts of employees by Education level from attrition_eq, sorted by descending count.\nAdd a percent column of percentages (100 times the count divided by the total count).\n\n\n\nE5.R\n\n# Use equal counts stratified sampling to get 30 employees from each Education group\nattrition_eq &lt;- attrition_pop %&gt;%\n  group_by(Education) %&gt;% \n  slice_sample(n = 30) %&gt;%\n  ungroup()\n\n# See the results\nattrition_eq\n\n\n\n# From previous step\nattrition_eq &lt;- attrition_pop %&gt;%\n  group_by(Education) %&gt;% \n  slice_sample(n = 30) %&gt;%\n  ungroup()\n\n# Get the counts and percents from attrition_eq\neducation_counts_eq &lt;- attrition_eq %&gt;% \n  count(Education, sort = TRUE) %&gt;% \n  mutate(percent = 100 * n / sum(n))\n\n# See the results\neducation_counts_eq"
  },
  {
    "objectID": "Sampling_in_R_C2.html#section-4",
    "href": "Sampling_in_R_C2.html#section-4",
    "title": "53  Sampling Methods",
    "section": "53.6 ",
    "text": "53.6"
  },
  {
    "objectID": "Sampling_in_R_C2.html#instructions-100-xp-5",
    "href": "Sampling_in_R_C2.html#instructions-100-xp-5",
    "title": "53  Sampling Methods",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing attrition_pop, plot YearsAtCompany as a histogram with a binwidth of 1.\nSample 400 employees from attrition_pop weighted by YearsAtCompany.\nUsing attrition_weight, plot YearsAtCompany as a histogram with binwidth 1.\n\n\n\nE6.R\n\n# Using attrition_pop, plot YearsAtCompany as a histogram with binwidth 1\nggplot(attrition_pop, aes(YearsAtCompany)) +\n  geom_histogram(binwidth = 1)\n\n\n  \n\n# Sample 400 employees weighted by YearsAtCompany\nattrition_weight &lt;- attrition_pop %&gt;% \n  slice_sample(n = 400, weight_by = YearsAtCompany)\n\n# See the results\nattrition_weight\n\n\n# From previous step\nattrition_weight &lt;- attrition_pop %&gt;% \n  slice_sample(n = 400, weight_by = YearsAtCompany)\n\n# Using attrition_weight, plot YearsAtCompany as a histogram with binwidth 1\nggplot(attrition_weight, aes(x = YearsAtCompany)) +\n  geom_histogram(binwidth = 1)"
  },
  {
    "objectID": "Sampling_in_R_C2.html#section-5",
    "href": "Sampling_in_R_C2.html#section-5",
    "title": "53  Sampling Methods",
    "section": "53.7 ",
    "text": "53.7"
  },
  {
    "objectID": "Sampling_in_R_C2.html#instructions-100-xp-6",
    "href": "Sampling_in_R_C2.html#instructions-100-xp-6",
    "title": "53  Sampling Methods",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGet the unique JobRole values from attrition_pop.\nRandomly sample four JobRole values from job_roles_pop.\nFilter attrition_pop for the sampled job roles. That is, filter for rows where JobRole is in job_roles_samp.\nFor each job role in the filtered dataset, take a random sample of ten rows.\n\n\n\nE7.R\n\n# Get unique JobRole values\njob_roles_pop &lt;- unique(attrition_pop$JobRole)\n\n# Randomly sample four JobRole values\njob_roles_samp &lt;- sample(job_roles_pop, size = 4)\n\n# See the result\njob_roles_samp\n\n\n\n# From previous step\njob_roles_pop &lt;- unique(attrition_pop$JobRole)\njob_roles_samp &lt;- sample(job_roles_pop, size = 4)\n\n# Filter for rows where JobRole is in job_roles_samp\nattrition_filtered &lt;- attrition_pop %&gt;% \n  filter(JobRole %in% job_roles_samp)\n\n# Randomly sample 10 employees from each sampled job role\nattrition_clus &lt;- attrition_filtered %&gt;% \n  group_by(JobRole) %&gt;% \n  slice_sample(n = 10)\n\n# See the result\nattrition_clus"
  },
  {
    "objectID": "Sampling_in_R_C3.html#section",
    "href": "Sampling_in_R_C3.html#section",
    "title": "54  Sampling Distributions",
    "section": "54.1 ",
    "text": "54.1"
  },
  {
    "objectID": "Sampling_in_R_C3.html#instructions-100-xp",
    "href": "Sampling_in_R_C3.html#instructions-100-xp",
    "title": "54  Sampling Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGenerate a simple random sample from attrition_pop of ten rows. Summarize to calculate the mean proportion of employee attrition (Attrition equals “Yes”). Calculate the relative error between mean_attrition_srs10 and mean_attrition_pop as a percentage.\nCalculate the relative error percentage again. This time, use a simple random sample of one hundred rows of attrition_pop.\n\n\n\nE1.R\n\n# Generate a simple random sample of 10 rows \nattrition_srs10 &lt;- attrition_pop %&gt;% \n  slice_sample(n = 10)\n\n# Calculate the proportion of employee attrition in the sample\nmean_attrition_srs10 &lt;- attrition_srs10 %&gt;% \n  summarize(mean_attrition = mean(Attrition == \"Yes\")) %&gt;% \n  pull(mean_attrition)\n\n# Calculate the relative error percentage\nrel_error_pct10 &lt;- 100 * abs(mean_attrition_pop - mean_attrition_srs10) / mean_attrition_pop\n\n# See the result\nrel_error_pct10\n\n\n\n# Calculate the relative error percentage again with a sample of 100 rows\nattrition_srs100 &lt;- attrition_pop %&gt;% \n  slice_sample(n = 100)\n\nmean_attrition_srs100 &lt;- attrition_srs100 %&gt;% \n  summarize(mean_attrition = mean(Attrition == \"Yes\")) %&gt;% \n  pull(mean_attrition)\n\nrel_error_pct100 &lt;- 100 * abs(mean_attrition_pop - mean_attrition_srs100) / mean_attrition_pop\n\n# See the result\nrel_error_pct100"
  },
  {
    "objectID": "Sampling_in_R_C3.html#section-1",
    "href": "Sampling_in_R_C3.html#section-1",
    "title": "54  Sampling Distributions",
    "section": "54.2 ",
    "text": "54.2"
  },
  {
    "objectID": "Sampling_in_R_C3.html#instructions-100-xp-1",
    "href": "Sampling_in_R_C3.html#instructions-100-xp-1",
    "title": "54  Sampling Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nReplicate the provided code so that it runs 500 times. Assign the resulting vector of sample means to mean_attritions.\nCreate a tibble with a column named sample_mean to store mean_attritions.\nUsing sample_means, draw a histogram of the sample_mean column with a binwidth of 0.05.\n\n\n\nE2.R\n\n# Replicate this code 500 times\nmean_attritions &lt;- replicate(\n  n = 500,\n  attrition_pop %&gt;% \n    slice_sample(n = 20) %&gt;% \n    summarize(mean_attrition = mean(Attrition == \"Yes\")) %&gt;% \n    pull(mean_attrition)\n)\n\n# See the result\nhead(mean_attritions)\n\n\n\n# From previous step\nmean_attritions &lt;- replicate(\n  n = 500,\n  attrition_pop %&gt;% \n    slice_sample(n = 20) %&gt;% \n    summarize(mean_attrition = mean(Attrition == \"Yes\")) %&gt;% \n    pull(mean_attrition)\n)\n\n# Store mean_attritions in a tibble in a column named sample_mean\nsample_means &lt;- tibble(sample_mean = mean_attritions)\n\n# Plot a histogram of the `sample_mean` column, binwidth 0.05\nggplot(sample_means, aes(sample_mean)) +\n  geom_histogram(binwidth = 0.05)"
  },
  {
    "objectID": "Sampling_in_R_C3.html#section-2",
    "href": "Sampling_in_R_C3.html#section-2",
    "title": "54  Sampling Distributions",
    "section": "54.3 ",
    "text": "54.3"
  },
  {
    "objectID": "Sampling_in_R_C3.html#instructions-100-xp-2",
    "href": "Sampling_in_R_C3.html#instructions-100-xp-2",
    "title": "54  Sampling Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExpand a grid representing 5 8-sided dice. That is, create a tibble with five columns, named die1 to die5. The rows should contain all possibilities for throwing five dice, each numbered 1 to 8.\n\n*Add a column, mean_roll, to dice, that contains the mean of the five rolls.\n\nUsing the dice dataset, plot mean_roll, converted to a factor, as a bar plot.\n\n\n\nE3.R\n\n# Expand a grid representing 5 8-sided dice\ndice &lt;- expand_grid(\n  die1 = 1:8,\n  die2 = 1:8,\n  die3 = 1:8,\n  die4 = 1:8,\n  die5 = 1:8\n)\n\n# See the result\ndice\n\n\ndice &lt;- expand_grid(\n  die1 = 1:8,\n  die2 = 1:8,\n  die3 = 1:8,\n  die4 = 1:8,\n  die5 = 1:8\n) %&gt;% \n  # Add a column of mean rolls\n  mutate(mean_roll = (die1 + die2 + die3 + die4 + die5) / 5)\n  \n  \n\n# From previous step\ndice &lt;- expand_grid(\n  die1 = 1:8,\n  die2 = 1:8,\n  die3 = 1:8,\n  die4 = 1:8,\n  die5 = 1:8\n) %&gt;% \n  mutate(mean_roll = (die1 + die2 + die3 + die4 + die5) / 5)\n\n# Using dice, draw a bar plot of mean_roll as a factor\nggplot(dice, aes(factor(mean_roll))) +\n  geom_bar()"
  },
  {
    "objectID": "Sampling_in_R_C3.html#section-3",
    "href": "Sampling_in_R_C3.html#section-3",
    "title": "54  Sampling Distributions",
    "section": "54.4 ",
    "text": "54.4"
  },
  {
    "objectID": "Sampling_in_R_C3.html#instructions-100-xp-3",
    "href": "Sampling_in_R_C3.html#instructions-100-xp-3",
    "title": "54  Sampling Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSample one to eight, five times, with replacement. Assign to five_rolls.\nCalculate the mean of five_rolls.\nReplicate the sampling code 1000 times, assigning to sample_means_1000.\nCreate a tibble, and store sample_means_1000 in the a column named sample_mean.\nUsing the sample_means dataset, plot sample_mean, converted to a factor, as a bar plot.\n\n\n\nE4.R\n\n# Sample one to eight, five times, with replacement\nfive_rolls &lt;- sample(1:8, size = 5, replace = TRUE)\n  \n# Calculate the mean of five_rolls\nmean(five_rolls)\n\n\n\n# Replicate the sampling code 1000 times\nsample_means_1000 &lt;- replicate(\n  n = 1000,\n  expr = {\n    five_rolls &lt;- sample(1:8, size = 5, replace = TRUE)\n    mean(five_rolls)\n  }\n)\n\n# See the result\nsample_means_1000\n\n\n\n# From previous step\nsample_means_1000 &lt;- replicate(\n  n = 1000,\n  expr = {\n    five_rolls &lt;- sample(1:8, size = 5, replace = TRUE)\n    mean(five_rolls)\n  }\n)\n\n# Wrap sample_means_1000 in the sample_mean column of a tibble\nsample_means &lt;- tibble(\n  sample_mean = sample_means_1000\n)\n\n# See the result\nsample_means\n\n\n\n# From previous steps\nsample_means_1000 &lt;- replicate(\n  n = 1000,\n  expr = {\n    five_rolls &lt;- sample(1:8, size = 5, replace = TRUE)\n    mean(five_rolls)\n  }\n)\nsample_means &lt;- tibble(\n  sample_mean = sample_means_1000\n)\n\n# Using sample_means, draw a bar plot of sample_mean as a factor\nggplot(sample_means, aes(factor(sample_mean))) +\n  geom_bar()"
  },
  {
    "objectID": "Sampling_in_R_C3.html#a",
    "href": "Sampling_in_R_C3.html#a",
    "title": "54  Sampling Distributions",
    "section": "54.5 A",
    "text": "54.5 A"
  },
  {
    "objectID": "Sampling_in_R_C3.html#instructions-100-xp-4",
    "href": "Sampling_in_R_C3.html#instructions-100-xp-4",
    "title": "54  Sampling Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing sampling_distribution_5, calculate the mean across all the replicates of the mean_attritions (a mean of sample means). Store this in a column called mean_mean_attrition.\nDo the same calculation using sampling_distribution_50 and sampling_distribution_500.\n\n\n\nE5.R\n\n# Calculate the mean across replicates of the mean attritions in sampling_distribution_5\nmean_of_means_5 &lt;- sampling_distribution_5 %&gt;%\n  summarize(mean_mean_attrition = mean(mean_attrition))\n\n# Do the same for sampling_distribution_50\nmean_of_means_50 &lt;- sampling_distribution_50 %&gt;%\n  summarize(mean_mean_attrition = mean(mean_attrition))\n\n# ... and for sampling_distribution_500\nmean_of_means_500 &lt;- sampling_distribution_500 %&gt;%\n  summarize(mean_mean_attrition = mean(mean_attrition))\n\n# See the results\nmean_of_means_5\nmean_of_means_50\nmean_of_means_500"
  },
  {
    "objectID": "Sampling_in_R_C3.html#section-4",
    "href": "Sampling_in_R_C3.html#section-4",
    "title": "54  Sampling Distributions",
    "section": "54.6 ",
    "text": "54.6"
  },
  {
    "objectID": "Sampling_in_R_C3.html#instructions-100-xp-5",
    "href": "Sampling_in_R_C3.html#instructions-100-xp-5",
    "title": "54  Sampling Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing sampling_distribution_5, calculate the standard deviation across all the replicates of the mean_attritions (a standard deviation of sample means). Store this in a column called sd_mean_attrition.\nDo the same calculation using sampling_distribution_50 and sampling_distribution_500.\n\n\n\nE6.R\n\n# Calculate the standard deviation across replicates of the mean attritions in sampling_distribution_5\nsd_of_means_5 &lt;- sampling_distribution_5 %&gt;%\n  summarize(sd_mean_attrition = sd(mean_attrition))\n\n# Do the same for sampling_distribution_50\nsd_of_means_50 &lt;- sampling_distribution_50 %&gt;%\n  summarize(sd_mean_attrition = sd(mean_attrition))\n\n# ... and for sampling_distribution_500\nsd_of_means_500 &lt;- sampling_distribution_500 %&gt;%\n  summarize(sd_mean_attrition = sd(mean_attrition))\n\n# See the results\nsd_of_means_5\nsd_of_means_50\nsd_of_means_500"
  },
  {
    "objectID": "Sampling_in_R_C3.html#section-5",
    "href": "Sampling_in_R_C3.html#section-5",
    "title": "54  Sampling Distributions",
    "section": "54.7 ",
    "text": "54.7"
  },
  {
    "objectID": "Sampling_in_R_C3.html#instructions-100-xp-6",
    "href": "Sampling_in_R_C3.html#instructions-100-xp-6",
    "title": "54  Sampling Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE7.R"
  },
  {
    "objectID": "Sampling_in_R_C4.html#section",
    "href": "Sampling_in_R_C4.html#section",
    "title": "55  Bootstrap Distributions",
    "section": "55.1 ",
    "text": "55.1"
  },
  {
    "objectID": "Sampling_in_R_C4.html#instructions-100-xp",
    "href": "Sampling_in_R_C4.html#instructions-100-xp",
    "title": "55  Bootstrap Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGenerate a single bootstrap resample from spotify_sample\nSummarize to calculate the mean danceability of spotify_1_resample as mean_danceability, then pull out this value to get a numeric vector of length 1.\nReplicate the expression provided 1000 times.\nStore mean_danceability_1000 in a tibble, in a column named resample_mean.\nUsing bootstrap_distn, draw a histogram of the resample_means with binwidth 0.002.\n\n\n\nE1.R\n\n# Generate 1 bootstrap resample\nspotify_1_resample &lt;- spotify_sample %&gt;% \n  slice_sample(prop = 1, replace = TRUE)\n\n# See the result\nspotify_1_resample\n\n\n\n# From previous step\nspotify_1_resample &lt;- spotify_sample %&gt;% \n  slice_sample(prop = 1, replace = TRUE)\n\n# Calculate mean danceability of resample\nmean_danceability_1 &lt;- spotify_1_resample %&gt;% \n  summarize(mean_danceability = mean(danceability)) %&gt;% \n  pull(mean_danceability)\n\n# See the result\nmean_danceability_1\n\n\n\n\n# Replicate this 1000 times\nmean_danceability_1000 &lt;- replicate(\n  n = 1000,\n  expr = {\n    spotify_1_resample &lt;- spotify_sample %&gt;% \n      slice_sample(prop = 1, replace = TRUE)\n    spotify_1_resample %&gt;% \n      summarize(mean_danceability = mean(danceability)) %&gt;% \n      pull(mean_danceability)\n  }\n)\n\n# See the result\nmean_danceability_1000\n\n\n\n# From previous steps\nmean_danceability_1000 &lt;- load_step_4()\n\n# Store the resamples in a tibble\nbootstrap_distn &lt;- tibble(\n  resample_mean = mean_danceability_1000\n)\n\n# Draw a histogram of the resample means with binwidth 0.002\nggplot(bootstrap_distn, aes(resample_mean)) +\n  geom_histogram(binwidth = 0.002)"
  },
  {
    "objectID": "Sampling_in_R_C4.html#section-1",
    "href": "Sampling_in_R_C4.html#section-1",
    "title": "55  Bootstrap Distributions",
    "section": "55.2 ",
    "text": "55.2"
  },
  {
    "objectID": "Sampling_in_R_C4.html#instructions-100-xp-1",
    "href": "Sampling_in_R_C4.html#instructions-100-xp-1",
    "title": "55  Bootstrap Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGenerate a sampling distribution of 2000 replicates. Sample 500 rows of the population without replacement. Calculate the statistic of interest (the mean popularity) in the column mean_popularity. Pull out the statistic so it is a single numeric value (not a tibble).\nGenerate a bootstrap distribution of 2000 replicates. Sample 500 rows of the sample with replacement. Calculate the statistic of interest (the mean popularity) in the column mean_popularity. Pull out the statistic so it is a single numeric value (not a tibble).\n\n\n\nE2.R\n\n# Generate a sampling distribution\nmean_popularity_2000_samp &lt;- replicate(\n  # Use 2000 replicates\n  n = 2000,\n  expr = {\n    # Start with the population\n    spotify_population %&gt;% \n      # Sample 500 rows without replacement\n      slice_sample(n = 500) %&gt;% \n      # Calculate the mean popularity as mean_popularity\n      summarize(mean_popularity = mean(popularity)) %&gt;% \n      # Pull out the mean popularity\n      pull(mean_popularity)\n  }\n)\n\n# See the result\nmean_popularity_2000_samp\n\n\n\n# Generate a bootstrap distribution\nmean_popularity_2000_boot &lt;- replicate(\n  # Use 2000 replicates\n  n = 2000,\n  expr = {\n    # Start with the sample\n    spotify_sample %&gt;% \n      # Sample same number of rows with replacement\n      slice_sample(prop = 1, replace = TRUE) %&gt;% \n      # Calculate the mean popularity\n      summarize(mean_popularity = mean(popularity)) %&gt;% \n      # Pull out the mean popularity\n      pull(mean_popularity)\n  }\n)\n\n# See the result\nmean_popularity_2000_boot"
  },
  {
    "objectID": "Sampling_in_R_C4.html#section-2",
    "href": "Sampling_in_R_C4.html#section-2",
    "title": "55  Bootstrap Distributions",
    "section": "55.3 ",
    "text": "55.3"
  },
  {
    "objectID": "Sampling_in_R_C4.html#instructions-100-xp-2",
    "href": "Sampling_in_R_C4.html#instructions-100-xp-2",
    "title": "55  Bootstrap Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the mean popularity with summarize() in 4 ways.\nPopulation: from spotify_population, take the mean of popularity.\nSample: from spotify_sample, take the mean of popularity.\nSampling distribution: from sampling_distribution, take the mean of sample_mean.\nBootstrap distribution: from bootstrap_distribution, take the mean of resample_mean.\n\n\n\nE3.R\n\n# Calculate the true population mean popularity\npop_mean &lt;- spotify_population %&gt;% \n  summarize(mean(popularity))\n\n# Calculate the original sample mean popularity\nsamp_mean &lt;- spotify_sample %&gt;% \n  summarize(mean(popularity))\n\n# Calculate the sampling dist'n estimate of mean popularity\nsamp_distn_mean &lt;- sampling_distribution %&gt;% \n  summarize(mean(sample_mean))\n\n# Calculate the bootstrap dist'n estimate of mean popularity\nboot_distn_mean &lt;- bootstrap_distribution %&gt;% \n  summarize(mean(resample_mean))\n\n# See the results\nc(pop = pop_mean, samp = samp_mean, samp_distn = samp_distn_mean, boot_distn = boot_distn_mean)"
  },
  {
    "objectID": "Sampling_in_R_C4.html#section-3",
    "href": "Sampling_in_R_C4.html#section-3",
    "title": "55  Bootstrap Distributions",
    "section": "55.4 ",
    "text": "55.4"
  },
  {
    "objectID": "Sampling_in_R_C4.html#instructions-100-xp-3",
    "href": "Sampling_in_R_C4.html#instructions-100-xp-3",
    "title": "55  Bootstrap Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the standard deviation of popularity with summarize() in 4 ways.\nPopulation: from spotify_population, take the standard deviation of popularity.\nOriginal sample: from spotify_sample, take the standard deviation of popularity.\nSampling distribution: from sampling_distribution, take the standard deviation of sample_mean and multiply by the square root of the sample size (500).\nBootstrap distribution: from bootstrap_distribution, take the standard deviation of resample_mean and multiply by the square root of the sample size.\n\n\n\nE4.R\n\n# Calculate the true population std dev popularity\npop_sd &lt;- spotify_population %&gt;% \n  summarize(sd(popularity))\n\n# Calculate the original sample std dev popularity\nsamp_sd &lt;- spotify_sample %&gt;% \n  summarize(sd(popularity))\n\n# Calculate the sampling dist'n estimate of std dev popularity\nsamp_distn_sd &lt;- sampling_distribution %&gt;% \n  summarize(sd(sample_mean) * sqrt(500))\n\n# Calculate the bootstrap dist'n estimate of std dev popularity\nboot_distn_sd &lt;- bootstrap_distribution %&gt;% \n  summarize(sd(resample_mean) * sqrt(500))\n\n# See the results\nc(pop = pop_sd, samp = samp_sd, sam_distn = samp_distn_sd, boot_distn = boot_distn_sd)\n\n##Calculating confidence intervals\nWe can use the cumulative distribution function and its inverse to calculate confidence intervals in R. You’ll do so with the Spotify data now."
  },
  {
    "objectID": "Sampling_in_R_C4.html#a",
    "href": "Sampling_in_R_C4.html#a",
    "title": "55  Bootstrap Distributions",
    "section": "55.5 A",
    "text": "55.5 A"
  },
  {
    "objectID": "Sampling_in_R_C4.html#instructions-100-xp-4",
    "href": "Sampling_in_R_C4.html#instructions-100-xp-4",
    "title": "55  Bootstrap Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGenerate a 95% confidence interval using the quantile method.\nSummarize to get the 0.025 quantile as lower, and the 0.975 quantile as upper.\nGenerate a 95% confidence interval using the standard error method.\nCalculate point_estimate as the mean of resample_mean, and standard_error as the standard deviation of resample_mean.\nCalculate lower as the 0.025 quantile of an inv. CDF from a normal distribution with mean point_estimate and standard deviation standard_error.\nCalculate upper as the 0.975 quantile of that same inv. CDF.\n\n\n\nE5.R\n\n# Generate a 95% confidence interval using the quantile method\nconf_int_quantile &lt;- bootstrap_distribution %&gt;% \n  summarize(\n    lower = quantile(resample_mean, 0.025),\n    upper = quantile(resample_mean, 0.975)\n  )\n\n# See the result\nconf_int_quantile\n\n\n# Generate a 95% confidence interval using the std error method\nconf_int_std_error &lt;- bootstrap_distribution %&gt;% \n  summarize(\n    point_estimate = mean(resample_mean),\n    standard_error = sd(resample_mean),\n    lower = qnorm(0.025, point_estimate, standard_error),\n    upper = qnorm(0.975, point_estimate, standard_error)\n  )\n\n# See the result\nconf_int_std_error"
  },
  {
    "objectID": "Sampling_in_R_C4.html#section-4",
    "href": "Sampling_in_R_C4.html#section-4",
    "title": "55  Bootstrap Distributions",
    "section": "55.6 ",
    "text": "55.6"
  },
  {
    "objectID": "Sampling_in_R_C4.html#instructions-100-xp-5",
    "href": "Sampling_in_R_C4.html#instructions-100-xp-5",
    "title": "55  Bootstrap Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE6.R"
  },
  {
    "objectID": "Sampling_in_R_C4.html#section-5",
    "href": "Sampling_in_R_C4.html#section-5",
    "title": "55  Bootstrap Distributions",
    "section": "55.7 ",
    "text": "55.7"
  },
  {
    "objectID": "Sampling_in_R_C4.html#instructions-100-xp-6",
    "href": "Sampling_in_R_C4.html#instructions-100-xp-6",
    "title": "55  Bootstrap Distributions",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\nE7.R"
  },
  {
    "objectID": "Sampling_in_R_C1.html#simple-sampling-with-dplyr",
    "href": "Sampling_in_R_C1.html#simple-sampling-with-dplyr",
    "title": "52  Introduction to Sampling",
    "section": "52.1 Simple sampling with dplyr",
    "text": "52.1 Simple sampling with dplyr\nThroughout this chapter you’ll be exploring song data from Spotify. Each row of the dataset represents a song, and there are 41656 rows. Columns include the name of the song, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. We’ll start by looking at the durations.\nYour first task is to sample the song dataset and compare a calculation on the whole population and on a sample.\nspotify_population is available and dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C1.html#simple-sampling-with-base-r",
    "href": "Sampling_in_R_C1.html#simple-sampling-with-base-r",
    "title": "52  Introduction to Sampling",
    "section": "52.2 Simple sampling with base-R",
    "text": "52.2 Simple sampling with base-R\nWhile dplyr provides great tools for sampling data frames, if you want to work with vectors you can use base-R.\nLet’s turn it up to eleven and look at the loudness property of each song.\nspotify_population is available."
  },
  {
    "objectID": "Sampling_in_R_C1.html#are-findings-from-the-sample-generalizable",
    "href": "Sampling_in_R_C1.html#are-findings-from-the-sample-generalizable",
    "title": "52  Introduction to Sampling",
    "section": "52.3 Are findings from the sample generalizable?",
    "text": "52.3 Are findings from the sample generalizable?\nYou just saw how convenience sampling—collecting data via the easiest method can result in samples that aren’t representative of the whole population. Equivalently, this means findings from the sample are not generalizable to the whole population. Visualizing the distributions of the population and the sample can help determine whether or not the sample is representative of the population.\nThe Spotify dataset contains a column named acousticness, which is a confidence measure from zero to one of whether the track is acoustic, that is, it was made with instruments that aren’t plugged in. Here, you’ll look at acousticness in the total population of songs, and in a sample of those songs.\nspotify_population and spotify_mysterious_sample are available; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Sampling_in_R_C1.html#are-these-findings-generalizable",
    "href": "Sampling_in_R_C1.html#are-these-findings-generalizable",
    "title": "52  Introduction to Sampling",
    "section": "52.4 Are these findings generalizable?",
    "text": "52.4 Are these findings generalizable?\nLet’s look at another sample to see if it is representative of the population. This time, you’ll look at the duration_minutes column of the Spotify dataset, which contains the length of the song in minutes.\nspotify_population and spotify_mysterious_sample2 are available; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Sampling_in_R_C1.html#generating-random-numbers",
    "href": "Sampling_in_R_C1.html#generating-random-numbers",
    "title": "52  Introduction to Sampling",
    "section": "52.5 Generating random numbers",
    "text": "52.5 Generating random numbers\nYou’ve seen sample() and it’s dplyr cousin, slice_sample() for generating pseudo-random numbers from a set of values. A related task is to generate random numbers that follow a statistical distribution, like the uniform distribution or the normal distribution.\nEach random number generation function has a name beginning with “r”. It’s first argument is the number of numbers to generate, but other arguments are distribution-specific. Free hint: Try args(runif) and args(rnorm) to see what arguments you need to pass to those functions.\nn_numbers is available and set to 5000; ggplot2 is loaded."
  },
  {
    "objectID": "Sampling_in_R_C2.html#simple-random-sampling",
    "href": "Sampling_in_R_C2.html#simple-random-sampling",
    "title": "53  Sampling Methods",
    "section": "53.1 Simple random sampling",
    "text": "53.1 Simple random sampling\nThe simplest method of sampling a population is the one you’ve seen already. It is known as simple random sampling (sometimes abbreviated to “SRS”), and involves picking rows at random, one at a time, where each row has the same chance of being picked as any other.\nTo make it easier to see which rows end up in the sample, it’s helpful to include a row ID column in the dataset before you take the sample.\nIn this chapter, we’ll look at sampling methods using a synthetic (fictional) employee attrition dataset from IBM, where “attrition” means leaving the company.\nattrition_pop is available; dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C2.html#systematic-sampling",
    "href": "Sampling_in_R_C2.html#systematic-sampling",
    "title": "53  Sampling Methods",
    "section": "53.2 Systematic sampling",
    "text": "53.2 Systematic sampling\nOne sampling method that avoids randomness is called systematic sampling. Here, you pick rows from the population at regular intervals.\nFor example, if the population dataset had one thousand rows and you wanted a sample size of five, you’d pick rows 200, 400, 600, 800, and 1000.\nattrition_pop is available; dplyr and tibble are loaded."
  },
  {
    "objectID": "Sampling_in_R_C2.html#is-systematic-sampling-ok",
    "href": "Sampling_in_R_C2.html#is-systematic-sampling-ok",
    "title": "53  Sampling Methods",
    "section": "53.3 Is systematic sampling OK?",
    "text": "53.3 Is systematic sampling OK?\nSystematic sampling has a problem: if the data has been sorted, or there is some sort of pattern or meaning behind the row order, then the resulting sample may not be representative of the whole population. The problem can be solved by shuffling the rows, but then systematic sampling is equivalent to simple random sampling.\nHere you’ll look at how to determine whether or not there is a problem.\nattrition_sys_samp is available and has been given a row ID column; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Sampling_in_R_C2.html#proportional-stratified-sampling",
    "href": "Sampling_in_R_C2.html#proportional-stratified-sampling",
    "title": "53  Sampling Methods",
    "section": "53.4 Proportional stratified sampling",
    "text": "53.4 Proportional stratified sampling\nIf you are interested in subgroups within the population, then you may need to carefully control the counts of each subgroup within the population. Proportional stratified sampling results in subgroup sizes within the sample that are representative of the subgroup sizes within the population. It is equivalent to performing a simple random sample on each subgroup.\nattrition_pop is available; dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C2.html#equal-counts-stratified-sampling",
    "href": "Sampling_in_R_C2.html#equal-counts-stratified-sampling",
    "title": "53  Sampling Methods",
    "section": "53.5 Equal counts stratified sampling",
    "text": "53.5 Equal counts stratified sampling\nIf one subgroup is larger than another subgroup in the population, but you don’t want to reflect that difference in your analysis, then you can use equal counts stratified sampling to generate samples where each subgroup has the same amount of data. For example, if you are analyzing blood types, O is the most common blood type worldwide, but you may wish to have equal amounts of O, A, B, and AB in your sample.\n\nattrition_pop is available; dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C2.html#weighted-sampling",
    "href": "Sampling_in_R_C2.html#weighted-sampling",
    "title": "53  Sampling Methods",
    "section": "53.6 Weighted sampling",
    "text": "53.6 Weighted sampling\nStratified sampling provides rules about the probability of picking rows from your dataset at the subgroup level. A generalization of this is weighted sampling, which lets you specify rules about the probability of picking rows at the row level. The probability of picking any given row is proportional to the weight value for that row.\nattrition_pop is available; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Sampling_in_R_C2.html#performing-cluster-sampling",
    "href": "Sampling_in_R_C2.html#performing-cluster-sampling",
    "title": "53  Sampling Methods",
    "section": "53.7 Performing cluster sampling",
    "text": "53.7 Performing cluster sampling\nNow that you know when to use cluster sampling, it’s time to put it into action. In this exercise you’ll explore the JobRole column of the attrition dataset. You can think of each job role as a subgroup of the whole population of employees.\nattrition_pop is available; dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C2.html#kinds-of-sampling",
    "href": "Sampling_in_R_C2.html#kinds-of-sampling",
    "title": "53  Sampling Methods",
    "section": "53.8 3 kinds of sampling",
    "text": "53.8 3 kinds of sampling\nLet’s compare the performance of point estimates using simple, stratified, and cluster sampling. Before we do that, you’ll have to set up the samples.\nIn these exercises, we’ll use the RelationshipSatisfaction column of the attrition dataset, which categorizes the employee’s relationship with the company. It’s an ordered factor with four levels: Low, Medium, High, and Very_High."
  },
  {
    "objectID": "Sampling_in_R_C2.html#instructions-100-xp-7",
    "href": "Sampling_in_R_C2.html#instructions-100-xp-7",
    "title": "53  Sampling Methods",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPerform simple random sampling on attrition_pop to get one quarter of the population.\nPerform stratified sampling on attrition_pop to get one quarter of the population of each RelationshipSatisfaction group. Remember to ungroup the result.\nGet unique values of attrition_pop’s RelationshipSatisfaction column. Randomly sample satisfaction_unique to get two values. Perform cluster sampling on the selected satisfaction groups, sampling one quarter of the population and ungrouping the result.\n\n\n\nE8.R\n\n# Perform simple random sampling to get 0.25 of the population\nattrition_srs &lt;- attrition_pop %&gt;% \n  slice_sample(prop = 0.25)\n\n\n\n# Perform stratified sampling to get 0.25 of each relationship group\nattrition_strat &lt;- attrition_pop %&gt;%\n  group_by(RelationshipSatisfaction) %&gt;% \n  slice_sample(prop = 0.25) %&gt;% \n  ungroup()\n  \n\n\n# Get unique values of RelationshipSatisfaction\nsatisfaction_unique &lt;- unique(attrition_pop$RelationshipSatisfaction)\n\n# Randomly sample for 2 of the unique satisfaction values\nsatisfaction_samp &lt;- sample(satisfaction_unique, size = 2)\n\n# Perform cluster sampling on the selected group getting 0.25 of the population\nattrition_clust &lt;- attrition_pop %&gt;%\n  filter(RelationshipSatisfaction %in% satisfaction_samp) %&gt;% \n  group_by(RelationshipSatisfaction) %&gt;% \n  slice_sample(n = nrow(attrition_pop) / 4) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "Sampling_in_R_C2.html#summary-statistics-on-different-kinds-of-sample",
    "href": "Sampling_in_R_C2.html#summary-statistics-on-different-kinds-of-sample",
    "title": "53  Sampling Methods",
    "section": "53.9 Summary statistics on different kinds of sample",
    "text": "53.9 Summary statistics on different kinds of sample\nNow you have three types of sample (simple, stratified, cluster), you can compare point estimates from each sample to the population parameter. That is, you can calculate the same summary statistic on each sample and see how it compares to the summary statistic for the population.\nHere, we’ll look at how satisfaction with the company affects whether or not the employee leaves the company. That is, you’ll calculate the proportion of employees who left the company (they have an Attrition value of “Yes”), for each value of RelationshipSatisfaction.\nattrition_pop, attrition_srs, attrition_strat, and attrition_clust are available; dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C2.html#instructions-100-xp-8",
    "href": "Sampling_in_R_C2.html#instructions-100-xp-8",
    "title": "53  Sampling Methods",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGroup by RelationshipSatisfaction level. Summarize to calculate a column named mean_attrition as the mean of the cases where Attrition is equal to “Yes”.\nCalculate the proportion of employee attrition for each relationship satisfaction group, this time on the simple random sample, attrition_srs.\nCalculate the proportion of employee attrition for each relationship satisfaction group, this time on the stratified sample, attrition_strat.\nCalculate the proportion of employee attrition for each relationship satisfaction group, this time on the cluster sample, attrition_clust.\n\n\n\nE9.R\n\n# Use the whole population dataset \nmean_attrition_pop &lt;- attrition_pop %&gt;% \n  # Group by relationship satisfaction level\n  group_by(RelationshipSatisfaction) %&gt;% \n  # Calculate the proportion of employee attrition\n  summarize(mean_attrition = mean(Attrition == \"Yes\"))\n\n# See the result\nmean_attrition_pop\n\n\n\n# Calculate the same thing for the simple random sample \nmean_attrition_srs &lt;- attrition_srs %&gt;% \n  group_by(RelationshipSatisfaction) %&gt;% \n  summarize(mean_attrition = mean(Attrition == \"Yes\"))\n\n# See the result\nmean_attrition_srs\n\n\n\n# Calculate the same thing for the stratified sample \nmean_attrition_strat &lt;- attrition_strat %&gt;% \n  group_by(RelationshipSatisfaction) %&gt;% \n  summarize(mean_attrition = mean(Attrition == \"Yes\"))\n\n# See the result\nmean_attrition_strat\n\n\n\n# Calculate the same thing for the cluster sample \nmean_attrition_clust &lt;- attrition_clust %&gt;% \n  group_by(RelationshipSatisfaction) %&gt;% \n  summarize(mean_attrition = mean(Attrition == \"Yes\"))\n\n# See the result\nmean_attrition_clust"
  },
  {
    "objectID": "Sampling_in_R_C3.html#calculating-relative-errors",
    "href": "Sampling_in_R_C3.html#calculating-relative-errors",
    "title": "54  Sampling Distributions",
    "section": "54.1 Calculating relative errors",
    "text": "54.1 Calculating relative errors\nThe size of the sample you take affects how accurately the point estimates reflect the corresponding population parameter. For example, when you calculate a sample mean, you want it to be close to the population mean. However, if your sample is too small, this might not be the case.\nThe most common metric for assessing accuracy is relative error. This is the absolute difference between the population parameter and the point estimate, all divided by the population parameter. It is sometimes expressed as a percentage.\nattrition_pop and mean_attrition_pop are available; dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C3.html#replicating-samples",
    "href": "Sampling_in_R_C3.html#replicating-samples",
    "title": "54  Sampling Distributions",
    "section": "54.2 Replicating samples",
    "text": "54.2 Replicating samples\nWhen you calculate a point estimate such as a sample mean, the value you calculate depends on the rows that were included in the sample. That means that there is some randomness in the answer. In order to quantify the variation caused by this randomness, you can create many samples and calculate the sample mean (or other statistic) for each sample.\nattrition_pop is available; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Sampling_in_R_C3.html#exact-sampling-distribution",
    "href": "Sampling_in_R_C3.html#exact-sampling-distribution",
    "title": "54  Sampling Distributions",
    "section": "54.3 Exact sampling distribution",
    "text": "54.3 Exact sampling distribution\nTo quantify how the point estimate (sample statistic) you are interested in varies, you need to know all the possible values it can take, and how often. That is, you need to know its distribution.\nThe distribution of a sample statistic is called the sampling distribution. When we can calculate this exactly, rather than using an approximation, it is known as the exact sampling distribution.\nLet’s take another look at the sampling distribution of dice rolls. This time, we’ll look at five eight-sided dice. (These have the numbers one to eight.)"
  },
  {
    "objectID": "Sampling_in_R_C3.html#approximate-sampling-distribution",
    "href": "Sampling_in_R_C3.html#approximate-sampling-distribution",
    "title": "54  Sampling Distributions",
    "section": "54.4 Approximate sampling distribution",
    "text": "54.4 Approximate sampling distribution\nCalculating the exact sampling distribution is only possible in very simple situations. With just five eight-sided dice, the number of possible rolls is 8 ^ 5, which is over thirty thousand. When the dataset is more complicated, for example where a variable has hundreds or thousands or categories, the number of possible outcomes becomes too difficult to compute exactly.\nIn this situation, you can calculate an approximate sampling distribution by simulating the exact sampling distribution. That is, you can repeat a procedure over and over again to simulate both the sampling process and the sample statistic calculation process.\ntibble and ggplot2 are loaded."
  },
  {
    "objectID": "Sampling_in_R_C3.html#population-sampling-distribution-means",
    "href": "Sampling_in_R_C3.html#population-sampling-distribution-means",
    "title": "54  Sampling Distributions",
    "section": "54.5 Population & sampling distribution means",
    "text": "54.5 Population & sampling distribution means\nOne of the useful features of sampling distributions is that you can quantify them. In particular, you can calculate summary statistics on them. Here, we’ll look at the relationship between the mean of the sampling distribution and the population parameter that the sampling is supposed to estimate.\nThree sampling distributions are provided. In each case, the employee attrition dataset was sampled using simple random sampling, then the mean attrition was calculated. This was done 1000 times to get a sampling distribution of mean attritions. One sampling distribution used a sample size of 5 for each replicate, one used 50, and one used 500.\nattrition_pop, sampling_distribution_5, sampling_distribution_50, and sampling_distribution_500 are available; dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C3.html#population-and-sampling-distribution-variation",
    "href": "Sampling_in_R_C3.html#population-and-sampling-distribution-variation",
    "title": "54  Sampling Distributions",
    "section": "54.6 Population and sampling distribution variation",
    "text": "54.6 Population and sampling distribution variation\nYou just calculated the mean of the sampling distribution and saw how it is an estimate of the corresponding population parameter. Similarly, as a result of the central limit theorem, the standard deviation of the sampling distribution has an interesting relationship with the population parameter’s standard deviation and the sample size.\nattrition_pop, sampling_distribution_5, sampling_distribution_50, and sampling_distribution_500 are available; dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C4.html#generating-a-bootstrap-distribution",
    "href": "Sampling_in_R_C4.html#generating-a-bootstrap-distribution",
    "title": "55  Bootstrap Distributions",
    "section": "55.1 Generating a bootstrap distribution",
    "text": "55.1 Generating a bootstrap distribution\nThe process for generating a bootstrap distribution is remarkably similar to the process for generating a sampling distribution; only the first step is different.\nTo make a sampling distribution, you start with the population and sample without replacement. To make a bootstrap distribution, you start with a sample and sample that with replacement. After that, the steps are the same: calculate the summary statistic that you are interested in on that sample/resample, then replicate the process many times. In each case, you can visualize the distribution with a histogram.\nHere, spotify_sample is a subset of the spotify_population dataset. To make it easier to see how resampling works, a row ID column has been added, and only the artist name, song name, and danceability columns have been included.\nspotify_sample is available; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Sampling_in_R_C4.html#sampling-distribution-vs.-bootstrap-distribution",
    "href": "Sampling_in_R_C4.html#sampling-distribution-vs.-bootstrap-distribution",
    "title": "55  Bootstrap Distributions",
    "section": "55.2 Sampling distribution vs. bootstrap distribution",
    "text": "55.2 Sampling distribution vs. bootstrap distribution\nThe sampling distribution and bootstrap distribution are closely linked. In situations where you can repeatedly sample from a population (these occasions are rare) and as you learn about both, it’s helpful to generate both the sampling distribution and the bootstrap distribution, one after the other, to see how they are related.\nHere, the statistic you are interested in is the mean popularity score of the songs.\nspotify_population (the whole dataset) and spotify_sample (500 rows only representing an original sample) are available; dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C4.html#compare-sampling-and-bootstrap-means",
    "href": "Sampling_in_R_C4.html#compare-sampling-and-bootstrap-means",
    "title": "55  Bootstrap Distributions",
    "section": "55.3 Compare sampling and bootstrap means",
    "text": "55.3 Compare sampling and bootstrap means\nTo make calculation easier, the distributions from the previous exercise have been included in tibbles. mean_popularity_2000_samp is in the sample_mean column of sampling_distribution, and mean_popularity_2000_boot is in the resample_mean column of bootstrap_distribution.\nspotify_population, spotify_sample, sampling_distribution, and bootstrap_distribution are available; dplyr is loaded."
  },
  {
    "objectID": "Sampling_in_R_C4.html#compare-sampling-and-bootstrap-standard-deviations",
    "href": "Sampling_in_R_C4.html#compare-sampling-and-bootstrap-standard-deviations",
    "title": "55  Bootstrap Distributions",
    "section": "55.4 Compare sampling and bootstrap standard deviations",
    "text": "55.4 Compare sampling and bootstrap standard deviations\nIn the same way that you looked at how the sampling distribution and bootstrap distribution could be used to estimate the population mean, you’ll now take a look at how they can be used to estimate variation, or more specifically, the standard deviation, in the population.\nspotify_population, spotify_sample, sampling_distribution, and bootstrap_distribution are available; dplyr is loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#test-for-single-proportions",
    "href": "Hypothesis_Testing_in_R_C3.html#test-for-single-proportions",
    "title": "58  Proportion Tests",
    "section": "58.1 Test for single proportions",
    "text": "58.1 Test for single proportions\nIn Chapter 1, you calculated a p-value for a test hypothesizing that the proportion of late shipments was greater than 6%. In that chapter, you used a bootstrap distribution to estimate the standard error of the statistic. A simpler alternative is to use an equation for the standard error based on the sample proportion, hypothesized proportion, and sample size.\nLet’s revisit the p-value using this simpler calculation.\nlate_shipments is available; dplyr is loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp",
    "href": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp",
    "title": "58  Proportion Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nHypothesize that the proportion of late shipments is 6%.\nCalculate the sample proportion of shipments where late equals “Yes” as prop_late, and pull out the value to get a numeric value.\nCalculate the number of observations in the sample.\nCalculate the numerator of the z-score as the difference between the sample proportion and the hypothesized proportion.\nCalculate the denominator of the z-score as the sample proportion times one minus the sample proportion, divided by the sample size, all square rooted.\nCalculate the z-score as the ratio of these numbers.\nTransform the z-score into a p-value, remembering that this is a “greater than” alternative hypothesis.\n\n\n\nE1.R\n\n# Hypothesize that the proportion of late shipments is 6%\np_0 &lt;- 0.06\n\n# Calculate the sample proportion of late shipments\np_hat &lt;- late_shipments %&gt;%\nsummarize(prop_late = mean(late == \"Yes\")) %&gt;%\npull(prop_late)\n\n# Calculate the sample size\nn &lt;- nrow(late_shipments)\n\n\n\n# From previous step\np_0 &lt;- 0.06\np_hat &lt;- late_shipments %&gt;%\n  summarize(prop_late = mean(late == \"Yes\")) %&gt;%\n  pull(prop_late)\nn &lt;- nrow(late_shipments)\n\n# Calculate the numerator of the test statistic\nnumerator &lt;- p_hat - p_0\n\n# Calculate the denominator of the test statistic\ndenominator &lt;- sqrt(p_0 * (1 - p_0) / n)\n\n# Calculate the test statistic\nz_score &lt;- numerator / denominator\n\n# See the result\nz_score \n\n\n\n\n# From previous step\np_0 &lt;- 0.06\np_hat &lt;- late_shipments %&gt;%\n  summarize(prop_late = mean(late == \"Yes\")) %&gt;%\n  pull(prop_late)\nn &lt;- nrow(late_shipments)\nnumerator &lt;- p_hat - p_0\ndenominator &lt;- sqrt(p_0 * (1 - p_0) / n)\nz_score &lt;- numerator / denominator\n\n# Calculate the p-value from the z-score\np_value &lt;-  pnorm(z_score, lower.tail = FALSE)\n\n# See the result\np_value"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#section",
    "href": "Hypothesis_Testing_in_R_C3.html#section",
    "title": "58  Proportion Tests",
    "section": "58.2 ",
    "text": "58.2"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-1",
    "href": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-1",
    "title": "58  Proportion Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the pooled sample proportion, p hat, as the mean of p_hats weighted by ns. Use weighted.mean() or arithmetic with this equation.\nCalculate the standard error of the sample.\nCalculate the pooled sample proportion times one minus the pooled sample proportion.\nDivide p_hat_times_not_p_hat by the sample sizes.\nCalculate the square root of the sum of p_hat_times_not_p_hat_over_ns.\nCalculate the z-score. Use the following equation. You’ll need square bracket indexing to access elements of p_hats.\nCalculate the p-value from the z-score.\n\n\n\nE2.R\n\n# See the sample variables\nprint(p_hats)\nprint(ns)\n\n# Calculate the pooled estimate of the population proportion\np_hat &lt;- weighted.mean(p_hats, ns)\n\n# See the result\np_hat\n\n\n\n\n# From previous step\np_hat &lt;- weighted.mean(p_hats, ns)\n\n# Calculate sample prop'n times one minus sample prop'n\np_hat_times_not_p_hat &lt;- p_hat * (1 - p_hat)\n\n# Divide this by the sample sizes\np_hat_times_not_p_hat_over_ns &lt;- p_hat_times_not_p_hat / ns\n\n# Calculate std. error\nstd_error &lt;- sqrt(sum(p_hat_times_not_p_hat_over_ns))\n\n# See the result\nstd_error\n\n\n\n\n# From previous steps\np_hat &lt;- weighted.mean(p_hats, ns)\np_hat_times_not_p_hat &lt;- p_hat * (1 - p_hat)\np_hat_times_not_p_hat_over_ns &lt;- p_hat_times_not_p_hat / ns\nstd_error &lt;- sqrt(sum(p_hat_times_not_p_hat_over_ns))\n\n# Calculate the z-score\nz_score &lt;- (p_hats[\"expensive\"] - p_hats[\"reasonable\"]) / std_error\n\n# See the result\nz_score\n\n\n\n\n# From previous steps\np_hat &lt;- weighted.mean(p_hats, ns)\np_hat_times_not_p_hat &lt;- p_hat * (1 - p_hat)\np_hat_times_not_p_hat_over_ns &lt;- p_hat_times_not_p_hat / ns\nstd_error &lt;- sqrt(sum(p_hat_times_not_p_hat_over_ns))\nz_score &lt;- (p_hats[\"expensive\"] - p_hats[\"reasonable\"]) / std_error\n\n# Calculate the p-value from the z-score\np_value &lt;- pnorm(z_score, lower.tail = FALSE)\n\n# See the result\np_value"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#section-1",
    "href": "Hypothesis_Testing_in_R_C3.html#section-1",
    "title": "58  Proportion Tests",
    "section": "58.3 ",
    "text": "58.3"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-2",
    "href": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-2",
    "title": "58  Proportion Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the late_shipments dataset, use prop_test() to perform a proportion test appropriate to the hypotheses.\n\nSpecify a hypothesis of late versus freight_cost_group.\nSet the order of the freight cost groups. -Specify the success value for late and the type of alternative hypothesis. -Don’t use Yates’ continuity correction.\n\n\n\n\nE3.R\n\n# Perform a proportion test appropriate to the hypotheses \ntest_results &lt;- late_shipments %&gt;% \n  prop_test(\n    late ~ freight_cost_group,\n    order = c(\"expensive\", \"reasonable\"),\n    success = \"Yes\",\n    alternative = \"greater\",\n    correct = FALSE\n  )\n\n# See the results\ntest_results"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#a",
    "href": "Hypothesis_Testing_in_R_C3.html#a",
    "title": "58  Proportion Tests",
    "section": "58.4 A",
    "text": "58.4 A"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-3",
    "href": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-3",
    "title": "58  Proportion Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the late_shipments dataset, draw a proportional stacked bar plot of vendor_inco_term with fill color by freight_cost_group.\nUsing the late_shipments dataset, perform a chi-square test of independence on freight_cost_group and vendor_inco_term.\n\n\n\nE4.R\n\n# Plot vendor_inco_term filled by freight_cost_group.\n# Make it a proportional stacked bar plot.\nggplot(late_shipments,  aes(vendor_inco_term, fill = freight_cost_group)) +\n  geom_bar(position = \"fill\")\n  \n\n\n# Perform a chi-square test of independence on freight_cost_group and vendor_inco_term\ntest_results &lt;- late_shipments %&gt;% \n  chisq_test(freight_cost_group ~ vendor_inco_term)\n\n# See the results\ntest_results"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#section-2",
    "href": "Hypothesis_Testing_in_R_C3.html#section-2",
    "title": "58  Proportion Tests",
    "section": "58.5 ",
    "text": "58.5"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-4",
    "href": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-4",
    "title": "58  Proportion Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the late_shipments dataset, count the vendor_inco_terms.\nGet the number of rows in late_shipments.\nAdd a column, n to the tibble being defined, containing hypothesized counts for each category.\nUsing the vendor_inco_term_counts dataset, plot n versus vendor_inco_term.\nMake it a precalculated bar plot (a.k.a. col plot).\nAdd points from the hypothesized dataset.\n\n\n\nE5.R\n\n# Using late_shipments, count the vendor incoterms\nvendor_inco_term_counts &lt;- late_shipments %&gt;% \n  count(vendor_inco_term)\n\n# Get the number of rows in the whole sample\nn_total &lt;- nrow(late_shipments)\n\nhypothesized &lt;- tribble(\n  ~ vendor_inco_term, ~ prop,\n  \"EXW\", 0.75,\n  \"CIP\", 0.05,\n  \"DDP\", 0.1,\n  \"FCA\", 0.1\n) %&gt;%\n  # Add a column of hypothesized counts for the incoterms\n  mutate(n = prop * n_total)\n\n# See the results\nhypothesized\n\n\n\n\n# From previous step\nvendor_inco_term_counts &lt;- late_shipments %&gt;% \n  count(vendor_inco_term)\nn_total &lt;- nrow(late_shipments)\nhypothesized &lt;- tribble(\n  ~ vendor_inco_term, ~ prop,\n  \"EXW\", 0.75,\n  \"CIP\", 0.05,\n  \"DDP\", 0.1,\n  \"FCA\", 0.1\n) %&gt;%\n  mutate(n = prop * n_total)\n\n# Using vendor_inco_term_counts, plot n vs. vendor_inco_term \nggplot(vendor_inco_term_counts,  aes(vendor_inco_term, n)) +\n  # Make it a (precalculated) bar plot\n  geom_col() +\n  # Add points from hypothesized \n  geom_point(data = hypothesized)"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#section-3",
    "href": "Hypothesis_Testing_in_R_C3.html#section-3",
    "title": "58  Proportion Tests",
    "section": "58.6 ",
    "text": "58.6"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-5",
    "href": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-5",
    "title": "58  Proportion Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the late_shipments dataset, perform a chi-square goodness of fit test on vendor_inco_term. The hypothesized proportions are given in hypothesized_props.\n\n\n\nE6.R\n\nhypothesized_props &lt;- c(\n  EXW = 0.75, CIP = 0.05, DDP = 0.1, FCA = 0.1\n)\n\n# Run chi-square goodness of fit test on vendor_inco_term\ntest_results &lt;- late_shipments %&gt;% \n  chisq_test(\n    response = vendor_inco_term,\n    p = hypothesized_props\n  )\n\n# See the results\ntest_results"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#section-4",
    "href": "Hypothesis_Testing_in_R_C3.html#section-4",
    "title": "58  Proportion Tests",
    "section": "58.7 ",
    "text": "58.7"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-6",
    "href": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-6",
    "title": "58  Proportion Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\n\n\n\nE7.R"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#section-5",
    "href": "Hypothesis_Testing_in_R_C3.html#section-5",
    "title": "58  Proportion Tests",
    "section": "58.8 ",
    "text": "58.8"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-7",
    "href": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-7",
    "title": "58  Proportion Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\n\n\n\nE8.R"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#section-6",
    "href": "Hypothesis_Testing_in_R_C3.html#section-6",
    "title": "58  Proportion Tests",
    "section": "58.9 ",
    "text": "58.9"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-8",
    "href": "Hypothesis_Testing_in_R_C3.html#instructions-100-xp-8",
    "title": "58  Proportion Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\n\n\n\nE9.R"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C1.html#calculating-the-sample-mean",
    "href": "Hypothesis_Testing_in_R_C1.html#calculating-the-sample-mean",
    "title": "56  Introduction to Hypothesis Testing",
    "section": "56.1 Calculating the sample mean",
    "text": "56.1 Calculating the sample mean\nThe late_shipments dataset contains supply chain data on the delivery of medical supplies. Each row represents one delivery of a part. The late columns denotes whether or not the part was delivered late. A value of “Yes” means that the part was delivered late, and a value of “No” means the part was delivered on time.\nLet’s begin our analysis by calculating a point estimate (sample statistic), namely the proportion of late shipments.\nlate_shipments is available; dplyr is loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C1.html#instructions-100-xp",
    "href": "Hypothesis_Testing_in_R_C1.html#instructions-100-xp",
    "title": "56  Introduction to Hypothesis Testing",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse View() to view the late_shipments dataset.\nCalculate the proportion of late shipments in the sample. That is, the mean cases where the late column is “Yes”.\n\n\n\nE1.R\n\n# View the late_shipments dataset\nView(late_shipments)\n\n# Calculate the proportion of late shipments\nlate_prop_samp &lt;- late_shipments %&gt;%\nsummarize(prop_late_shipments = mean(late == \"Yes\")) %&gt;% \n  pull(prop_late_shipments)\n\n\n# See the results\nlate_prop_samp"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C1.html#calculating-a-z-score",
    "href": "Hypothesis_Testing_in_R_C1.html#calculating-a-z-score",
    "title": "56  Introduction to Hypothesis Testing",
    "section": "56.2 Calculating a z-score",
    "text": "56.2 Calculating a z-score\nSince variables have arbitrary ranges and units, we need to standardize them. For example, it would be silly if a hypothesis test gave a different answer if your variables were in Euros instead of US dollars. Standardization avoids that.\nOne standardized value of interest in a hypothesis test is called a z-score. To calculate it, we need three numbers: the sample statistic (point estimate), the hypothesized statistic, and the standard error of the statistic (which we estimate from the bootstrap distribution).\nThe sample statistic is late_prop_samp.\nlate_shipments_boot_distn is a bootstrap distribution of the proportion of late shipments. The proportion of late shipments statistic is in the late_prop column.\nlate_prop_samp and late_shipments_boot_distn are available; dplyr is loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C1.html#instructions-100-xp-1",
    "href": "Hypothesis_Testing_in_R_C1.html#instructions-100-xp-1",
    "title": "56  Introduction to Hypothesis Testing",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nHypothesize that the proportion of late shipments is 6%.\nCalculate the standard error. That is, the standard deviation of the bootstrap distribution.\nCalculate the z-score.\n\n\n\nE2.R\n\n# Hypothesize that the proportion is 6%\nlate_prop_hyp &lt;- 0.06\n\n# Calculate the standard error\nstd_error &lt;- late_shipments_boot_distn %&gt;% \n  summarize(sd_late_prop = sd(late_prop)) %&gt;% \n  pull(sd_late_prop)\n\n# Find z-score of late_prop_samp\nz_score &lt;- (late_prop_samp - late_prop_hyp) / std_error\n\n# See the results\nz_score"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C1.html#calculating-p-values",
    "href": "Hypothesis_Testing_in_R_C1.html#calculating-p-values",
    "title": "56  Introduction to Hypothesis Testing",
    "section": "56.3 Calculating p-values",
    "text": "56.3 Calculating p-values\nIn order to determine whether to choose the null hypothesis or the alternative hypothesis, you need to calculate a p-value from the z-score.\nLet’s return to the late shipments dataset and the proportion of late shipments.\nThe null hypothesis, , is that the proportion of late shipments is six percent.\nThe alternative hypothesis, , is that the proportion of late shipments is greater than six percent.\nThe observed sample statistic, late_prop_samp, the null hypothesis statistic, late_prop_hyp (6%), and the bootstrap standard error, std_error are available."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C1.html#instructions-100-xp-2",
    "href": "Hypothesis_Testing_in_R_C1.html#instructions-100-xp-2",
    "title": "56  Introduction to Hypothesis Testing",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the z-score of late_prop_samp.\nCalculate the p-value for the z-score, assuming a right-tailed test.\n\n\n\nE3.R\n\n# Calculate the z-score of late_prop_samp\nz_score &lt;- (late_prop_samp - late_prop_hyp) / std_error\n\n# Calculate the p-value\np_value &lt;- pnorm(z_score, lower.tail = FALSE)\n                 \n# See the result\np_value"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C1.html#calculating-confidence-intervals",
    "href": "Hypothesis_Testing_in_R_C1.html#calculating-confidence-intervals",
    "title": "56  Introduction to Hypothesis Testing",
    "section": "56.4 Calculating confidence intervals",
    "text": "56.4 Calculating confidence intervals\nIf you give a single estimate of a sample statistic, you are bound to be wrong by some amount. For example, the hypothesized proportion of late shipments was 6%. Even if evidence suggests the null hypothesis that the proportion of late shipments is equal to this, for any new sample of shipments, the proportion is likely to be a little different. Consequently, it’s a good idea to state a confidence interval. That is, you say “we are 95% ‘confident’ the proportion of late shipments is between A and B” (for some value of A and B).\nSampling in R demonstrated two methods for calculating confidence intervals. Here, you’ll use quantiles of the bootstrap distribution to calculate the confidence interval.\nlate_prop_samp and late_shipments_boot_distn are available; dplyr is loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C1.html#instructions-100-xp-3",
    "href": "Hypothesis_Testing_in_R_C1.html#instructions-100-xp-3",
    "title": "56  Introduction to Hypothesis Testing",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSummarize the prop_late_shipments column of late_shipments_boot_distn to calculate the 95% confidence interval using the quantile method. Label the lower and upper CI values lower and upper\n\n\n\nE4.R\n\n# Calculate 95% confidence interval using quantile method\nconf_int_quantile &lt;- late_shipments_boot_distn %&gt;%\n  summarize(\n    lower = quantile(prop_late_shipments, 0.025),\n    upper = quantile(prop_late_shipments, 0.975)\n  )\n\n# See the result\nconf_int_quantile"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#two-sample-mean-test-statistic",
    "href": "Hypothesis_Testing_in_R_C2.html#two-sample-mean-test-statistic",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "57.1 Two sample mean test statistic",
    "text": "57.1 Two sample mean test statistic\nThe hypothesis test for determining if there is a difference between the means of two populations uses a different type of test statistic to the z-scores you saw in Chapter one. It’s called “t”, and can be calculated from three values from each sample using this equation.\nWhile trying to determine why some shipments are late, you may wonder if the weight of the shipments that were late is different from the weight of the shipments that were on time. The late_shipments dataset has been split into a “yes” group, where late == “Yes” and a “no” group where late == “No”. The weight of the shipment is given in the weight_kilograms variable.\nFor convenience, the sample means for the two groups are available as xbar_no and xbar_yes. The sample standard deviations are s_no and s_yes. The sample sizes are n_no and n_yes."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp",
    "href": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the numerator of the test statistic.\nCalculate the denominator of the test statistic.\nUse those two numbers to calculate the test statistic.\n\n\n\nE1.R\n\n# Calculate the numerator of the test statistic\nnumerator &lt;- xbar_no - xbar_yes\n\n# Calculate the denominator of the test statistic\ndenominator &lt;- sqrt(s_no ^ 2 / n_no + s_yes ^ 2 / n_yes)\n\n# Calculate the test statistic\nt_stat &lt;- numerator/denominator\n\n# See the result\nt_stat"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#from-t-to-p",
    "href": "Hypothesis_Testing_in_R_C2.html#from-t-to-p",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "57.2 From t to p",
    "text": "57.2 From t to p\nPreviously, you calculated the test statistic for the two-sample problem of whether the mean weight of shipments is lower for shipments that weren’t late (late == “No”) compared to shipments that were late (late == “Yes”). In order to make decisions about it, you need to transform the test statistic with a cumulative distribution function to get a p-value.\nRecall the hypotheses:\n\nH0 : The mean weight of shipments that weren’t late is the same as the mean weight of shipments that were late.\n\n\nHA: The mean weight of shipments that weren’t late is less than the mean weight of shipments that were late.\n\nThe test statistic, t_stat, is available, as are the samples sizes for each group, n_no and n_yes. Use a significance level of alpha = 0.05."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-1",
    "href": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-1",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate the degrees of freedom for the test.\nUse the test statistic, t_stat, to calculate the p-value.\n\n\n\nE2.R\n\n# Calculate the degrees of freedom\ndegrees_of_freedom &lt;- n_no + n_yes - 2\n\n# Calculate the p-value from the test stat\np_value &lt;- pt(t_stat, df = degrees_of_freedom)\n\n# See the result\np_value"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#visualizing-the-difference",
    "href": "Hypothesis_Testing_in_R_C2.html#visualizing-the-difference",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "57.3 Visualizing the difference",
    "text": "57.3 Visualizing the difference\nBefore you start running hypothesis tests, it’s a great idea to perform some exploratory data analysis. That is, calculating summary statistics and visualizing distributions.\nHere, you’ll look at the proportion of county-level votes for the Democratic candidate in 2012 and 2016, dem_votes_potus_12_16. Since the counties are the same in both years, these samples are paired. The columns containing the samples are dem_percent_12 and dem_percent_16.\ndem_votes_potus_12_16 is available; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-2",
    "href": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-2",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nView the dem_votes_potus_12_16 dataset.\nMutate dem_votes_potus_12_16 to add a diff column containing the percentage of votes for the democratic candidate in 2012 minus the votes for the democratic candidate in 2016.\nSummarize sample_dem_data to calculate the mean of the diff column as xbar_diff and the standard deviation of that column as s_diff.\nUsing sample_dem_data, plot diff as a histogram with binwidth 1.\n\n\n\nE3.R\n\n# View the dem_votes_potus_12_16 dataset\nView(dem_votes_potus_12_16)\n\n# Calculate the differences from 2012 to 2016\nsample_dem_data &lt;- dem_votes_potus_12_16 %&gt;% \n  mutate(diff = dem_percent_12 - dem_percent_16)\n\n# See the result\nsample_dem_data\n\n\n\n\n# From previous step\nsample_dem_data &lt;- dem_votes_potus_12_16 %&gt;% \n  mutate(diff = dem_percent_12 - dem_percent_16)\n\n# Find mean and standard deviation of differences\ndiff_stats &lt;- sample_dem_data %&gt;% \n  summarize(\n    xbar_diff = mean(diff),\n    s_diff = sd(diff)\n  )\n\n# See the result\ndiff_stats\n\n\n\n\n# From previous step\nsample_dem_data &lt;- dem_votes_potus_12_16 %&gt;% \n  mutate(diff = dem_percent_12 - dem_percent_16)\n\n# Using sample_dem_data, plot diff as a histogram\nggplot(sample_dem_data, aes(x = diff))+  geom_histogram(binwidth =1)"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#using-t.test",
    "href": "Hypothesis_Testing_in_R_C2.html#using-t.test",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "57.4 Using t.test()",
    "text": "57.4 Using t.test()\nManually calculating test statistics and transforming them with a CDF to get a p-value is a lot of effort to do every time you need to compare two sample means. The comparison of two sample means is called a t-test, and R has a t.test() function to accomplish it. This function provides some flexibility in how you perform the test.\nAs in the previous exercise, you’ll explore the difference between the proportion of county-level votes for the Democratic candidate in 2012 and 2016.\nsample_dem_data is available, and has columns diff, dem_percent_12, and dem_percent_16."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-3",
    "href": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-3",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nConduct a t-test on the sample differences (the diff column of sample_dem_data). Use an appropriate alternative hypothesis chosen from “two.sided”, “less”, and “greater”.\nConduct a paired test on the democratic votes in 2012 and 2016 (the dem_percent_12 and dem_percent_16 columns of sample_dem_data). Use an appropriate alternative hypothesis.\n\n\n\nE4.R\n\n# Conduct a t-test on diff\ntest_results &lt;- t.test(# Vector of differences\n  sample_dem_data$diff,\n  # Choose between \"two.sided\", \"less\", \"greater\"\n    alternative = \"greater\")\n  \n\n# See the results\ntest_results\n\n\n\n\n\n# Conduct a paired t-test on dem_percent_12 and dem_percent_16\ntest_results &lt;- t.test(\n    sample_dem_data$dem_percent_12,\n    sample_dem_data$dem_percent_16,\n    alternative =\"greater\",\n    #mu =0,\n    paired =TRUE\n      )\n\n# See the results\ntest_results"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#visualizing-many-categories",
    "href": "Hypothesis_Testing_in_R_C2.html#visualizing-many-categories",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "57.5 Visualizing many categories",
    "text": "57.5 Visualizing many categories\nSo far in this chapter, we’ve only considered the case of differences in a numeric variable between two categories. Of course, many datasets contain more categories. Before you get to conducting tests on many categories, it’s often helpful to perform exploratory data analysis. That is, calculating summary statistics for each group and visualizing the distributions of the numeric variable for each category using box plots.\nHere, we’ll return to the late shipments data, and how the price of each package (pack_price) varies between the three shipment modes (shipment_mode): “Air”, “Air Charter”, and “Ocean”.\nlate_shipments is available; dplyr and ggplot2 are loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-4",
    "href": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-4",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the late_shipments dataset, group by shipment_mode.\nSummarize to calculate the mean of pack_price as xbar_pack_price and the standard deviation of pack_price as s_pack_price.\nUsing late_shipments, plot pack_price versus shipment_mode as a box plot with flipped x and y coordinates.\n\n\n\nE5.R\n\n# Using late_shipments, group by shipment mode, and calculate the mean and std dev of pack price\ngroup_by(late_shipments, shipment_mode) %&gt;%\n  summarize(\n    xbar_pack_price = mean(pack_price),\n     s_pack_price = sd(pack_price))\n\n\n\n\n\n# Using late_shipments, plot pack_price vs. shipment_mode\n# as a box plot with flipped x and y coordinates\nlate_shipments %&gt;%\n  ggplot(aes(x = shipment_mode,\n   y = pack_price))+ \n    geom_boxplot()+ \n     coord_flip()"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#conducting-an-anova-test",
    "href": "Hypothesis_Testing_in_R_C2.html#conducting-an-anova-test",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "57.6 Conducting an ANOVA test",
    "text": "57.6 Conducting an ANOVA test\n\nThe box plots made it look like the distribution of pack price was different for each of the three shipment modes. However, it didn’t tell us whether the mean pack price was different in each category. To determine that, we can use an ANOVA test. The null and alternative hypotheses can be written as follows.\n\nPack prices for every category of shipment mode are the same.\n\n\nPack prices for some categories of shipment mode are different.\n\n\nWe’ll set a significance level of 0.1.\nlate_shipments is available."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-5",
    "href": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-5",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun a linear regression of pack_price versus shipment_mode using the late_shipments dataset. The formula takes the form response ~ explanatory.\nPerform ANOVA on mdl_pack_price_vs_shipment_mode.\n\n\n\nE6.R\n\n# Run a linear regression of pack price vs. shipment mode \nmdl_pack_price_vs_shipment_mode &lt;- lm(pack_price ~ shipment_mode,\n data = late_shipments)\n\n# See the results\nsummary(mdl_pack_price_vs_shipment_mode)\n\n\n\n# From previous step\nmdl_pack_price_vs_shipment_mode &lt;- lm(\n    pack_price ~ shipment_mode,\n     data = late_shipments\n     )\n\n# Perform ANOVA on the regression model\nanova(mdl_pack_price_vs_shipment_mode)"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#pairwise-t-tests",
    "href": "Hypothesis_Testing_in_R_C2.html#pairwise-t-tests",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "57.7 Pairwise t-tests",
    "text": "57.7 Pairwise t-tests\nThe ANOVA test didn’t tell us which categories of shipment mode had significant differences in pack prices. To pinpoint which categories had differences, we could instead use pairwise t-tests.\nlate_shipments is available."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-6",
    "href": "Hypothesis_Testing_in_R_C2.html#instructions-100-xp-6",
    "title": "57  Two-Sample and ANOVA Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPerform pairwise t-tests on late_shipments’s pack_price variable, grouped by shipment_mode. Don’t do any p-value adjustment, and keep the default “two.sided” alternative hypothesis specification.\nModify the pairwise t-tests to use Bonferroni p-value adjustment.\n\n\n\nE7.R\n\n# Perform pairwise t-tests on pack price, grouped by shipment mode, no p-value adjustment\ntest_results &lt;- pairwise.t.test(\n  late_shipments$pack_price,\n  late_shipments$shipment_mode,\n  p.adjust.method = \"none\"\n)\n\n# See the results\ntest_results\n\n\n\n\n# Modify the pairwise t-tests to use Bonferroni p-value adjustment\ntest_results &lt;- pairwise.t.test(\n  late_shipments$pack_price,\n  late_shipments$shipment_mode,\n  p.adjust.method = \"bonferroni\"\n)\n\n# See the results\ntest_results"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#section",
    "href": "Hypothesis_Testing_in_R_C4.html#section",
    "title": "59  Non-Parametric Tests",
    "section": "59.7 ",
    "text": "59.7"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp",
    "href": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp",
    "title": "59  Non-Parametric Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the late_shipments dataset, get counts by the freight_cost_group columns. Insert a suitable number to inspect whether the counts are “big enough” for a two sample t-test.\nUsing the late_shipments dataset, get counts by the late column. Insert a suitable number to inspect whether the counts are “big enough” for a one sample proportion test.\nUsing the late_shipments dataset, get counts by the vendor_inco_term and freight_cost_group columns. Insert a suitable number to inspect whether the counts are “big enough” for a chi-square independence test.\nUsing the late_shipments dataset, get counts by the shipment_mode column. Insert a suitable number to inspect whether the counts are “big enough” for an ANOVA test.\n\n\n\nE1.R\n\n# Get counts by freight_cost_group\ncounts &lt;- late_shipments %&gt;%\n  count(freight_cost_group)\n\n# See the result\ncounts\n\n# Inspect whether the counts are big enough\nall(counts$n &gt;= 30)\n\n\n\n\n\n# Get counts by late\ncounts &lt;- late_shipments %&gt;%\n  count(late)\n\n# See the result\ncounts\n\n# Inspect whether the counts are big enough\nall(counts$n &gt;= 10)\n\n\n\n\n# Count the values of vendor_inco_term and freight_cost_group\ncounts &lt;- late_shipments %&gt;%\n  count(vendor_inco_term, freight_cost_group)\n\n# See the result\ncounts\n\n# Inspect whether the counts are big enough\nall(counts$n &gt;= 5)\n\n\n\n# Count the values of shipment_mode\ncounts &lt;- late_shipments %&gt;%\n  count(shipment_mode)\n\n# See the result\ncounts\n\n# Inspect whether the counts are big enough\nall(counts$n &gt;= 30)"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#section-1",
    "href": "Hypothesis_Testing_in_R_C4.html#section-1",
    "title": "59  Non-Parametric Tests",
    "section": "59.8 ",
    "text": "59.8"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-1",
    "href": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-1",
    "title": "59  Non-Parametric Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the late_shipments dataset, specify that we are interested in late proportions across freight_cost_group, where “Yes” denotes success.\nExtend the pipeline to declare a null hypothesis that the variables are independent.\n\n\n\nE2.R\n\n# Specify that we are interested in late proportions across freight_cost_groups, where \"Yes\" denotes success\nspecified &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  )\n\n# See the result\nspecified\n\n\n\n\n# Extend the pipeline to declare a null hypothesis that the variables are independent\nhypothesized &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  ) %&gt;% \n  hypothesize(null = \"independence\")\n\n# See the result\nhypothesized"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#section-2",
    "href": "Hypothesis_Testing_in_R_C4.html#section-2",
    "title": "59  Non-Parametric Tests",
    "section": "59.9 ",
    "text": "59.9"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-2",
    "href": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-2",
    "title": "59  Non-Parametric Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nExtend the infer pipeline to generate two thousand permutation replicates. (Note this will take a few seconds to complete.)\nComplete the infer pipeline for the null distribution by calculating the difference in proportions, setting the order to expensive proportion minus reasonable proportion.\nVisualize the null distribution.\n\n\n\nE3.R\n\n# Extend the pipeline to generate 2000 permutations\ngenerated &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  ) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 2000, type = \"permute\")\n\n# See the result\ngenerated\n\n\n\n\n# Extend the pipeline to calculate the difference in proportions (expensive minus reasonable)\nnull_distn &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  ) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 2000, type = \"permute\") %&gt;% \n  calculate(\n    stat = \"diff in props\", \n    order = c(\"expensive\", \"reasonable\")\n  )\n\n# See the result\nnull_distn\n\n\n\n\n# From previous steps\nnull_distn &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  ) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 2000, type = \"permute\") %&gt;% \n  calculate(\n    stat = \"diff in props\", \n    order = c(\"expensive\", \"reasonable\")\n  )\n\n# Visualize the null distribution\nvisualize(null_distn)"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#a",
    "href": "Hypothesis_Testing_in_R_C4.html#a",
    "title": "59  Non-Parametric Tests",
    "section": "59.4 A",
    "text": "59.4 A"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-3",
    "href": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-3",
    "title": "59  Non-Parametric Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCopy, paste, and modify the null distribution pipeline to get the observed statistic.\nVisualize the null distribution, adding a vertical line at the observed statistic.\nGet the p-value from the null distribution and observed statistic, assuming an appropriate direction for the alternative hypothesis.\n\n\n\nE4.R\n\nnull_distn &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  ) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 2000, type = \"permute\") %&gt;% \n  calculate(\n    stat = \"diff in props\", \n    order = c(\"expensive\", \"reasonable\")\n  )\n\n# Copy, paste, and modify the pipeline to get the observed statistic\nobs_stat &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  ) %&gt;% \n  calculate(\n    stat = \"diff in props\", \n    order = c(\"expensive\", \"reasonable\")\n  )\n\n# See the result\nobs_stat\n\n\n\n# From previous steps\nnull_distn &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  ) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 2000, type = \"permute\") %&gt;% \n  calculate(\n    stat = \"diff in props\", \n    order = c(\"expensive\", \"reasonable\")\n  )\nobs_stat &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  ) %&gt;% \n  calculate(\n    stat = \"diff in props\", \n    order = c(\"expensive\", \"reasonable\")\n  )\n\n# Visualize the null dist'n, adding a vertical line at the observed statistic\nvisualize(null_distn) +\n  geom_vline(aes(xintercept = stat), data = obs_stat)\n  \n  \n  \n\n# From previous steps\nnull_distn &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  ) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 2000, type = \"permute\") %&gt;% \n  calculate(\n    stat = \"diff in props\", \n    order = c(\"expensive\", \"reasonable\")\n  )\nobs_stat &lt;- late_shipments %&gt;% \n  specify(\n    late ~ freight_cost_group, \n    success = \"Yes\"\n  ) %&gt;% \n  calculate(\n    stat = \"diff in props\", \n    order = c(\"expensive\", \"reasonable\")\n  )\n\n# Get the p-value\np_value &lt;- get_p_value(\n  null_distn, obs_stat, \n  direction = \"greater\"\n)\n\n# See the result\np_value"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#section-3",
    "href": "Hypothesis_Testing_in_R_C4.html#section-3",
    "title": "59  Non-Parametric Tests",
    "section": "59.5 ",
    "text": "59.5"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-4",
    "href": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-4",
    "title": "59  Non-Parametric Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSpecify weight in kilograms versus whether or not the shipment was late.\nDeclare a null hypothesis of independence.\nGenerate 1000 permutation replicates.\nCalculate the difference in means, setting the order as “No” minus “Yes”.\nCalculate the difference in means observed in the late_shipments dataset.\nGet the p-value from the null distribution and the observed difference in means, setting an appropriate direction.\n\n\n\nE5.R\n\n# Fill out the null distribution pipeline\nnull_distn &lt;- late_shipments %&gt;% \n  # Specify weight_kilograms vs. late\n  specify(weight_kilograms ~ late) %&gt;% \n  # Declare a null hypothesis of independence\n  hypothesize(null = \"independence\") %&gt;% \n  # Generate 1000 permutation replicates\n  generate(reps = 1000, type = \"permute\") %&gt;% \n  # Calculate the difference in means (\"No\" minus \"Yes\")\n  calculate(\nstat = \"diff in means\",\norder = c(\"No\",\n\"Yes\"))\n\n# See the results\nnull_distn\n\n\n\n\n\n# From previous step\nnull_distn &lt;- late_shipments %&gt;% \n  specify(weight_kilograms ~ late) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"diff in means\", order = c(\"No\", \"Yes\"))\n\n# Calculate the observed difference in means\nobs_stat &lt;- late_shipments %&gt;%\nspecify(weight_kilograms ~ late) %&gt;%\ncalculate(\nstat = \"diff in means\",\norder = c(\"No\",\n\"Yes\")\n)\n\n# See the result\nobs_stat\n\n\n\n\n# From previous steps\nnull_distn &lt;- late_shipments %&gt;% \n  specify(weight_kilograms ~ late) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"diff in means\", order = c(\"No\", \"Yes\"))\nobs_stat &lt;- late_shipments %&gt;% \n  specify(weight_kilograms ~ late) %&gt;% \n  calculate(stat = \"diff in means\", order = c(\"No\", \"Yes\"))\n\n# Get the p-value\np_value &lt;- get_p_value(\n  null_distn, obs_stat,\n  direction = \"less\"\n)\n\n# See the result\np_value"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#section-4",
    "href": "Hypothesis_Testing_in_R_C4.html#section-4",
    "title": "59  Non-Parametric Tests",
    "section": "59.6 ",
    "text": "59.6"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-5",
    "href": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-5",
    "title": "59  Non-Parametric Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUsing the late_shipments dataset, run a Wilcoxon-Mann-Whitney test on the weight in kilograms versus whether or not the shipment was late.\nUsing the late_shipments dataset, run a Kruskal-Wallace test on the weight in kilograms versus the shipment mode.\n\n\n\nE6.R\n\n# Run a Wilcoxon-Mann-Whitney test on weight_kilograms vs. late\ntest_results &lt;- wilcox.test(\n    weight_kilograms ~ late,\n    data = late_shipments,\n)\n\n# See the result\ntest_results\n\n\n\n# Run a Kruskal-Wallace test on weight_kilograms vs. shipment_mode\ntest_results &lt;- kruskal.test(\n    weight_kilograms ~ shipment_mode,\n    data=late_shipments\n    )\n\n# See the result\ntest_results"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#section-5",
    "href": "Hypothesis_Testing_in_R_C4.html#section-5",
    "title": "59  Non-Parametric Tests",
    "section": "59.7 ",
    "text": "59.7"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-6",
    "href": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-6",
    "title": "59  Non-Parametric Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\n\n\n\nE7.R"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#section-6",
    "href": "Hypothesis_Testing_in_R_C4.html#section-6",
    "title": "59  Non-Parametric Tests",
    "section": "59.8 ",
    "text": "59.8"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-7",
    "href": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-7",
    "title": "59  Non-Parametric Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\n\n\n\nE8.R"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#section-7",
    "href": "Hypothesis_Testing_in_R_C4.html#section-7",
    "title": "59  Non-Parametric Tests",
    "section": "59.9 ",
    "text": "59.9"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-8",
    "href": "Hypothesis_Testing_in_R_C4.html#instructions-100-xp-8",
    "title": "59  Non-Parametric Tests",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\n\n\n\n\nE9.R"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html",
    "href": "Hypothesis_Testing_in_R_C3.html",
    "title": "58  Proportion Tests",
    "section": "",
    "text": "59"
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#test-of-two-proportions",
    "href": "Hypothesis_Testing_in_R_C3.html#test-of-two-proportions",
    "title": "58  Proportion Tests",
    "section": "58.2 Test of two proportions",
    "text": "58.2 Test of two proportions\nYou may wonder if the amount paid for freight affects whether or not the shipment was late. Recall that in late_shipments dataset, whether or not the shipment was late is stored in the late column. Freight costs are stored in the freight_cost_group column, and the categories are “expensive” and “reasonable”.\nWe can form hypotheses to test.\n\nH0: late expensive - late reasonable = 0\n\n\nHA: late expensive - late reasonable &gt; 0\n\np_hats contains the estimates of population proportions (sample proportions) for the “expensive” and “reasonable” groups. ns contains the sample sizes for these groups."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#prop_test-for-two-samples",
    "href": "Hypothesis_Testing_in_R_C3.html#prop_test-for-two-samples",
    "title": "58  Proportion Tests",
    "section": "58.3 prop_test() for two samples",
    "text": "58.3 prop_test() for two samples\nThat took a lot of effort to calculate the p-value, so while it is useful to see how the calculations work, it isn’t practical to do in real-world analyses. For daily usage, it’s better to use the infer package.\nRecall the hypotheses.\n\nH0: late expensive - late reasonable = 0\n\n\nHA: late expensive - late reasonable &gt; 0\n\nlate_shipments is available; infer is loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#performing-a-chi-square-test",
    "href": "Hypothesis_Testing_in_R_C3.html#performing-a-chi-square-test",
    "title": "58  Proportion Tests",
    "section": "58.4 Performing a chi-square test",
    "text": "58.4 Performing a chi-square test\nThe chi-square independence test compares proportions of successes of a categorical variable across categories of another categorical variable.\nTrade deals often use a form of business shorthand in order to specify the exact details of their contract. These are International Chamber of Commerce (ICC) international commercial terms, or incoterms for short.\nThe late_shipments dataset includes a vendor_inco_term that describes the incoterms that applied to a given shipment. The choices are:\n\nEXW: “Ex works”. The buyer pays for transportation of the goods. CIP: “Carriage and insurance paid to”. The seller pays for freight and insurance until the goods board a ship. DDP: “Delivered duty paid”. The seller pays for transportation of the goods until they reach a destination port. FCA: “Free carrier”. The seller pays for transportation of the goods.\n\nPerhaps the incoterms affect whether or not the freight costs are expensive. Test these hypotheses with a significance level of 0.01.\n\nH0: vendor_inco_term and freight_cost_group are independent.\n\n\nHA: vendor_inco_term and freight_cost_group are associated.\n\nlate_shipments is available; ggplot2 and infer are loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#visualizing-goodness-of-fit",
    "href": "Hypothesis_Testing_in_R_C3.html#visualizing-goodness-of-fit",
    "title": "58  Proportion Tests",
    "section": "58.5 Visualizing goodness of fit",
    "text": "58.5 Visualizing goodness of fit\nThe chi-square goodness of fit test compares proportions of each level of a categorical variable to hypothesized values. Before running such a test, it can be helpful to visually compare the distribution in the sample to the hypothesized distribution.\nRecall the vendor incoterms in the late_shipments dataset. Let’s hypothesize that the four values occur with these frequencies in the population of shipments.\n\nEXW: 0.75 CIP: 0.05 DDP: 0.1 FCA: 0.1\n\nlate_shipments is available; tibble, dplyr, ggplot2, and infer are loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C3.html#performing-a-goodness-of-fit-test",
    "href": "Hypothesis_Testing_in_R_C3.html#performing-a-goodness-of-fit-test",
    "title": "58  Proportion Tests",
    "section": "58.6 Performing a goodness of fit test",
    "text": "58.6 Performing a goodness of fit test\nThe bar plot of vendor_inco_term suggested that its distribution across the four categories was quite close to the hypothesized distribution. You’ll need to perform a chi-square goodness of fit test to see whether the differences are statistically significant.\nTo decide which hypothesis to choose, we’ll set a significance level of 0.1.\nlate_shipments is available; tibble, dplyr, ggplot2, and infer are loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#testing-sample-size",
    "href": "Hypothesis_Testing_in_R_C4.html#testing-sample-size",
    "title": "59  Non-Parametric Tests",
    "section": "59.1 Testing sample size",
    "text": "59.1 Testing sample size\nIn order to conduct a hypothesis test, and be sure that the result is fair, a sample must meet three requirements: it is a random sample of the population; the observations are independent; and there are enough observations. Of these, only the last condition is easily testable with code.\nThe minimum sample size depends on the type of hypothesis tests you want to perform. Let’s test some scenarios on the late_shipments dataset.\nlate_shipments is available; dplyr is loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#specifying-and-hypothesizing",
    "href": "Hypothesis_Testing_in_R_C4.html#specifying-and-hypothesizing",
    "title": "59  Non-Parametric Tests",
    "section": "59.2 Specifying and hypothesizing",
    "text": "59.2 Specifying and hypothesizing\nIn Chapter 3, you ran a two sample proportion test on the proportion of late shipments across freight cost groups. Recall the hypotheses.\n\nH0: vendor_inco_term and freight_cost_group are independent.\n\n\nHA: vendor_inco_term and freight_cost_group are associated.\n\nLet’s compare that traditional approach using prop_test() with a simulation-based infer pipeline.\nlate_shipments is available; dplyr and infer are loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#generating-calculating",
    "href": "Hypothesis_Testing_in_R_C4.html#generating-calculating",
    "title": "59  Non-Parametric Tests",
    "section": "59.3 Generating & calculating",
    "text": "59.3 Generating & calculating\nThe infer pipeline for hypothesis testing requires four steps to calculate the null distribution: specify, hypothesize, generate, and calculate.\nLet’s continue the pipeline you began in the previous coding exercise. We’ll get a set of differences in proportions that are distributed as though the null hypothesis, that the proportion of late shipments is the same across freight cost groups, is true.\nlate_shipments is available; dplyr, infer, and ggplot2 are loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#observed-statistic-and-p-value",
    "href": "Hypothesis_Testing_in_R_C4.html#observed-statistic-and-p-value",
    "title": "59  Non-Parametric Tests",
    "section": "59.4 Observed statistic and p-value",
    "text": "59.4 Observed statistic and p-value\nYou now have a null distribution. In order to get a p-value and weigh up the evidence against the null hypothesis, you need to calculate the difference in proportions that is observed in the late_shipments sample.\nlate_shipments is available; dplyr, infer, and ggplot2 are loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#simulation-based-t-test",
    "href": "Hypothesis_Testing_in_R_C4.html#simulation-based-t-test",
    "title": "59  Non-Parametric Tests",
    "section": "59.5 Simulation-based t-test",
    "text": "59.5 Simulation-based t-test\nIn Chapter 2 you manually performed the steps for a t-test to explore these hypotheses.\nH0: The mean weight of shipments that weren’t late is the same as the mean weight of shipments that were late.\nHA: The mean weight of shipments that weren’t late is less than the mean weight of shipments that were late.\nYou can run the test more concisely using infer’s t_test().\n\nlate_shipments %&gt;% t_test( weight_kilograms ~ late, order = c(“No”, “Yes”), alternative = “less” )\n\nt_test() assumes that the null distribution is normal. We can avoid assumptions by using a simulation-based non-parametric equivalent.\nlate_shipments is available; dplyr and infer are loaded."
  },
  {
    "objectID": "Hypothesis_Testing_in_R_C4.html#rank-sum-tests",
    "href": "Hypothesis_Testing_in_R_C4.html#rank-sum-tests",
    "title": "59  Non-Parametric Tests",
    "section": "59.6 Rank sum tests",
    "text": "59.6 Rank sum tests\nAnother class of non-parametric hypothesis tests are called rank sum tests. Ranks are the positions of numeric values from smallest to largest. Think of them as positions in running events: whoever has the fastest (smallest) time is rank 1, second fastest is rank 2, and so on.\nBy calculating on the ranks of data instead of the actual values, you can avoid making assumptions about the distribution of the test statistic. It’s most robust in the same way that a median is more robust than a mean.\nTwo commonly used rank-based tests are the Wilcoxon-Mann-Whitney test, which is like a non-parametric t-test, and the Kruskal-Wallis test, which is like a non-parametric ANOVA.\nlate_shipments is available."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#starts-with-ends-with",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#starts-with-ends-with",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "32.1 Starts with, ends with",
    "text": "32.1 Starts with, ends with\nYou’ve already seen how you can search for certain characters at the beginning of a string using the caret “^”. Of course, regular expressions also offer a way to search for things at the end of a string. This is what the dollar sign “$” will do.\nWhen creating a pattern to look for something at the beginning of a line, use the caret followed by a search term “^”. When looking for something at the end, type the search term first and then append the dollar sign ”$”. The order of appearance is very important when creating regular expressions."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun the first line to have a look at the movie titles you’re working with.\nCreate a pattern that lists all the movies that start with “The”.\nCreate a pattern that searches movies that end with “3D”.\n\n\n\n\nE1.R\n\n# Familiarize yourself with the vector by printing it\nmovie_titles\n\n# List all movies that start with \"The\"\nmovie_titles[str_detect(\n  movie_titles,\n  pattern = \"^The\"\n)]\n\n# List all movies that end with \"3D\"\nmovie_titles[str_detect(\n  movie_titles,\n  pattern = \"3D$\"\n)]"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#if-you-dont-know-what-youre-looking-for",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#if-you-dont-know-what-youre-looking-for",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "32.2 If you don’t know what you’re looking for",
    "text": "32.2 If you don’t know what you’re looking for\nSo far you’ve used str_detect() which returns TRUE if the pattern matches and FALSE otherwise. But regular expressions are also excellent at extracting the searched term from a larger amount of text. You can use the str_match() function for that.\nThe next special character you’ll get to know is the period: “.”. The period matches any character, it’s like a wild card. So if you search for example for “…” you will find three characters - be it letters or numbers or even white spaces.\nThis is pretty handy, except if you need to search for an actual full stop “.”. In that case: escape the period with two backslashes: “\\.”"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp-1",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp-1",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nMatch not only Saw 4 but also the other sequels.\nMatch the first four characters of all the movie titles that start with “K”.\nDetect the movie that ends with an actual full stop “.”.\n\n\n\nE2.R\n\n# Here's an example pattern that will find the movie Saw 4\nstr_match(movie_titles, pattern = \"Saw 4\")\n\n# Match all sequels of the movie \"Saw\"\nstr_match(movie_titles, pattern = \"Saw..\")\n\n# Match the letter K and three arbitrary characters\nstr_match(movie_titles, pattern = \"^K...\")\n\n# Detect whether the movie titles end with a full stop\nstr_detect(movie_titles, pattern = \"\\\\.$\")"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#digits-words-and-spaces",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#digits-words-and-spaces",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "32.3 Digits, words and spaces",
    "text": "32.3 Digits, words and spaces\nSo far in your life you might have always searched for one number or word exactly. Now you have a much more flexible tool at hand, to search for:\n\n\\d digits (zero to nine) \\w word characters (letters, numbers or underscores) \\s white spaces (also tabs and line breaks)\n\nPlus, you can use square brackets [A-Za-z] and have a list of possible values inside.\nYou already found all sequels of “Saw”. Can you create a pattern that matches all sequels in the list movie_titles? They usually have a number at the end, right?\nFurthermore, the list contains duplicates introduced by “Grey” (British) and “Gray” (American English). Create a pattern that matches both versions of the color.\nLastly, list out all movie titles that contain special, non word characters."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp-2",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp-2",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nMatch all movies titles that end with a space followed by a digit.\nMatch both “Grey” and “Gray” with a custom pattern […].\nWrite a pattern that matches everything but word characters \\w and spaces \\s.\n\n\n\nE3.R\n\n# List all movies that end with a space and a digit\nmovie_titles[str_detect(movie_titles,\n  pattern = \"\\\\s\\\\d$\"\n)]\n\n# List all movies that contain \"Grey\" or \"Gray\"\nmovie_titles[str_detect(movie_titles,\n  pattern = \"Gr\\\\wy\"\n)]\n\n# List all movies with strange characters (no word or space)\nmovie_titles[str_detect(movie_titles,\n  pattern = \"[^\\\\w\\\\s]\"\n)]"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#match-repetitions",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#match-repetitions",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "32.4 Match repetitions",
    "text": "32.4 Match repetitions\nAlright, in this exercise your patterns will get much more powerful. You now know how to use repetitions to match exactly the desired number of digits or letters.\nBy using a number in curly braces {} you can define how many occurrences you want to search for. With one number e.g. {2}, you’ll match that exact number of repetitions. With a number and a comma, the number serves as a minimum: {2,} (two repetitions or more). The second number is a maximum, so {2,4} is between 2 and 4 repetitions.\nThe plus sign + and the asterisk * are an even quicker way to define repetition: The first will match one or more occurrences and the latter will match zero, one or more. These two are often used in combination with the period . to match an unknown number of arbitrary characters."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp-3",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp-3",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFind all titles that contain a number with two or more digits.\nMatch the first word of every title by searching one or more word characters at the beginning of the string.\nMatch the word “Knight” and everything that comes before it.\n\n\n\nE4.R\n\n# This lists all movies with two or more digits in a row\nmovie_titles[str_detect(\n  movie_titles,\n  pattern = \"\\\\d{2,}\"\n)]\n\n# List just the first words of every movie title\nstr_match(movie_titles, pattern = \"\\\\w+\")\n\n# Match everything that comes before \"Knight\"\nstr_match(movie_titles, pattern = \".*Knight\")"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#this-or-that",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#this-or-that",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "32.5 This or that",
    "text": "32.5 This or that\nSearching one word is easy, right? But searching exactly two or three words, you could not do that with a plain old “Control + F” search. But with regular expressions you are now able to define a search pattern that achieves this. You can use the str_view() to see what your regular expression matches.\nWhen you connect multiple words with a pipe operator | you will match both the thing that comes before the pipe and the thing after. And you’re not limited to just two. You can also have three options connected with two pipes Hello Anna|Berta|Colin.\nYou can furthermore use parentheses to group certain words together, looking e.g. for Hello (Anna|Berta|Colin) will produce a different result than the pattern above. Try out both options and compare the results."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp-4",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp-4",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a pattern that searches for movies starting with “Finding” and followed by the words “Nemo”, “Harmony” or “Dory”.\nNow create the same pattern but wrap the three possibilities in parentheses () to compare the results.\nChoose the one of the two patterns that matches the full movie names “Finding Nemo”, “Finding Harmony” and “Finding Dory” and pass it to the third str_match() call.\n\n\n\nE5.R\n\n# Append the three options: Match Nemo, Harmony or Dory\nstr_view(lines, pattern = \"Finding Nemo|Harmony|Dory\")\n\n# Wrap the three options in parentheses and compare the results\nstr_view(lines, pattern = \"Finding (Nemo|Harmony|Dory)\")\n\n# Use the pattern from above that matched the whole movie names\nstr_match(lines, pattern = \"Finding (Nemo|Harmony|Dory)\")"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#the-question-mark-and-its-two-meanings",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#the-question-mark-and-its-two-meanings",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "32.6 The question mark and its two meanings",
    "text": "32.6 The question mark and its two meanings\nThe or operator is good if you know exactly what options are valid, and also if you’re sure that one of the options is present. But what if you want to match a pattern where one part is sometimes present and sometimes isn’t? This is where the question mark ? comes in:\nThe ? can make the preceding group or character optional. With it, a regular expression matches, even if a certain part of the pattern is missing. But be aware, if it follows a multiplier like * or +, the question mark can have a second effect:\nThe ? can also make the preceding multiplier “lazy” instead of “greedy”. This means that instead of regular expressions looking for the maximum number of characters, the ? has the power to find the minimum number of text matches."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp-5",
    "href": "Intermediate_Regular_Expressions_in_R_C1.html#instructions-100-xp-5",
    "title": "32  Regular Expressions: Writing Custom Patterns",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nMatch both the singular “Screen” as well as the plural “Screens” by making the last “s” optional.\nMatch a random amount of arbitrary characters in front of a comma by using .*.\nMatch the same pattern with a question mark ? after the star - do you spot the difference?\n\n\n\nE6.R\n\n# Match both Screen and Screens by making the last \"s\" optional\nstr_match(lines, pattern = \"Screens|Screen\")\n\n# Match a random amount of arbitrary characters, followed by a comma\nstr_match(lines, pattern = \".*,\")\n\n# Match the same pattern followed by a comma, but the \"lazy\" way\nstr_match(lines, pattern = \".*?,\")"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#stop-pasting-start-gluing",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#stop-pasting-start-gluing",
    "title": "33  Creating Strings with Data",
    "section": "33.1 Stop pasting, start gluing",
    "text": "33.1 Stop pasting, start gluing\nThe function paste() concatenates strings with a space in between, so paste(“Hi”, “there”) will output “Hi there”. There is also the paste0() function that doesn’t add a space, the result of which would be “Hithere”. But when you concatenate multiple strings and variables, you end up writing a lot of double quotes ” and commas , and with code that is not very readable. Plus you can only work with variables that are already present.\nThese are the two use cases where the glue() function really shines. You can either work with variables that are available in the global scope or you can create variables on the fly. In this exercise, you’ll see the difference between paste() and glue() in action."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp",
    "title": "33  Creating Strings with Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRecreate the sentence that was created with paste0() using glue().\nCreate a temporary variable n which stores the length of characters in firstname and pass it sentence being created.\n\n\n\nE1.R\n\nfirstname &lt;- \"John\"\nlastname &lt;- \"Doe\"\n\npaste0(firstname, \"'s last name is \", lastname, \".\")\n\n# Create the same result as the paste above with glue\nglue(\"___'s last name is ___.\")\n\n# Create a temporary varible \"n\" and use it inside glue\nglue(\n  \"The name {firstname} consists of ___ characters.\",\n  ___ = nchar(firstname)\n)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#gluing-data-frames",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#gluing-data-frames",
    "title": "33  Creating Strings with Data",
    "section": "33.2 Gluing data frames",
    "text": "33.2 Gluing data frames\nData is very often stored in data frames. Most of the time, we want to create an analysis that is also readable for humans. For example, it could be valuable to print a sentence about the size of our data frames. By combining glue() with nrow() and ncol(), we can return values and create a sentence that reports on the dimensions of our data frames.\nLuckily, the glue package is part of the tidyverse package collection and was built with data frames in mind, so we can operate on entire data frame columns. We can, for example, use it inside mutate() to create a new column with a concatenated string that contains values of other columns. In this exercise, you will apply these examples on the users data frame which contains values of other columns."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-1",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-1",
    "title": "33  Creating Strings with Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the glue() function to report the number of rows and columns users has, by storing them in the n and m temporary variables respectively.\nInspect the data frame users by just executing the line that prints the column names.\nMutate users to create a new column n_logins which reports the number of times users logged in by using the name and logins columns respectively.\n\n\n\nE2.R\n\n# Create two temporary variables \"n\" and \"m\" and use them\nglue(\n  \"The data frame 'users' has ___ rows and ___ columns.\",\n  ___ = nrow(users),\n  ___ = ncol(users)\n)\n\n# This lists the column names of the data frame users\ncolnames(users)\n\n# Use them to create a sentence about the numbers of logins\nusers %&gt;% mutate(\n  n_logins = glue(\"___ logged in ___ times.\")\n)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#formulating-a-question-from-a-list",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#formulating-a-question-from-a-list",
    "title": "33  Creating Strings with Data",
    "section": "33.3 Formulating a question from a list",
    "text": "33.3 Formulating a question from a list\nPart of working with data is becoming proficient and familiar with inspecting and understanding data frames, lists, and vectors. However, it’s important to be able to communicate your findings with reports and plots in a human-readable way across any organization. In the video exercise, you saw how to create sentences from lists with the glue_collapse() function.\nIn this exercise, you will create a small questionnaire with glue_collapse() that could later be fed to the base R command menu()."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-2",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-2",
    "title": "33  Creating Strings with Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nConstruct a well formulated question by passing fruits to glue_collapse().\nSeparate the fruits with a comma and a space “,” and an “, or” between the last two elements.\n\n\n\nE3.R\n\nfruits &lt;- list(\"Apple\", \"Banana\", \"Cherries\", \"Dragon Fruit\")\n\n# Use \", \" as a separator and \", or \" between the last fruits\nquestion &lt;- glue(\n  \"Which of these do you prefer: {answers}?\",\n  answers = glue_collapse(\n    fruits,\n    sep = \", \",\n    last = \", or \"\n  )\n)\n\n# Print question\nprint(question)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#collapsing-data-frames",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#collapsing-data-frames",
    "title": "33  Creating Strings with Data",
    "section": "33.4 Collapsing data frames",
    "text": "33.4 Collapsing data frames\nIn the last exercise you already combined glue() and glue_collapse() to create a correct English sentence from a vector. But very often, you will work not with vectors but with data frames. Luckily the workflow for data frame columns is the same as it is for vectors.\nIn the scope you have again our data frame users with three names and numbers of logins in it. Use glue_collapse() and print the columns of the data frame in a human readable form."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-3",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-3",
    "title": "33  Creating Strings with Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPrint the column names of users by printing each column name separated by “,” in glue_collapse().\nUse glue() and glue_collapse() to list the names of users stored in the name column in a human-readable way, while ensuring the last name is preceded by ” and “.\nRepeat the same steps, this time for the number of logins stored in the logins column.\n\n\n\nE4.R\n\n# List colnames separated a comma and a white space\nglue_collapse(colnames(users), sep = \", \")\n\n# Use \" and \" as a separator for the last elements\nglue(\n  \"Our users are called {names}.\",\n  names = glue_collapse(users$name, sep = \", \", last = \" and \")\n)\n\n# Use the same way to output also the \"logins\" of the users\nglue(\n  \"Our users have logged in {logins} times.\",\n  logins = glue_collapse(users$logins, sep = \", \", last = \" and \")\n)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#construct-or-patterns-with-glue",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#construct-or-patterns-with-glue",
    "title": "33  Creating Strings with Data",
    "section": "33.5 Construct “or patterns” with glue",
    "text": "33.5 Construct “or patterns” with glue\nIn the last two lessons you learned to create strings from other strings or vectors or lists, even data frames. Together with the knowledge you have about regular expressions, you are now able to create patterns for all these data types.\nUsing glue_collapse() you can concatenate the contents of a vector of the column of a data frame and create long patterns that would otherwise be very tedious and error prone to write from hand.\nFor this exercise, we have a vector users as an input. It is the result of a database export. It contains some rows of a database but also some other information that we don’t care about. Using regular expressions we can match only the parts that we are interested in, in this case, the usernames."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-4",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-4",
    "title": "33  Creating Strings with Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the vector usernames to create a pattern that matches either of the three names in the vector.\nBind the three names together using the regular expression “or” character as a separator.\n\n\n\nE5.R\n\nusernames &lt;- c(\"Bryan\", \"Barbara\", \"Tom\")\n\n# Create a pattern using the vector above separated by \"or\"s\nuser_pattern &lt;- glue_collapse(usernames, sep = \"|\")\n\nstr_view(users, user_pattern)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#using-the-or-pattern-with-a-larger-dataset",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#using-the-or-pattern-with-a-larger-dataset",
    "title": "33  Creating Strings with Data",
    "section": "33.6 Using the “or pattern” with a larger dataset",
    "text": "33.6 Using the “or pattern” with a larger dataset\nNow that you’ve understood the principle of concatenating multiple possibilities from a vector, you’ll go one step further and apply this to a larger dataset. Available in the global scope are two variables: articles and politicians. The first is a collection of news articles about Swiss politics. The latter is a list of names of Swiss politicians that appear in the articles.\nNow it’s your job to find out which names appear in which of the articles and which politician appears how many times in all the articles."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-5",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-5",
    "title": "33  Creating Strings with Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the vector politicians to create a regular expression that matches all the names that are stored in that vector.\nCreate a new column in the data frame articles which contains all politician names that appear in the column text.\nGlue all articles together so you’re able to count the number of occurrences per politician more easily.\nUse the vector politicians as a pattern and pass it to str_count().\n\n\n\nE6.R\n\n# Construct a pattern that searches for all politicians\npolit_pattern &lt;- glue_collapse(politicians, sep = \"|\")\n\n# Use the pattern to match all names in the column \"text\"\narticles %&lt;&gt;%\n  mutate(mentions = str_match_all(text, polit_pattern))\n\n# Concatenate all elements in the column text\nall_articles_in_one &lt;- glue_collapse(articles$text)\n\n# Pass the vector politicians to count all its elements\nstr_count(all_articles_in_one, politicians)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#make-advanced-patterns-more-readable",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#make-advanced-patterns-more-readable",
    "title": "33  Creating Strings with Data",
    "section": "33.7 Make advanced patterns more readable",
    "text": "33.7 Make advanced patterns more readable\nAlright, you already master creating patterns that match a list of names by collapsing them using the pipe | as a separator. But collapsing is also useful to concatenate small, digestible parts of a pattern. Using glue_collapse() and vector (with or without names, both is possible) as its only input, you can create very long and complicated patterns out of small pieces that are much easier to interpret.\nThe variable users was exported from our database and is again available in the global scope. But in this exercise, we are not only interested in the usernames, but also the digit and the email address that follow."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-6",
    "href": "Intermediate_Regular_Expressions_in_R_C2.html#instructions-100-xp-6",
    "title": "33  Creating Strings with Data",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nHave a look at the contents of users again.\nWhere we expect our username, write a pattern that matches one or more alphabetical letters. Use square brackets [] to create this custom pattern.\nWhere we expect the number of logins for that user, write a pattern that matches one or more digits. Use the digit character class for this.\nAs the email comes last in every line, write a simple pattern that matches one or more arbitrary characters.\n\n\n\nE7.R\n\n# Familiarize yourself with users by printing its contents\nprint(users)\n\nadvanced_pattern &lt;- glue_collapse(c(\n  # Match one or more alphabetical letters\n  \"username\" = \"^[A-Za-z]+\",\n  \": \",\n  # Match one or more digit\n  \"logins\" = \"\\\\d+\",\n  \", \",\n  # Match one or more arbitrary characters\n  \"email\" = \".+$\"\n))\n\nstr_view(users, advanced_pattern)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C3.html#match-all-capturing-groups",
    "href": "Intermediate_Regular_Expressions_in_R_C3.html#match-all-capturing-groups",
    "title": "34  Extracting Structured Data From Text",
    "section": "34.1 Match all capturing groups",
    "text": "34.1 Match all capturing groups\nIn this exercise, you will work with a text file named top_10 which stores movie names and their rank. In this multi-line text, \\n is used to start a new line. You will use the str_split() function to split the text file into multiple lines.\nThe newly created one-rowed matrix top_10_lines then contains ten lines with the same pattern: The rank of the movie, followed by a dot and a space and the movie title itself. The function str_match() and two capturing groups () will make it possible to extract these two pieces of information from plain text into a tabular form."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C3.html#instructions-100-xp",
    "href": "Intermediate_Regular_Expressions_in_R_C3.html#instructions-100-xp",
    "title": "34  Extracting Structured Data From Text",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the str_split() function to split the text into its lines, outputting a character matrix by enabling simplify.\nFamiliarize yourself with the structure of a line. It contains the rank and the title of a movie.\nExtract the rank and the title of a movie by using capturing groups in the str_match() function.\n\n\n\nE1.R\n\n# Split the input by line break and enable simplify\ntop_10_lines &lt;- str_split(\n  top_10,\n  pattern = \"\\\\n\",\n  simplify = TRUE\n)\n\n# Inspect the first three lines and analyze their form\ntop_10_lines[1:3]\n\n# Add to the pattern two capturing groups that match rank and title\nstr_match(\n  top_10_lines,\n  pattern = \"(\\\\d+)\\\\. (.+)\"\n)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C3.html#search-and-replace",
    "href": "Intermediate_Regular_Expressions_in_R_C3.html#search-and-replace",
    "title": "34  Extracting Structured Data From Text",
    "section": "34.2 Search and replace",
    "text": "34.2 Search and replace\nThe function str_replace() is a general function to replace parts of a string. A common application is to replace something with an empty string - which is a simple way to remove unneeded parts from a string.\nWith capturing groups, str_replace() gets even more interesting: They enable you to change the order of things. By adding so called “backreferences” to the replacement, str_replace() will replace these references with the contents of the corresponding capturing group. For example: If you write \\1, this will be replaced with the 1st capturing group.\nIn this exercise, you’ll see the first use (remove a substring) and the second (reorder two parts of a string) side by side. In the scope, you’ll find the variable top_10_lines from the last exercise."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C3.html#instructions-100-xp-1",
    "href": "Intermediate_Regular_Expressions_in_R_C3.html#instructions-100-xp-1",
    "title": "34  Extracting Structured Data From Text",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRemove 3D from the end of each line in top_10_lines by replacing it with an empty string.\nForm a new sentence with the two capturing groups. Reorder them so they result in e.g. “Karate Kid is on rank 1”.\n\n\n\nE2.R\n\n# Remove a space followed by \"3D\" at the end of the line\nstr_replace(\n  top_10_lines,\n  pattern = \" 3D\",\n  replacement = \"\"\n)\n\n# Use backreferences 2 and 1 to create a new sentence\nstr_replace(\n  top_10_lines,\n  pattern = \"(\\\\d+)\\\\. (.*)\",\n  replacement = \"\\\\2 is at rank \\\\1\"\n)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C3.html#creating-a-regex-that-matches-your-needs",
    "href": "Intermediate_Regular_Expressions_in_R_C3.html#creating-a-regex-that-matches-your-needs",
    "title": "34  Extracting Structured Data From Text",
    "section": "34.3 Creating a regex that matches your needs",
    "text": "34.3 Creating a regex that matches your needs\nIn this exercise, you’re going to replicate what you just saw in the video exercise by extracting the letters “3D” from the “line” column from the screens_per_movie data frame.\nFor the extract() function to work correctly, you need to make sure that the following requirement is met: The number of capturing groups in the regular expression regex must be identical to the length of the vector into. If that’s not the case, you will run into an error.\nCan you resolve this issue so “3D” and that one or more number \\d+ get extracted correctly from the data frame screens_per_movie?"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C3.html#instructions-100-xp-2",
    "href": "Intermediate_Regular_Expressions_in_R_C3.html#instructions-100-xp-2",
    "title": "34  Extracting Structured Data From Text",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a regular expression regex that has two capturing groups (). Their contents will be extracted into the new columns.\nMake sure you do not remove the original text column.\nMake sure the second captured group gets converted into numbers.\n\n\n\nE3.R\n\nextract(\n  screens_per_movie,\n  line,\n  into = c(\"is_3d\", \"screens\"),\n  # Capture two groups: \"3D\" and \"one or more digits\"\n  regex = \"(3D).*?(\\\\d+)$\",\n  # Pass TRUE or FALSE, the original column should not be removed\n  remove = FALSE,\n  # Pass TRUE or FALSE, the result should get converted to numbers\n  convert = TRUE\n)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C3.html#extracting-an-advanced-regular-expression",
    "href": "Intermediate_Regular_Expressions_in_R_C3.html#extracting-an-advanced-regular-expression",
    "title": "34  Extracting Structured Data From Text",
    "section": "34.4 Extracting an advanced regular expression",
    "text": "34.4 Extracting an advanced regular expression\nIn this exercise, you will build on top of the prior exercises by creating a more advanced regular expression to capture the title of the movie, the company name of the distributor and the number of screens in each line of the screens_per_movie data frame.\nEvery line of screens_per_movie contains these three sections. Using extract you will extract these three and get three new columns with exactly the information you want in a tabular and structured form. This step is key if you want to make sense of unstructured data and bring it into a form that you can later analyze and visualize."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C3.html#instructions-100-xp-3",
    "href": "Intermediate_Regular_Expressions_in_R_C3.html#instructions-100-xp-3",
    "title": "34  Extracting Structured Data From Text",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nFirst, inspect the first three rows of screens_per_movie and make yourself familiar with the structure of the data.\nBring the three capturing groups “to life”: The first should match anything, the second match one or more word characters and the last match one or more digits.\nNow use the extract() function create the three new columns “title”, “distributor” and “screens” from the column line that contains our raw text input.\n\n\n\nE4.R\n\n# Print the first three lines of screens_per_movie\nscreens_per_movie[1:3, ]\n\n\n\n\n# Print the first three lines of screens_per_movie\nscreens_per_movie[1:3, ]\n\n# Match anything, one or more word chars and one or more digits\nstr_match(\n  screens_per_movie[3, ]$line,\n  \"(.*)\\\\s{2,}(\\\\w+)\\\\s{2,}(\\\\d+)\"\n)\n\n\n\n\n\n# Print the first three lines of screens_per_movie\nscreens_per_movie[1:3, ]\n\n# Match anything, one or more word chars and one or more digits\nstr_match(\n  screens_per_movie[3, ]$line,\n  \"(.*)\\\\s{2,}(\\\\w+)\\\\s{2,}(\\\\d+)\"\n)\n\n# Extract the column line into title, distributor, screens\nextract(\n  screens_per_movie,\n  col = line,\n  into = c(\"title\", \"distributor\", \"screens\"),\n  regex = \"(.*)\\\\s{2,}(\\\\w+)\\\\s{2,}(\\\\d+)\")"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C3.html#extract-names-with-context",
    "href": "Intermediate_Regular_Expressions_in_R_C3.html#extract-names-with-context",
    "title": "34  Extracting Structured Data From Text",
    "section": "34.5 Extract names with context",
    "text": "34.5 Extract names with context\nLet’s take out our dataset about Swiss politicians again. It consist of two variables: articles which is a collection of news articles about Swiss politics and politicians which is a vector with several names of Swiss politicians.\nYou already counted the number of occurrences per name, but wouldn’t it be interesting if you could not only count the names but also see in what context the names are used? You could for example compare whether the contexts differ from female to male politicians. To do so, you’ll have to extract the text surrounding our politician names.\nAs the text contains word characters \\w as well as punctuation [:punct:] like periods . or commas ,, you will have to create a pattern that matches both of these character types."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C3.html#instructions-100-xp-4",
    "href": "Intermediate_Regular_Expressions_in_R_C3.html#instructions-100-xp-4",
    "title": "34  Extracting Structured Data From Text",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the vector politicians and collapse it to create an “or pattern” like you did in chapter 2.\nCreate a custom pattern in square brackets [] that matches both word characters as well as punctuations.\nUsing glue, add the newly created context both in front of as well as after the polit_pattern. The \\s? indicated that after there can be a space or no space after the politician names.\n\n\n\nE5.R\n\n# Create our polit_pattern again by collapsing \"politicians\"\npolit_pattern &lt;- glue_collapse(politicians, sep = \"|\")\n\n# Match one or more word characters or punctuations\ncontext &lt;- \"([\\\\w[:punct:]]+\\\\s){0,10}\"\n\n# Add this pattern in front and after the polit_pattern\npolit_pattern_with_context &lt;- glue(\n  \"{context}({polit_pattern})\\\\s?{context}\"\n)\n\nstr_extract_all(\n  articles$text,\n  pattern = polit_pattern_with_context\n)"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C4.html#finding-a-match-to-a-search-typo",
    "href": "Intermediate_Regular_Expressions_in_R_C4.html#finding-a-match-to-a-search-typo",
    "title": "35  Similarities Between Strings",
    "section": "35.1 Finding a match to a search typo",
    "text": "35.1 Finding a match to a search typo\nHuman input is very error-prone. People mistype all kinds of texts, including their name or address, and you as a data scientist need to find a way to handle that. Calculating string distances is one way to tackle this problem.\nIn our small vector usernames you’ll find three different names. You will be tasked with finding the closest name possible to the inputted name “Emile Brown”. Can you find a similar name in the vector usernames? Use amatch() to search the vector and print out a recommendation similar to the one you have seen on Google."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C4.html#instructions-100-xp",
    "href": "Intermediate_Regular_Expressions_in_R_C4.html#instructions-100-xp",
    "title": "35  Similarities Between Strings",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSpecify the maximum edit distance for the amatch() function as 1.\nUse the return value of amatch() which is stored in closest_index to print the name in usernames.\n\n\n\nE1.R\n\nusernames &lt;- c(\"Max Power\", \"Emilie Brown\", \"Max Mustermann\")\n\n# Search usernames with a maximum edit distance of 1\nclosest_index &lt;- amatch(\n  x = \"Emile Brown\",\n  table = usernames,\n  maxDist  = 1,\n  method = \"lv\"\n)\n\n# Print the matched name in usernames at closest_index\nprint(glue(\n  \"Did you mean {name_matched}?\",\n  name_matched = usernames[closest_index]\n))"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C4.html#trying-out-different-methods",
    "href": "Intermediate_Regular_Expressions_in_R_C4.html#trying-out-different-methods",
    "title": "35  Similarities Between Strings",
    "section": "35.2 Trying out different methods",
    "text": "35.2 Trying out different methods\nPerfect, you already have learned about multiple methods of calculating string distances. Which method to use depends on a lot of circumstances, so it’s a good idea to play around with the different methods and their parameters a bit to get to know them better. For this exercise you’ll use the search term “Marya Carey” - a mistyped version of the name “Mariah Carey”. How similar is the mistyped name to the real one with different methods of string distances?\nThe goal is to find parameters that will yield a low distance on the two names described above while maintaining a large distance to the other names in the list that are not the person one is searching for."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C4.html#instructions-100-xp-1",
    "href": "Intermediate_Regular_Expressions_in_R_C4.html#instructions-100-xp-1",
    "title": "35  Similarities Between Strings",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nGenerate the q-grams for substring length values of 1 and 2.\nCalculate the string distance between search and names using the q-gram method for substring length values of 1 and 2.\nCalculate the string distance between search and names by using the “osa” method.\n\n\n\nE2.R\n\nsearch &lt;- \"Mariah Carey\"\nnames &lt;- c(\"M. Carey\", \"Mick Jagger\", \"Michael Jackson\")\n\n# Pass the values 1 and 2 as \"q\" and inspect the qgrams\nqgrams(\"Mariah Carey\", \"M. Carey\", q = 1)\nqgrams(\"Mariah Carey\", \"M. Carey\", q = 2)\n\n# Try the qgram method on the variables search and names\nstringdist(search, names, method = \"qgram\", q = 1)\nstringdist(search, names, method = \"qgram\", q = 2)\n\n# Try the default method (osa) on the same input and compare\nstringdist(search, names, method = \"osa\")"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C4.html#performing-a-string-distance-join",
    "href": "Intermediate_Regular_Expressions_in_R_C4.html#performing-a-string-distance-join",
    "title": "35  Similarities Between Strings",
    "section": "35.3 Performing a string distance join",
    "text": "35.3 Performing a string distance join\nBringing together two different data sources is a very common task in data analysis. Whenever possible, you should use clearly identifiable values like an email address to join two tables by. But what if a user only inputted their name and you have to look it up in a user database? The difficulty: People might abbreviate their first or last name, mistype something, or leave out parts entirely.\nIn the scope there are two data frames: user_input and database. The first contains the flawed user input and the second the correct names, but both data sources contain the same 100 names. How many of them can you match with a string distance join? By the way: There is no distance method defined, so the default, Optimal String Alignment distance “osa” will be used."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C4.html#instructions-100-xp-2",
    "href": "Intermediate_Regular_Expressions_in_R_C4.html#instructions-100-xp-2",
    "title": "35  Similarities Between Strings",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nJoin user_input and database with a maximum string distance max_dist so exactly eighty names are matched successfully. Experiment until you find the right maximum distance.\nUse the newly created table joined to print a human friendly report sentence.\n\n\n\nE3.R\n\n # Join the data frames on a maximum string distance of 2\njoined &lt;- stringdist_join(\n  user_input,\n  database,\n  by = c(\"user_input\" = \"name\"),\n  max_dist = 3,\n  distance_col = \"distance\",\n  ignore_case = TRUE\n)\n\n# Print the number of rows of the newly created data frame\nprint(glue(\n  \"{n} out of 100 names were matched successfully\",\n  n = nrow(joined)\n))"
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C4.html#finding-matches-based-on-two-conditions",
    "href": "Intermediate_Regular_Expressions_in_R_C4.html#finding-matches-based-on-two-conditions",
    "title": "35  Similarities Between Strings",
    "section": "35.4 Finding matches based on two conditions",
    "text": "35.4 Finding matches based on two conditions\nIn this exercise, you’ll match 2 datasets with corresponding movie titles, but that also contain typos. In the first table movie_titles, there are ten movies that you should match with the second table movie_db. But they are based on scanned documents and they contain errors by the Optical Character Recognition software.\nBoth tables contain the columns title and year. Use these to find matches between them.\nCreate 2 helper functions that match entries that are similar or equal. One for the movie titles (based on stringdist()) and one for comparing years, using abs() (that returns the delta)."
  },
  {
    "objectID": "Intermediate_Regular_Expressions_in_R_C4.html#instructions-100-xp-3",
    "href": "Intermediate_Regular_Expressions_in_R_C4.html#instructions-100-xp-3",
    "title": "35  Similarities Between Strings",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nMake the function is_string_distance_below_three() return TRUE if the stringdistance between left and right is below 3.\nMake is_closer_than_three_years() return TRUE if the absolute difference between left and right is smaller than three.\nUse the helper functions to join the two data frames on the two columns “title” and “year”\n\n\n\nE4.R\n\n# Calculate the string distance - it should be smaller than 3\nis_string_distance_below_three &lt;- function(left, right) {\n  stringdist(left, right) &lt; 3\n}\n\nis_string_distance_below_three(\"Hi there\", \"Hi there\")\n\n\n\n\nis_string_distance_below_three &lt;- function(left, right) {\n  stringdist(left, right) &lt; 3\n}\n\n# Check if the absolute value between left and right is smaller than three\nis_closer_than_three_years &lt;- function(left, right) {\n  abs(left - right) &lt; 3\n}\n\nis_closer_than_three_years(2015, 2014)\n\n\n\n\n\n\n\nis_string_distance_below_three &lt;- function(left, right) {\n  stringdist(left, right) &lt; 3\n}\n\nis_closer_than_three_years &lt;- function(left, right) {\n  abs(left - right) &lt; 3\n}\n\n# Join by \"title\" and \"year\" with our two helper functions\nfuzzy_left_join(\n  movie_titles, movie_db,\n  by = c(\"title\", \"year\"),\n  match_fun = c(\"title\" = is_string_distance_below_three, \n                \"year\" = is_closer_than_three_years)\n)"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#introduction-to-spatial-data",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#introduction-to-spatial-data",
    "title": "71  Plotting Polygons",
    "section": "71.1 Introduction to Spatial Data",
    "text": "71.1 Introduction to Spatial Data\nWe have been mapping points, but there are several spatial features that can be mapped, including polygons. In R, polygons are often stored in a SpatialPolygonsDataFrame that holds the polygon, coordinate information, and a data frame with one row per polygon.\nA SpatialPolygonsDataFrame called shp that contains the zip code boundaries for North Carolina has been loaded for you. shp has five slots that store various types of information:\ndata: data associated with each polygon polygons: coordinates to plot polygons plotOrder: order in which polygons are plotted bbox: bounding box for geographic data (i.e., a rectangle) proj4string: coordinate reference system Let’s take a closer look inside the shp object."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp",
    "title": "71  Plotting Polygons",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nPrint the summary() of the shp data.\nPrint the class of shp.\nPrint the slot names of shp.\n\n\n\nE1.R\n\n# Print a summary of the `shp` data\nsummary(shp)\n\n\n\n# Print the class of `shp`\nclass(shp)\n\n\n\n# Print the slot names of `shp`\nslotNames(`shp`)"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#exploring-spatial-data",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#exploring-spatial-data",
    "title": "71  Plotting Polygons",
    "section": "71.2 Exploring Spatial Data",
    "text": "71.2 Exploring Spatial Data\nThe data slot in shp holds a data frame like we are used to working with. However, since it is stored inside a SpatialPolygonsDataFrame, we access the data frame a little differently using the @ operator.\n\nglimpse(shp@data)\n\n\nObservations: 808 Variables: 2 $ GEOID10  27925, 28754, 28092… $ ALAND10  624688620, 223734670, 317180853 …\n\nOur data frame has 808 observations of two variables:\nGEOID10: the zip code of each polygon ALAND10: the area (square meters) of each polygon"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-1",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-1",
    "title": "71  Plotting Polygons",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the glimpse() function to look at the data slot of shp.\nPrint the class of the data slot of shp.\nPrint the GEOID10 variable.\n\n\n\nE2.R\n\n# Glimpse the data slot of shp\nglimpse(shp@data)\n\n\n# Print the class of the data slot of shp\nclass(shp@data)\n\n\n# Print GEOID10\nshp@data$GEOID10"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#joining-spatial-data",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#joining-spatial-data",
    "title": "71  Plotting Polygons",
    "section": "71.3 Joining Spatial Data",
    "text": "71.3 Joining Spatial Data\nWe can join data onto the data frame stored in the data slot of our SpatialPolygonsDataFrame. In this chapter, we are interested in the mean income at the zip code level as reported by the IRS. Once we have the income data joined onto the information in the data slot of shp we can map the mean income of zip codes on our leaflet map.\nA data frame called nc_income has been loaded for you. Let’s get started by taking a look at the nc_income data. Then we’ll join it onto the information in the data slot of shp and see if there are any zip codes in our data that are missing income information."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-2",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-2",
    "title": "71  Plotting Polygons",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the glimpse() function to look at nc_income.\nSummarize the nc_income data using the summary() function.\nleft_join() the nc_income data onto shp@data.\nUse the summarize() function with across() to print the number of missing values in each variable in shp_nc_income.\n\n\n\nE3.R\n\n# Glimpse the nc_income data\nglimpse(nc_income)\n\n\n\n\n\n# Glimpse the nc_income data\nglimpse(nc_income)\n\n# Summarize the nc_income data\nsummary(nc_income)\n\n\n\n\n\n# Glimpse the nc_income data\nglimpse(nc_income)\n\n# Summarize the nc_income data\nsummary(nc_income)\n\n# Left join nc_income onto shp@data \nshp_nc_income &lt;- shp@data %&gt;% \n                left_join(nc_income, by = c(\"GEOID10\" = \"zipcode\"))\n\n\n\n\n\n# Glimpse the nc_income data\nglimpse(nc_income)\n\n# Summarize the nc_income data\nsummary(nc_income)\n\n# Left join nc_income onto shp@data and store in shp_nc_income\nshp_nc_income &lt;- shp@data %&gt;% \n                left_join(nc_income, by = c(\"GEOID10\" = \"zipcode\"))\n\n# Print the number of missing values of each variable in shp_nc_income\nshp_nc_income  %&gt;%\n  summarize(across(everything(), ~sum(is.na(.x))))"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#addpolygons-function",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#addpolygons-function",
    "title": "71  Plotting Polygons",
    "section": "71.4 addPolygons() Function",
    "text": "71.4 addPolygons() Function\nLet’s look at those zip codes with missing data to hypothesize why they do not have income data.\nWe are mapping ZCTAs (not actual zip codes) so not every part of NC will have a boundary. Our boundaries may overlap because the file was simplified to reduce size. These are trade offs to consider when mapping polygons.\nEnough nuance, let’s make a map. You can pipe the shp data directly into our calls to leaflet(), addTiles(), and addPolygons() without supplying any additional arguments to map North Carolina’s zip codes. To get you started, the shp SpatialPolygonsDataFrame including the IRS income variables has been loaded for you."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-3",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-3",
    "title": "71  Plotting Polygons",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nMap the polygons in shp using addPolygons().\nCreate a new SpatialPolygonsDataFrame called shp_na that contains information on zip codes with missing income data.\nMap the polygons in shp_na using addPolygons().\n\n\n\nE4.R\n\n# map the polygons in shp\nshp %&gt;% \n    leaflet() %&gt;% \n    addTiles() %&gt;% \n    addPolygons()\n    \n\n\n\n# map the polygons in shp\nshp %&gt;% \n    leaflet() %&gt;% \n    addTiles() %&gt;% \n    addPolygons()\n\n# which zips were not in the income data?\nshp_na &lt;- shp[is.na(shp$mean_income),]\n\n\n\n\n# map the polygons in shp\nshp %&gt;% \n    leaflet() %&gt;% \n    addTiles() %&gt;% \n    addPolygons()\n\n# which zips were not in the income data?\nshp_na &lt;- shp[is.na(shp$mean_income),]\n\n# map the polygons in shp_na\nshp_na %&gt;% \n    leaflet() %&gt;% \n    addTiles() %&gt;% \n    addPolygons()"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#nc-high-income-zips",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#nc-high-income-zips",
    "title": "71  Plotting Polygons",
    "section": "71.5 NC High Income Zips",
    "text": "71.5 NC High Income Zips\nDid you have a hypothesis of why certain zip codes are missing income information? It looks to me like many of them are areas that likely have low populations (e.g., parks, colleges, etc.) and the IRS only reports income data on zip codes with more than 100 filers.\nNow let’s focus in on a subset of zip codes with income data, namely the 25% of zip codes in NC with the highest mean incomes. Where do think these will fall within the states?\nLet’s take a look and find out."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-4",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-4",
    "title": "71  Plotting Polygons",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nSummarize the mean income variable to find the cut point for the top quartile of mean income.\nCreate a subset called high_inc that includes only zip codes in the top quartile of mean income.\nMap the boundaries of the zip codes in the top quartile of mean income.\n\n\n\nE5.R\n\n# summarize the mean income variable\nsummary(shp$mean_income)\n\n\n\n# summarize the mean income variable\nsummary(shp$mean_income)\n\n# subset shp to include only zip codes in the top quartile of mean income\nhigh_inc &lt;- shp[!is.na(shp$mean_income) & shp$mean_income &gt; 55917,]\n\n\n\n\n# summarize the mean income variable\nsummary(shp$mean_income)\n\n# subset shp to include only zip codes in the top quartile of mean income\nhigh_inc &lt;- shp[!is.na(shp$mean_income) & shp$mean_income &gt; 55917,]\n\n# map the boundaries of the zip codes in the top quartile of mean income\nhigh_inc %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons()"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#addpolygon-options",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#addpolygon-options",
    "title": "71  Plotting Polygons",
    "section": "71.6 addPolygon() Options",
    "text": "71.6 addPolygon() Options\nSo far we have used the default appearance for addPolygons(). There are several more ways to customize the polygons.\nThe arguments to addPolygons() we will focus on are:\n\nweight: the thickness of the boundary lines in pixels\ncolor: the color of the polygons\nlabel: the information to appear on hover\nhighlightOptions: options to highlight a polygon on hover\n\n\naddPolygons(weight = 2, color = “red”, label = ~paste0(“Total Income:”, dollar(income)), highlight = highlightOptions(weight = 10, color = “blue”, bringToFront = TRUE))\n\nThe high_inc SpatialPolygonsDataFrame you created in the previous exercise has been loaded for you."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-5",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-5",
    "title": "71  Plotting Polygons",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse the arguments of addPolygons() to map the high income zip codes in NC with:\n\nA boundary thickness of 1 pixel,\nPolygons that are colored with the nc_pal palette and are highlighted on hover, and\nLabels that display the words “Mean Income:” followed by the mean income of the zip code.\n\n\n\n\nE6.R\n\n# create color palette with colorNumeric()\nnc_pal &lt;- colorNumeric(\"YlGn\", domain = high_inc@data$mean_income)\n\nhigh_inc %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  # set boundary thickness to 1 and color polygons\n  addPolygons(___ = ___, ___ = ~nc_pal(mean_income),\n              # add labels that display mean income\n              label = ___(\"Mean Income: \", dollar(mean_income)),\n              # highlight polygons on hover\n              ___ = highlightOptions(weight = 5, color = \"white\",\n              bringToFront = TRUE))"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#lets-do-some-logging",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#lets-do-some-logging",
    "title": "71  Plotting Polygons",
    "section": "71.7 Let’s do some Logging",
    "text": "71.7 Let’s do some Logging\nLet’s look at the quartiles of mean zip code incomes in NC.\n\nsummary(high_inc$mean_income)\n\n\nMin. 1st Qu. Median Mean 3rd Qu. Max. 55947 62380 71655 83682 90695 550849\n\nThe range is nearly $500,000! However, the median is much closer to the min than to the max, indicating a right-skew. Since the mean income variable contains exceptionally large values, the continuous color gradient is not very helpful. Log transforming a right-skewed variable pulls large values closer to the mean and yields a more symmetrically distributed variable.\nLog transforming the mean income on our map increases the variation in the color gradient across the high income zip codes and enables better visualization of the distribution of mean income across the state."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-6",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-6",
    "title": "71  Plotting Polygons",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a logged version of the nc_pal color palette.\nApply the logged color palette to the leaflet map.\nComment out the map tile from the leaflet map to more easily visualize the variation in mean income (add # without a space prior to the function that adds the map tile).\n\n\n\nE7.R\n\n# Use the log function to create a new version of nc_pal\nnc_pal &lt;- colorNumeric(\"YlGn\", domain = log(high_inc@data$mean_income))\n\n# comment out the map tile\nhigh_inc %&gt;%\n  leaflet() %&gt;%\n  #___addProviderTiles(\"CartoDB\") %&gt;%\n  # apply the new nc_pal to the map\n  addPolygons(weight = 1, color = ~nc_pal(log(mean_income)), fillOpacity = 1,\n              label = ~paste0(\"Mean Income: \", dollar(mean_income)),\n              highlightOptions = highlightOptions(weight = 5, color = \"white\", bringToFront = TRUE))"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#wealthiest-zip-codes-in-america",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#wealthiest-zip-codes-in-america",
    "title": "71  Plotting Polygons",
    "section": "71.8 Wealthiest Zip Codes in America",
    "text": "71.8 Wealthiest Zip Codes in America\nZooming out from North Carolina, let’s put together a polygon layer for the entire country. Then we will work to add this layer to our map of American colleges.\nAccording to the 2015 IRS data, there are 427 zip codes in America with a mean income of $ 200,000 or more. It might be interesting to know which colleges are located in these affluent areas. Let’s map these 427 polygons and explore our leaflet map to better understand where the highest income zip codes in America are. To get us started, a SpatialPolygonsDataFrame called wealthy_zips that contains information on each zip code in America with a mean income of $ 200,000 or more has been loaded for you.\nBefore we work with this data, take a minute to hypothesize where many of these zip codes are located."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-7",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-7",
    "title": "71  Plotting Polygons",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCall the slotNames function to print the slot names of wealthy_zips.\nPrint a summary of the mean_income variable.\nAdd the boundaries from wealthy_zips to a leaflet map using the addPolygons() function and customize the polygons so they are green and are included in a group called “Wealthy Zipcodes”.\n\n\n\nE8.R\n\n# Print the slot names of `wealthy_zips`\nslotNames(wealthy_zips)\n\n\n\n\n\n# Print a summary of the `mean_income` variable\nsummary(wealthy_zips@data$mean_income)\n\n\n\n# plot zip codes with mean incomes &gt;= $200k\nwealthy_zips %&gt;% \n  leaflet() %&gt;% \n  addProviderTiles(\"CartoDB\") %&gt;% \n  # set color to green and create Wealth Zipcodes group\n  addPolygons(weight = 1, fillOpacity = .7, color = \"green\",  group = \"Wealthy Zipcodes\", \n              label = ~paste0(\"Mean Income: \", dollar(mean_income)),\n              highlightOptions = highlightOptions(weight = 5, color = \"white\", bringToFront = TRUE))"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#final-map",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#final-map",
    "title": "71  Plotting Polygons",
    "section": "71.9 Final Map",
    "text": "71.9 Final Map\nBecause our leaflet map is built in layers, we can add different types of information to the same base map (e.g., points and polygons). When adding new layers based on different data to an existing leaflet object we must specify the data argument with the function that creates the new layer to override the data that is piped through the chain of functions.\n\nm4 %&gt;% addCircleMarkers(data = private)\n\nLet’s combine our point and polygon maps to add a layer highlighting America’s wealthiest zip codes to our map of every college in America. To get us started, the last layered version of the college map m4 has been printed for you and the wealthy_zips SpatialPolygonsDataFrame is pre-loaded.\nAfter you create the final map, take a few minutes to explore (and enjoy) your leaflet map!"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-8",
    "href": "Interactive_Maps_with_leaflet_in_R_C4.html#instructions-100-xp-8",
    "title": "71  Plotting Polygons",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd a layer of polygons to m4 using the information in wealthy_zips and update the layer controls to include the “Wealthy Zip Codes” group.\nHave fun exploring the final map of the course!\n\n\n\nE9.R\n\n# Add polygons using wealthy_zips\nfinal_map &lt;- m4 %&gt;% \n   addPolygons(data = wealthy_zips, weight = 1, fillOpacity = .5, color = \"Grey\",  group = \"Wealthy Zip Codes\", \n              label = ~paste0(\"Mean Income: \", dollar(mean_income)),\n              highlight = highlightOptions(weight = 5, color = \"white\", bringToFront = TRUE)) %&gt;% \n    # Update layer controls including \"Wealthy Zip Codes\"\n    addLayersControl(baseGroups = c(\"OSM\", \"Carto\", \"Esri\"), \n                         overlayGroups = c(\"Public\", \"Private\", \"For-Profit\", \"Wealthy Zip Codes\"))\n\n\n\n\n\n# Add polygons using wealthy_zips\nfinal_map &lt;- m4 %&gt;% \n   addPolygons(data = wealthy_zips, weight = 1, fillOpacity = .5, color = \"Grey\",  group = \"Wealthy Zip Codes\", \n              label = ~paste0(\"Mean Income: \", dollar(mean_income)),\n              highlightOptions = highlightOptions(weight = 5, color = \"white\", bringToFront = TRUE)) %&gt;% \n    # Update layer controls including \"Wealthy Zip Codes\"\n    addLayersControl(baseGroups = c(\"OSM\", \"Carto\", \"Esri\"), \n                         overlayGroups = c(\"Public\", \"Private\", \"For-Profit\", \"Wealthy Zip Codes\"))     \n\n# Print and explore your very last map of the course!\nfinal_map"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#cleaning-up-the-base-map",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#cleaning-up-the-base-map",
    "title": "69  Plotting Points",
    "section": "69.1 Cleaning up the Base Map",
    "text": "69.1 Cleaning up the Base Map\nIf you are storing leaflet maps in objects, there will come a time when you need to remove markers or reset the view. You can accomplish these tasks with the following functions.\n\nclearMarkers()- Remove one or more features from a map clearBounds()- Clear bounds and automatically determine bounds based on map elements\n\nTo remove the markers and to reset the bounds of our m map we would:\n\nm &lt;- m %&gt;% addMarkers(lng = dc_hq\\(lon, lat = dc_hq\\)lat) %&gt;% setView(lat = 50.9, lng = 4.7, zoom = 5)\n\n\nm %&gt;% clearMarkers() %&gt;% clearBounds()\n\nThe leaflet map of DataCamp’s headquarters has been printed for you."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp",
    "title": "69  Plotting Points",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nUse clearMarkers() to remove markers and clearBounds() to restore the default view and store in the map_clear object.\nPrint map_clear\n\n\n\nE1.R\n\n# Remove markers, reset bounds, and store the updated map in the m object\nmap_clear &lt;- map  %&gt;%\n        clearMarkers()  %&gt;% \n        clearBounds()\n        \n\n\n\n# Remove markers, reset bounds, and store the updated map in the m object\nmap_clear &lt;- map %&gt;%\n        clearMarkers() %&gt;% \n        clearBounds()\n\n# Print the cleared map\nprint(map_clear)"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#exploring-the-ipeds-data-ii",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#exploring-the-ipeds-data-ii",
    "title": "69  Plotting Points",
    "section": "69.2 Exploring the IPEDS Data II",
    "text": "69.2 Exploring the IPEDS Data II\nMost analyses require data wrangling. Luckily, there are many functions in the tidyverse that facilitate data frame cleaning. For example, the drop_na() function will remove observations with missing values. By default, drop_na() will check all columns for missing values and will remove all observations with one or more missing values.\n\nmiss_ex &lt;- tibble( animal = c(“dog”, “cat”, “rat”, NA), name = c(“Woodruf”, “Stryker”, NA, “Morris”), age = c(1:4)) miss_ex\n\n\nmiss_ex %&gt;% drop_na() %&gt;% arrange(desc(age))\n\n\n70 A tibble: 2 x 3\nanimal name age    1 cat Stryker 2 2 dog Woodruf 1"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-1",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-1",
    "title": "69  Plotting Points",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRemove colleges with missing sector information using drop_na().\nCount the number of four-year colleges in each state.\nPrint the US states in descending order by the number of colleges in each state.\n\n\n\nE2.R\n\n# Remove colleges with missing sector information\nipeds &lt;- \n    ipeds_missing %&gt;% \n    drop_na()\n\n\n\n\n# Count the number of four-year colleges in each state\nipeds %&gt;% \n    group_by(state) %&gt;% \n    count()\n    \n    \n\n\n\n# Create a list of US States in descending order by the number of colleges in each state\nipeds  %&gt;% \n    group_by(state)  %&gt;% \n    count()  %&gt;% \n    arrange(desc(n))"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#california-colleges",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#california-colleges",
    "title": "69  Plotting Points",
    "section": "70.1 California Colleges",
    "text": "70.1 California Colleges\nNow it is your turn to map all of the colleges in a state. In this exercise, we’ll apply our example of mapping Maine’s colleges to California’s colleges. The first step is to set up your data by filtering the ipeds data frame to include only colleges in California. For reference, you will find how we accomplished this with the colleges in Maine below.\n\nmaine_colleges &lt;- ipeds %&gt;% filter(state == “ME”)\n\n\nmaine_colleges\n\n\n71 A tibble: 21 x 5\n                 name       lng      lat state sector_label\n                &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;\n1 Bates College -70.20333 44.10530 ME Private 2 Bowdoin College -69.96524 43.90690 ME Private"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-2",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-2",
    "title": "69  Plotting Points",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCreate a data frame called ca with data on only colleges in California.\nUse addMarkers() to plot all of the colleges in ca on map.\n\n\n\nE3.R\n\n# Create a dataframe called `ca` with data on only colleges in California\nca &lt;- ipeds %&gt;%\n        filter(state == \"CA\")\n\n# Use `addMarkers` to plot all of the colleges in `ca` on the `m` leaflet map\nmap %&gt;%\n    addMarkers(lng = ca$lng, lat = ca$lat)"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#the-city-of-colleges",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#the-city-of-colleges",
    "title": "69  Plotting Points",
    "section": "71.1 The City of Colleges",
    "text": "71.1 The City of Colleges\nBased on our map of California colleges it appears that there is a cluster of colleges in and around the City of Angels (e.g., Los Angeles). Let’s take a closer look at these institutions on our leaflet map.\nThe coordinates for the center of LA are provided for you in the la_coords data frame.\nla_coords &lt;- data.frame(lat = 34.05223, lon = -118.2437) Once you create a map focused on LA, try panning and zooming the map. Can you find the cluster of colleges East of LA known as the Claremont Colleges?\nWhen there are hundreds of markers, do you find the pin markers helpful or do they get in your way?\nThe coordinates of LA have been provided in the la_coords data frame and the ca data frame of California colleges and the map have been loaded for you."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-3",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-3",
    "title": "69  Plotting Points",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCenter the map on LA.\nSet the zoom level to 8 and store the map in the map_zoom object.\nPrint the map stored in the map_zoom object.\n\n\n\nE4.R\n\n# Center the map on LA \nmap %&gt;% \n   addMarkers(data = ca) %&gt;% \n    setView(lat = la_coords$lat, lng = la_coords$lon, zoom = 12)\n\n\n\n# Set the zoom level to 8 and store in the map_zoom object\nmap_zoom &lt;- \n    map %&gt;% \n      addMarkers(data = ca) %&gt;% \n      setView(lat = la_coords$lat, lng = la_coords$lon, zoom = 8)\n      \nmap_zoom"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#circle-markers",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#circle-markers",
    "title": "69  Plotting Points",
    "section": "71.2 Circle Markers",
    "text": "71.2 Circle Markers\nCircle markers are notably different from pin markers:\nWe can control their size They do not “stand-up” on the map We can more easily change their color There are many ways to customize circle markers and the design of your leaflet map. To get started we will focus on the following arguments.\naddCircleMarkers(map, lng = NULL, lat = NULL, radius = 10, color = “#03F”, popup = NULL) The first argument map takes a leaflet object, which we will pipe directly into addCircleMarkers(). lng and lat are the coordinates we are mapping. The other arguments can customize the appearance and information presented by each marker.\nThe ca data frame and the leaflet object map have been loaded for you."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-4",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-4",
    "title": "69  Plotting Points",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nClear the pin markers from the map.\nUse addCircleMarkers() to plot each college as a circle.\nChange the radius of each circle to be 2 pixels and the color to red.\n\n\n\nE5.R\n\n# Clear the markers from the map \nmap2 &lt;- map %&gt;% \n            clearMarkers()\n\n\n\n# Use addCircleMarkers() to plot each college as a circle\nmap2 %&gt;%\n    addCircleMarkers(lng = ca$lng, lat = ca$lat)\n    \n\n\n\n# Change the radius of each circle to be 2 pixels and the color to red\nmap2 %&gt;% \n    addCircleMarkers(lng = ca$lng, lat = ca$lat,\n                     radius = 2, color = \"red\")"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#making-our-map-pop",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#making-our-map-pop",
    "title": "69  Plotting Points",
    "section": "71.3 Making our Map Pop",
    "text": "71.3 Making our Map Pop\nSimilar to building a plot with ggplot2 or manipulating data with dplyr, your map needs to be stored in an object if you reference it later in your code.\nSpeaking of dplyr, the %&gt;% operator can pipe data into the function chain that creates a leaflet map.\n\nipeds %&gt;% leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers(popup = ~name, color = “#FF0000”)\n\nPiping makes our code more readable and allows us to refer to variables using the ~ operator rather than repeatedly specifying the data frame.\nThe color argument in addCircleMarkers() takes the name of a color or a hex code. For example, red or #FF0000.\nmap has been printed for you. Notice the circle markers are gone!"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-5",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-5",
    "title": "69  Plotting Points",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd circle markers with popups for college names.\nChange circle markers’ color to “#2cb42c” (this case exactly!) and store map in the map_color object.\nPrint map_color\n\n\n\nE6.R\n\n# Add circle markers with popups for college names\nmap %&gt;% \n    addCircleMarkers(data = ca, radius = 2, popup = ~name)\n    \n\n\n\n\n# Add circle markers with popups for college names\nmap %&gt;%\n    addCircleMarkers(data = ca, radius = 2, popup = ~name)\n\n# Change circle markers' color to #2cb42c and store map in map_color object\nmap_color &lt;- map %&gt;% \n    addCircleMarkers(data = ca, radius = 2, color = \"#2cb42c\", popup = ~name)\n    \n\n\n\n# Add circle markers with popups for college names\nmap %&gt;%\n    addCircleMarkers(data = ca, radius = 2, popup = ~name)\n\n# Change circle color to #2cb42c and store map in map_color object\nmap_color &lt;- map %&gt;% \n    addCircleMarkers(data = ca, radius = 2, color = \"#2cb42c\", popup = ~name)\n\n# Print map_color\nprint(map_color)"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#building-a-better-pop-up",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#building-a-better-pop-up",
    "title": "69  Plotting Points",
    "section": "71.4 Building a Better Pop-up",
    "text": "71.4 Building a Better Pop-up\nWith the paste0() function and a few html tags, we can customize our popups. paste0() converts its arguments to characters and combines them into a single string without separating the arguments.\n\naddCircleMarkers(popup = ~paste0(name, “”, sector_label))\n\nWe can use the  tag to create a line break to have each element appear on a separate line.\nTo distinguish different data elements, we can make the name of each college italics by wrapping the name variable in \n\naddCircleMarkers(popup = ~paste0(“”, name, “”, “”, sector_label))"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-6",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-6",
    "title": "69  Plotting Points",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nClear the bounds and markers on the map stored in map.\nAdd circle markers with pop-ups that display both the institution name and sector.\nMake the institution name in each pop-up bold.\n\n\n\nE7.R\n\n# Clear the bounds and markers on the map object and store in map2\nmap2 &lt;- map %&gt;% \n            clearMarkers() %&gt;% \n            clearBounds()\n\n\n\n\n\n# Add circle markers with popups that display both the institution name and sector\nmap2 %&gt;% \n    addCircleMarkers(data = ca, radius = 2, \n                     popup = ~paste0(name, \"&lt;br/&gt;\", sector_label))\n\n\n\n\n\n# Make the institution name in each popup bold\nmap2 %&gt;% \n    addCircleMarkers(data = ca, radius = 2, \n                     popup = ~paste0(\"&lt;b&gt;\", name, \"&lt;/b&gt;\", \"&lt;br/&gt;\", sector_label))"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#swapping-popups-for-labels",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#swapping-popups-for-labels",
    "title": "69  Plotting Points",
    "section": "71.5 Swapping Popups for Labels",
    "text": "71.5 Swapping Popups for Labels\nPopups are great, but they require a little extra effort. That is when labels come to our aid. Using the label argument in the addCircleMarkers() function we can get more information about one of our markers with a simple hover!\nipeds %&gt;% leaflet() %&gt;% addProviderTiles(“CartoDB”) %&gt;% addCircleMarkers(label = ~name, radius = 2) Labels are especially helpful when mapping more than a few locations as they provide quick access to detail about what each marker represents."
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-7",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-7",
    "title": "69  Plotting Points",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd circle markers with labels identifying the name of each college to the current map.\nAdd sector information to the label inside parentheses.\n\n\n\nE8.R\n\n# Add circle markers with labels identifying the name of each college\nmap %&gt;% \n    addCircleMarkers(data = ca, radius = 2, label = ~name)\n\n\n\n# Use paste0 to add sector information to the label inside parentheses \nmap %&gt;% \n    addCircleMarkers(data = ca, radius = 2, label = ~paste0(name, \" (\", sector_label, \")\"))"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#creating-a-color-palette-using-colorfactor",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#creating-a-color-palette-using-colorfactor",
    "title": "69  Plotting Points",
    "section": "71.6 Creating a Color Palette using colorFactor",
    "text": "71.6 Creating a Color Palette using colorFactor\nSo far we have only used color to customize the style of our map. With colorFactor() we can create a color palette that maps colors the levels of a factor variable.\n\npal &lt;- colorFactor(palette = c(“blue”, “red”, “green”), levels = c(“Public”, “Private”, “For-Profit”))\n\n\nm %&gt;% addCircleMarkers(color = ~pal(sector_label))\n\nWhy might we not want to use this particular color palette?\nIf you are interested in using a continuous variable to color a map see colorNumeric().\n\npal &lt;- colorNumeric(palette = “RdBu”, domain = c(25:50))\n\n\nipeds %&gt;% leaflet() %&gt;% addProviderTiles(“CartoDB”) %&gt;% addCircleMarkers(radius = 2, color = ~pal(lat))"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-8",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-8",
    "title": "69  Plotting Points",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nMake a color palette called pal for the values of sector_label using colorFactor() using “red”, blue”, and “#9b4a11”.\nAdd circle markers that color colleges using pal() and the values of sector_label.\n\n\n\nE9.R\n\n# Make a color palette called pal for the values of `sector_label` using `colorFactor()`  \n# Colors should be: \"red\", \"blue\", and \"#9b4a11\" for \"Public\", \"Private\", and \"For-Profit\" colleges, respectively\npal &lt;- colorFactor(palette = c(\"red\", \"blue\", \"#9b4a11\"), \n                   levels = c(\"Public\", \"Private\", \"For-Profit\"))\n\n# Add circle markers that color colleges using pal() and the values of sector_label\nmap2 &lt;- \n    map %&gt;% \n        addCircleMarkers(data = ca, radius = 2, \n                         color = ~pal(sector_label), \n                         label = ~paste0(name, \" (\", sector_label, \")\"))\n\n# Print map2\nmap2"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#a-legendary-map",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#a-legendary-map",
    "title": "69  Plotting Points",
    "section": "71.7 A Legendary Map",
    "text": "71.7 A Legendary Map\nAdding information to our map using color is great, but it is only helpful if we remember what the colors represent. With addLegend() we can add a legend to remind us.\nThere are several arguments we can use to custom the legend to our liking, including opacity, title, and position. To create a legend for our colorNumeric() example, we would do the following.\n\npal &lt;- colorNumeric(palette = “RdBu”, domain = c(25:50))\n\n\nipeds %&gt;% leaflet() %&gt;% addProviderTiles(“CartoDB”) %&gt;% addCircleMarkers(radius = 2, color = ~pal(lat)) %&gt;% addLegend(pal = pal, values = c(25:50), opacity = 0.75, title = “Latitude”, position = “topleft”)"
  },
  {
    "objectID": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-9",
    "href": "Interactive_Maps_with_leaflet_in_R_C2.html#instructions-100-xp-9",
    "title": "69  Plotting Points",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nAdd a legend that displays the colors used in pal to m.\nCustomize the legend to have an opacity of .5, a title of “Sector”, and a position of “topright”.\n\n\n\nE10.R\n\n# Add a legend that displays the colors used in pal\nm %&gt;% \n    addLegend(pal = pal, \n              values = c(\"Public\", \"Private\", \"For-Profit\"))\n\n\n\n\n# Customize the legend\nm %&gt;% \n    addLegend(pal = pal, \n              values = c(\"Public\", \"Private\", \"For-Profit\"),\n              # opacity of .5, title of Sector, and position of topright\n              opacity  = 0.5, title = \"Sector\", position = \"topright\")"
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp",
    "href": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp",
    "title": "67  Confidence intervals",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCompute the sample proportion for each of the 1000 original samples, assigning to ex1_props.\n\nGroup by poll.\nSummarize to calculate stat as the mean() of cases of vote equalling “yes”.\n\nSelect one poll from which to resample, as one_poll.\n\nFilter for the first poll. That is, when poll equals 1.\nSelect the vote column.\n\nCompute p-hat for each resampled poll, as ex2_props.\n\nSpecify the response as vote. The success value is “yes”.\nGenerate 1000 replicates of type “bootstrap”.\nCalculate the “prop”ortion summary statistic.\n\nNote that because you are looking for an interval estimate, you have not made a hypothesis claim about the proportion (thus, there is no hypothesize step needed in the infer pipeline).\nUsing ex1_props, calculate the variability of p-hat In the call to summarize(), set variability to the standard deviation, sd(), of stat.\nDo the same with ex2_props to calculate the variability of p-hat. A- gain, because you are looking for an interval estimate, you have not made a hypothesis claim about the proportion (thus, there is no hypothesize step needed in the infer pipeline).\n\n\n\nE1.R\n\n# Compute p-hat for each poll\nex1_props &lt;- all_polls %&gt;% \n  # Group by poll\n  group_by(poll) %&gt;% \n  # Calculate proportion of yes votes\n  summarize(stat = mean(vote == \"yes\"))\n  \n# Review the result\nex1_props\n\n\n\n\n# Select one poll from which to resample\none_poll &lt;- all_polls %&gt;%\n  # Filter for the first poll\n  filter(poll == 1) %&gt;%\n  # Select vote\n  select(vote)\n  \n# Compute p-hat* for each resampled poll\nex2_props &lt;- one_poll %&gt;%\n  # Specify vote as the response, where yes means success\n  specify(response = vote, success = \"yes\") %&gt;%\n  # Generate 1000 reps of type bootstrap\n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  # Calculate the summary stat \"prop\"\n  calculate(stat = \"prop\")\n\n\n\n\n# From previous steps\nex1_props &lt;- all_polls %&gt;% \n  group_by(poll) %&gt;% \n  summarize(stat = mean(vote == \"yes\"))\nex2_props &lt;- all_polls %&gt;%\n  filter(poll == 1) %&gt;%\n  select(vote) %&gt;%\n  specify(response = vote, success = \"yes\") %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  calculate(stat = \"prop\")\n  \n# Calculate variability of p-hat\nex1_props %&gt;% \n  summarize(variability = sd(stat))\n  \n# Calculate variability of p-hat*\nex2_props %&gt;% \n  summarize(variability = sd(stat))"
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#visualizing-the-variability-of-p-hat",
    "href": "Foundations_of_Inference_in_R_C4.html#visualizing-the-variability-of-p-hat",
    "title": "67  Confidence intervals",
    "section": "67.1 Visualizing the variability of p-hat",
    "text": "67.1 Visualizing the variability of p-hat\nIn order to compare the variability of the sampled and values in the previous exercises, it is valuable to visualize their distributions. To recall, the exercises walked through two different experiments for investigating the variability of and :\n\nExperiment 1: Sample ( n=30 ) repeatedly from an extremely large population (gold standard, but unrealistic)\nExperiment 2: Resample ( n=30 ) repeatedly with replacement from a single sample of size 30"
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-1",
    "href": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-1",
    "title": "67  Confidence intervals",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCombine data from both experiments by calling bind_rows(), passing ex1_props and ex2_props. Call this both_ex_props. A dataset ID column named experiment will be created.\nUsing both_ex_props, plot stat colored by experiment.\nAdd a density curve using geom_density(), setting the bandwidth argument, bw, to 0.1.\n\n\n\nE2.R\n\n# Combine data from both experiments\nboth_ex_props &lt;- bind_rows(ex1_props, ex2_props, .id = \"experiment\")\n\n# Using both_ex_props, plot stat colored by experiment\nggplot(both_ex_props, aes(stat, color = experiment)) + \n  # Add a density layer with bandwidth 0.1\n  geom_density(bw = 0.1)"
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#empirical-rule",
    "href": "Foundations_of_Inference_in_R_C4.html#empirical-rule",
    "title": "67  Confidence intervals",
    "section": "67.2 Empirical Rule",
    "text": "67.2 Empirical Rule\nMany statistics we use in data analysis (including both the sample average and sample proportion) have nice properties that are used to better understand the population parameter(s) of interest.\nOne such property is that if the variability of the sample proportion (called the standard error, or ) is known, then approximately 95% of values (from different samples) will be within of the true population proportion.\nTo check whether that holds in the situation at hand, let’s go back to the polls generated by taking many samples from the same population.\nThe all_polls dataset contains 1000 samples of size 30 from a population with a probability of voting for Candidate X equal to 0.6.\nNote that you will use the R function sd() which calculates the variability of any set of numbers. In statistics, when sd() is applied to a variable (e.g., price of house) we call it the standard deviation. When sd() is applied to a statistic (e.g., set of sample proportions) we call it the standard error."
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-2",
    "href": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-2",
    "title": "67  Confidence intervals",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun the code to generate props, the proportion of individuals who are planning to vote yes in each poll. This is based upon ex1_props from previous exercises.\nAdd a column, is_in_conf_int that is TRUE when the sampled proportion of yes votes is less than 2 standard errors away from the true population proportion of yes votes. That is, the abs()solute difference between prop_yes and true_prop_yes is less than twice sd() of prop_yes.\nCalculate the proportion of sample statistics in the confidence interval, prop_in_conf_int, by taking the mean() of is_in_conf_int.\n\n\n\nE3.R\n\n# Proportion of yes votes by poll\nprops &lt;- all_polls %&gt;% \n  group_by(poll) %&gt;% \n  summarize(prop_yes = mean(vote == \"yes\"))\n\n# The true population proportion of yes votes\ntrue_prop_yes &lt;- 0.6\n\n# Proportion of polls within 2SE\nprops %&gt;%\n  # Add column: is prop_yes in 2SE of 0.6\n  mutate(is_in_conf_int = abs(prop_yes - true_prop_yes) &lt; 2 * sd(prop_yes)) %&gt;%\n  # Calculate  proportion in conf int\n  summarize(prop_in_conf_int = mean(is_in_conf_int))"
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#bootstrap-t-confidence-interval",
    "href": "Foundations_of_Inference_in_R_C4.html#bootstrap-t-confidence-interval",
    "title": "67  Confidence intervals",
    "section": "67.3 Bootstrap t-confidence interval",
    "text": "67.3 Bootstrap t-confidence interval\nThe previous exercises told you two things:\n\nYou can measure the variability associated with p-hat by resampling from the original sample.\nOnce you know the variability of p-hat, you can use it as a way to measure how far away the true proportion is.\n\nNote that the rate of closeness (here 95%) refers to how often a sample is chosen so that it is close to the population parameter. You won’t ever know if a particular dataset is close to the parameter or far from it, but you do know that over your lifetime, 95% of the samples you collect should give you estimates that are within of the true population parameter.\nThe votes from a single poll, one_poll, and the data from 1000 bootstrap resamples, one_poll_boot are available in your workspace. These are based on Experiment 2 from earlier in the chapter.\nAs in the previous exercise, when discussing the variability of a statistic, the number is referred to as the standard error."
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-3",
    "href": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-3",
    "title": "67  Confidence intervals",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate and assign the result to p_hat. In the call to summarize(), calculate stat as the mean of vote equalling “yes”.\nFind an interval of values that are plausible for the true parameter by calculating .\n\nThe lower bound of the confidence interval is p_hat minus twice the standard error of stat. Use sd() to calculate the standard error.\nThe upper bound is p_hat plus twice the standard error of stat.\n\n\n\n\nE4.R\n\n# From previous exercises\none_poll &lt;- all_polls %&gt;%\n  filter(poll == 1) %&gt;%\n  select(vote)\none_poll_boot &lt;- one_poll %&gt;%\n  specify(response = vote, success = \"yes\") %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  calculate(stat = \"prop\")\n  \np_hat &lt;- one_poll %&gt;%\n  # Calculate proportion of yes votes\n  summarize(stat = mean(vote == \"yes\")) %&gt;%\n  pull()\n\n# Create an interval of plausible values\none_poll_boot %&gt;%\n  summarize(\n    # Lower bound is p_hat minus 2 std errs\n    lower = p_hat - 2 * sd(stat),\n    # Upper bound is p_hat plus 2 std errs\n    upper = p_hat + 2 * sd(stat)\n  )"
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#bootstrap-percentile-interval",
    "href": "Foundations_of_Inference_in_R_C4.html#bootstrap-percentile-interval",
    "title": "67  Confidence intervals",
    "section": "67.4 Bootstrap percentile interval",
    "text": "67.4 Bootstrap percentile interval\nThe main idea in the previous exercise was that the distance between the original sample and the resampled (or bootstrapped) values gives a measure for how far the original is from the true population proportion.\nThe same variability can be measured through a different mechanism. As before, if is sufficiently close to the true parameter, then the resampled (bootstrapped) values will vary in such a way that they overlap with the true parameter.\nInstead of using as a way to measure the middle 95% of the sampled values, you can find the middle of the resampled values by removing the upper and lower 2.5%. Note that this second method of constructing bootstrap intervals also gives an intuitive way for making 90% or 99% confidence intervals as well as 95% intervals.\nThe bootstrapped resamples, one_poll_boot, and the proportion of yes votes, p_hat are available in your workspace."
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-4",
    "href": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-4",
    "title": "67  Confidence intervals",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nRun the code to remind yourself of the t-interval from the previous exercise. Calculate the 95 percent interval of the bootstrapped p-hat values contained in one_poll_boot. Summarize to calculate the lower end at the 2.5% quantile of stat by setting p to 0.025. Calculate the upper end in a similar way.\nPerform the same calculation using infer’s convenience function, get_confidence_interval(). For the interval, use level = 0.95, and call the output percentile_ci.\nvisualize() the distribution of bootstrapped proportions with the middle 95 percent highlighted. Set the endpoints of the highlighted region to percentile_ci. Set direction to “between” to highlight in-between those endpoints.\n\n\n\nE5.R\n\n# From previous exercise: bootstrap t-confidence interval\none_poll_boot %&gt;%\n  summarize(\n    lower = p_hat - 2 * sd(stat),\n    upper = p_hat + 2 * sd(stat)\n  )\n  \n# Manually calculate a 95% percentile interval\none_poll_boot %&gt;%\n  summarize(\n    lower = quantile(stat, 0.025),\n    upper = quantile(stat, 0.975)\n  )\n\n\n\n\n# From previous step\none_poll_boot %&gt;%\n  summarize(\n    lower = quantile(stat, 0.025),\n    upper = quantile(stat, 0.975)\n  )\n  \n# Calculate the same interval, more conveniently\npercentile_ci &lt;- one_poll_boot %&gt;% \n  get_confidence_interval(p = 0.025)\n  \n# Review the value\npercentile_ci\n\n\n\n\n# From previous step\npercentile_ci &lt;- one_poll_boot %&gt;% \n  get_confidence_interval(level = 0.95)\n  \none_poll_boot %&gt;% \n  # Visualize in-between the endpoints given by percentile_ci\n  visualize(endpoints = percentile_ci, direction = \"between\")"
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#sample-size-effects-on-bootstrap-cis",
    "href": "Foundations_of_Inference_in_R_C4.html#sample-size-effects-on-bootstrap-cis",
    "title": "67  Confidence intervals",
    "section": "67.5 Sample size effects on bootstrap CIs",
    "text": "67.5 Sample size effects on bootstrap CIs\nIn a previous multiple choice exercise, you realized that if you resampled the data with the wrong size (e.g. 300 or 3 instead of 30), the standard error (SE) of the sample proportions was off. With 300 resampled observations, the SE was too small. With 3 resampled observations, the SE was too large.\nHere, you will use the incorrect standard error (based on the incorrect sample size) to create a confidence interval. The idea is that when the standard error is off, the interval is not particularly useful, nor is it correct."
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-5",
    "href": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-5",
    "title": "67  Confidence intervals",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nA function for calculating the bootstrapped t-confidence interval, calc_t_conf_int(), is shown is the script. Read the code and try to understand it.\nCall calc_t_conf_int() on one_poll_boot to calculate the correct t-confidence interval.\nDo the same on one_poll_boot_300, to find an incorrect interval for the resamples of size 300.\nDo the same on one_poll_boot_3, to find an incorrect interval for the resamples of size 3.\n\n\n\nE6.R\n\ncalc_t_conf_int &lt;- function(resampled_dataset) {\n  resampled_dataset %&gt;%\n    summarize(\n      lower = p_hat - 2 * sd(stat),\n      upper = p_hat + 2 * sd(stat)\n    )\n}\n\n# Find the bootstrap t-confidence interval for 30 resamples\ncalc_t_conf_int(one_poll_boot)\n\n# ... and for 300 resamples\ncalc_t_conf_int(one_poll_boot_300)\n\n# ... and for 3 resamples\ncalc_t_conf_int(one_poll_boot_3)"
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#sample-proportion-value-effects-on-bootstrap-cis",
    "href": "Foundations_of_Inference_in_R_C4.html#sample-proportion-value-effects-on-bootstrap-cis",
    "title": "67  Confidence intervals",
    "section": "67.6 Sample proportion value effects on bootstrap CIs",
    "text": "67.6 Sample proportion value effects on bootstrap CIs\nOne additional element that changes the width of the confidence interval is the sample parameter value, p-hat.\nGenerally, when the true parameter is close to 0.5, the standard error of is larger than when the true parameter is closer to 0 or 1. When calculating a bootstrap t-confidence interval, the standard error controls the width of the CI, and here (given a true parameter of 0.8) the sample proportion is higher than in previous exercises, so the width of the confidence interval will be narrower."
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-6",
    "href": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-6",
    "title": "67  Confidence intervals",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\ncalc_p_hat() is shown in the script to calculate the sample proportions. calc_t_conf_int() from the previous exercise has been updated to now use any value of p_hat as an argument. Read their definitions and try to understand them.\nRun the code to calculate the bootstrap t-confidence interval for the original population.\nConsider a new population where the true parameter is 0.8, one_poll_0.8. Calculate of this new sample, using the same technique as with the original dataset. Call it p_hat_0.8.\nFind the bootstrap t-confidence interval using the new bootstrapped data, one_poll_boot_0.8, and the new p-hat. Notice that it is narrower than previously calculated.\n\n\n\nE7.R\n\ncalc_p_hat &lt;- function(dataset) {\n  dataset %&gt;%\n    summarize(stat = mean(vote == \"yes\")) %&gt;%\n    pull()\n}\ncalc_t_conf_int &lt;- function(resampled_dataset, p_hat) {\n  resampled_dataset %&gt;%\n    summarize(\n      lower = p_hat - 2 * sd(stat),\n      upper = p_hat + 2 * sd(stat)\n    )\n}\n\n# Find proportion of yes votes from original population\np_hat &lt;- calc_p_hat(one_poll)\n\n# Review the value\np_hat  \n\n# Calculate bootstrap t-confidence interval (original 0.6 param)\ncalc_t_conf_int(one_poll_boot, p_hat)\n\n# Find proportion of yes votes from new population\np_hat_0.8 &lt;- calc_p_hat(one_poll_0.8)\n  \n# Review the value\np_hat_0.8  \n  \n# Calculate the bootstrap t-confidence interval (new 0.8 param)\ncalc_t_conf_int(one_poll_boot_0.8, p_hat_0.8)"
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#percentile-effects-on-bootstrap-cis",
    "href": "Foundations_of_Inference_in_R_C4.html#percentile-effects-on-bootstrap-cis",
    "title": "67  Confidence intervals",
    "section": "67.7 Percentile effects on bootstrap CIs",
    "text": "67.7 Percentile effects on bootstrap CIs\nMost scientists use 95% intervals to quantify their uncertainty about an estimate. That is, they understand that over a lifetime of creating confidence intervals, only 95% of them will actually contain the parameter that they set out to estimate.\nThere are studies, however, which warrant either stricter or more lenient confidence intervals (and subsequent error rates).\nThe previous bootstrapped p-hat values have been loaded for you and are available in one_poll_boot."
  },
  {
    "objectID": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-7",
    "href": "Foundations_of_Inference_in_R_C4.html#instructions-100-xp-7",
    "title": "67  Confidence intervals",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nCalculate a 95% percentile interval by calling get_confidence_interval(), setting level to 0.95.\nDo the same for a 99% interval,\n… and a 90% interval.\nThe results you just got are stored in a dataframe called conf_int_data. With this dataset, plot ci_endpoints (vertical axis) vs. ci_percent (horizontal axis), and add a line layer using geom_line().\n\n\n\nE8.R\n\n# Calculate a 95% bootstrap percentile interval\none_poll_boot %&gt;% \n  get_confidence_interval(level = 0.95) \n\n# Calculate a 99% bootstrap percentile interval\none_poll_boot %&gt;% \n  get_confidence_interval(level = 0.99) \n\n# Calculate a 90% bootstrap percentile interval\none_poll_boot %&gt;% \n  get_confidence_interval(level = 0.90) \n\n# Plot ci_endpoints vs. ci_percent to compare the intervals\nggplot(conf_int_data, aes(ci_percent, ci_endpoints)) +\n  # Add a line layer\n  geom_line()"
  }
]